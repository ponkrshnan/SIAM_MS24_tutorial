{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5fc65ca-caea-4c5a-80cb-c0f383969d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from scipy.io import savemat\n",
    "\n",
    "from UQpy.scientific_machine_learning.neural_networks import DeepOperatorNetwork\n",
    "from UQpy.scientific_machine_learning.trainers import Trainer\n",
    "from dataset import load_data, rescale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7f1541e-d81f-42c2-938e-bc9e1537d3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"UQpy\")\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfc8a0dd-52d2-4183-805c-305dd805f6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Branch network\n",
    "\n",
    "class BranchNet(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.fnn = nn.Sequential(nn.Linear(101, 100), nn.Tanh())\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, (5, 5), padding=\"same\"),\n",
    "            nn.AvgPool2d(2, 1, padding=0),\n",
    "            nn.Conv2d(16, 16, (5, 5), padding=\"same\"),\n",
    "            nn.AvgPool2d(2, 1, padding=0),\n",
    "            nn.Conv2d(16, 16, (5, 5), padding=\"same\"),\n",
    "            nn.AvgPool2d(2, 1, padding=0),\n",
    "            nn.Conv2d(16, 64, (5, 5), padding=\"same\"),\n",
    "            nn.AvgPool2d(2, 1, padding=0),\n",
    "        )\n",
    "        self.dnn = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 6 * 6, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 200),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fnn(x)\n",
    "        x = x.view(-1, 1, 10, 10)\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.dnn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ba4cc63-4742-452f-b3ed-d5fb26235b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Trunk network\n",
    "\n",
    "class TrunkNet(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.fnn = nn.Sequential(\n",
    "            nn.Linear(2, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 200),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.Xmin = np.array([0.0, 0.0]).reshape((-1, 2))\n",
    "        self.Xmax = np.array([1.0, 1.0]).reshape((-1, 2))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = 2.0 * (x - self.Xmin) / (self.Xmax - self.Xmin) - 1.0\n",
    "        x = x.float()\n",
    "        x = self.fnn(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91205060-a8e9-4efe-b7bc-4eb9ddb34900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "\n",
    "branch_network = BranchNet()\n",
    "trunk_network = TrunkNet()\n",
    "model = DeepOperatorNetwork(branch_network, trunk_network, 2) # Number of ouptus: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df095581-e928-4563-b635-ca73801856ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets and create data loaders\n",
    "class ElasticityDataSet(Dataset):\n",
    "    \"\"\"Load the Elasticity dataset\"\"\"\n",
    "\n",
    "    def __init__(self, x, f_x, u_x, u_y):\n",
    "        self.x = x\n",
    "        self.f_x = f_x\n",
    "        self.u_x = u_x\n",
    "        self.u_y = u_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.f_x.shape[0])\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.x, self.f_x[i, :], (self.u_x[i, :, 0], self.u_y[i, :, 0])\n",
    "\n",
    "(F_train,Ux_train, Uy_train, F_test, Ux_test, Uy_test,\n",
    "    X, ux_train_mean, ux_train_std, uy_train_mean, uy_train_std,) = load_data()\n",
    "train_data = DataLoader(\n",
    "    ElasticityDataSet(\n",
    "        np.float32(X), np.float32(F_train), np.float32(Ux_train), np.float32(Uy_train)\n",
    "    ),\n",
    "    batch_size=100,\n",
    "    shuffle=True,\n",
    ")\n",
    "test_data = DataLoader(\n",
    "    ElasticityDataSet(\n",
    "        np.float32(X), np.float32(F_test), np.float32(Ux_test), np.float32(Uy_test)\n",
    "    ),\n",
    "    batch_size=100,\n",
    "    shuffle=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8aea1a0a-66d1-4ccf-9c75-7831f7df7345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "\n",
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, reduction: str = \"mean\", *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, prediction, label):\n",
    "        return F.mse_loss(\n",
    "            prediction[0], label[0], reduction=self.reduction\n",
    "        ) + F.mse_loss(prediction[1], label[1], reduction=self.reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d562f6f-1bc0-4b6d-9da5-6a381e5a832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and trainer\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
    "trainer = Trainer(model, optimizer, LossFunction())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6508bbbc-51eb-44a5-ba96-5beb52ff95d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] - 2024-05-15 17:23:51,114 - UQpy: Scientific Machine Learning: Beginning training and testing DeepOperatorNetwork\n",
      "[INFO] - 2024-05-15 17:23:54,373 - UQpy: Scientific Machine Learning: Epoch 1 / 1,000 Train Loss 98.72335411372937 Test Loss 4.101278781890869\n",
      "[INFO] - 2024-05-15 17:23:57,551 - UQpy: Scientific Machine Learning: Epoch 2 / 1,000 Train Loss 8.91635571028057 Test Loss 6.430164337158203\n",
      "[INFO] - 2024-05-15 17:24:00,708 - UQpy: Scientific Machine Learning: Epoch 3 / 1,000 Train Loss 2.9467433565541317 Test Loss 2.6804041862487793\n",
      "[INFO] - 2024-05-15 17:24:03,867 - UQpy: Scientific Machine Learning: Epoch 4 / 1,000 Train Loss 2.099652497391952 Test Loss 2.5019302368164062\n",
      "[INFO] - 2024-05-15 17:24:07,023 - UQpy: Scientific Machine Learning: Epoch 5 / 1,000 Train Loss 1.9123037300611798 Test Loss 2.377794027328491\n",
      "[INFO] - 2024-05-15 17:24:10,176 - UQpy: Scientific Machine Learning: Epoch 6 / 1,000 Train Loss 1.8285051646985506 Test Loss 2.3296563625335693\n",
      "[INFO] - 2024-05-15 17:24:13,320 - UQpy: Scientific Machine Learning: Epoch 7 / 1,000 Train Loss 1.7596152832633571 Test Loss 2.2294259071350098\n",
      "[INFO] - 2024-05-15 17:24:16,474 - UQpy: Scientific Machine Learning: Epoch 8 / 1,000 Train Loss 1.7026618342650564 Test Loss 2.1832902431488037\n",
      "[INFO] - 2024-05-15 17:24:19,623 - UQpy: Scientific Machine Learning: Epoch 9 / 1,000 Train Loss 1.6625576207512303 Test Loss 2.124746561050415\n",
      "[INFO] - 2024-05-15 17:24:22,807 - UQpy: Scientific Machine Learning: Epoch 10 / 1,000 Train Loss 1.6311353476423966 Test Loss 2.108959913253784\n",
      "[INFO] - 2024-05-15 17:24:26,009 - UQpy: Scientific Machine Learning: Epoch 11 / 1,000 Train Loss 1.6079863937277543 Test Loss 2.0762810707092285\n",
      "[INFO] - 2024-05-15 17:24:29,221 - UQpy: Scientific Machine Learning: Epoch 12 / 1,000 Train Loss 1.597854683273717 Test Loss 2.0664968490600586\n",
      "[INFO] - 2024-05-15 17:24:32,427 - UQpy: Scientific Machine Learning: Epoch 13 / 1,000 Train Loss 1.5806690517224764 Test Loss 2.0612528324127197\n",
      "[INFO] - 2024-05-15 17:24:35,637 - UQpy: Scientific Machine Learning: Epoch 14 / 1,000 Train Loss 1.5636800213863975 Test Loss 2.054243564605713\n",
      "[INFO] - 2024-05-15 17:24:38,853 - UQpy: Scientific Machine Learning: Epoch 15 / 1,000 Train Loss 1.553631249227022 Test Loss 2.0378966331481934\n",
      "[INFO] - 2024-05-15 17:24:42,055 - UQpy: Scientific Machine Learning: Epoch 16 / 1,000 Train Loss 1.538209921435306 Test Loss 2.0164170265197754\n",
      "[INFO] - 2024-05-15 17:24:45,273 - UQpy: Scientific Machine Learning: Epoch 17 / 1,000 Train Loss 1.5201659641767804 Test Loss 1.9966011047363281\n",
      "[INFO] - 2024-05-15 17:24:48,484 - UQpy: Scientific Machine Learning: Epoch 18 / 1,000 Train Loss 1.5087829200845015 Test Loss 1.9721922874450684\n",
      "[INFO] - 2024-05-15 17:24:51,711 - UQpy: Scientific Machine Learning: Epoch 19 / 1,000 Train Loss 1.487300195192036 Test Loss 1.9462547302246094\n",
      "[INFO] - 2024-05-15 17:24:54,942 - UQpy: Scientific Machine Learning: Epoch 20 / 1,000 Train Loss 1.4564936412008185 Test Loss 1.9028716087341309\n",
      "[INFO] - 2024-05-15 17:24:58,178 - UQpy: Scientific Machine Learning: Epoch 21 / 1,000 Train Loss 1.4081637608377557 Test Loss 1.846980094909668\n",
      "[INFO] - 2024-05-15 17:25:01,369 - UQpy: Scientific Machine Learning: Epoch 22 / 1,000 Train Loss 1.3241288975665444 Test Loss 1.7232775688171387\n",
      "[INFO] - 2024-05-15 17:25:04,575 - UQpy: Scientific Machine Learning: Epoch 23 / 1,000 Train Loss 1.1915670068640458 Test Loss 1.4639627933502197\n",
      "[INFO] - 2024-05-15 17:25:07,778 - UQpy: Scientific Machine Learning: Epoch 24 / 1,000 Train Loss 1.0329064569975202 Test Loss 1.2824734449386597\n",
      "[INFO] - 2024-05-15 17:25:11,016 - UQpy: Scientific Machine Learning: Epoch 25 / 1,000 Train Loss 0.9397463955377278 Test Loss 1.1857374906539917\n",
      "[INFO] - 2024-05-15 17:25:14,430 - UQpy: Scientific Machine Learning: Epoch 26 / 1,000 Train Loss 0.8720203167513797 Test Loss 1.0964343547821045\n",
      "[INFO] - 2024-05-15 17:25:17,638 - UQpy: Scientific Machine Learning: Epoch 27 / 1,000 Train Loss 0.8218305581494382 Test Loss 1.023635983467102\n",
      "[INFO] - 2024-05-15 17:25:20,848 - UQpy: Scientific Machine Learning: Epoch 28 / 1,000 Train Loss 0.7845421966753507 Test Loss 0.9737334251403809\n",
      "[INFO] - 2024-05-15 17:25:24,091 - UQpy: Scientific Machine Learning: Epoch 29 / 1,000 Train Loss 0.7503962438357504 Test Loss 0.9348520040512085\n",
      "[INFO] - 2024-05-15 17:25:27,306 - UQpy: Scientific Machine Learning: Epoch 30 / 1,000 Train Loss 0.7180442151270414 Test Loss 0.8842591047286987\n",
      "[INFO] - 2024-05-15 17:25:30,515 - UQpy: Scientific Machine Learning: Epoch 31 / 1,000 Train Loss 0.6834942620051535 Test Loss 0.840804934501648\n",
      "[INFO] - 2024-05-15 17:25:33,745 - UQpy: Scientific Machine Learning: Epoch 32 / 1,000 Train Loss 0.6509401468854201 Test Loss 0.7998851537704468\n",
      "[INFO] - 2024-05-15 17:25:36,969 - UQpy: Scientific Machine Learning: Epoch 33 / 1,000 Train Loss 0.6144812122771615 Test Loss 0.7625875473022461\n",
      "[INFO] - 2024-05-15 17:25:40,203 - UQpy: Scientific Machine Learning: Epoch 34 / 1,000 Train Loss 0.5879161138283578 Test Loss 0.7331600189208984\n",
      "[INFO] - 2024-05-15 17:25:43,440 - UQpy: Scientific Machine Learning: Epoch 35 / 1,000 Train Loss 0.5692170215280432 Test Loss 0.7147881984710693\n",
      "[INFO] - 2024-05-15 17:25:46,659 - UQpy: Scientific Machine Learning: Epoch 36 / 1,000 Train Loss 0.5538437586081656 Test Loss 0.7001655101776123\n",
      "[INFO] - 2024-05-15 17:25:49,905 - UQpy: Scientific Machine Learning: Epoch 37 / 1,000 Train Loss 0.5399320439288491 Test Loss 0.687024712562561\n",
      "[INFO] - 2024-05-15 17:25:53,142 - UQpy: Scientific Machine Learning: Epoch 38 / 1,000 Train Loss 0.5296858831455833 Test Loss 0.6783193945884705\n",
      "[INFO] - 2024-05-15 17:25:56,405 - UQpy: Scientific Machine Learning: Epoch 39 / 1,000 Train Loss 0.5203251964167545 Test Loss 0.6661096215248108\n",
      "[INFO] - 2024-05-15 17:25:59,672 - UQpy: Scientific Machine Learning: Epoch 40 / 1,000 Train Loss 0.5132645807768169 Test Loss 0.6567339897155762\n",
      "[INFO] - 2024-05-15 17:26:02,925 - UQpy: Scientific Machine Learning: Epoch 41 / 1,000 Train Loss 0.5061669804547962 Test Loss 0.6472471952438354\n",
      "[INFO] - 2024-05-15 17:26:06,150 - UQpy: Scientific Machine Learning: Epoch 42 / 1,000 Train Loss 0.500809128347196 Test Loss 0.6439204812049866\n",
      "[INFO] - 2024-05-15 17:26:09,366 - UQpy: Scientific Machine Learning: Epoch 43 / 1,000 Train Loss 0.4965943584316655 Test Loss 0.6362486481666565\n",
      "[INFO] - 2024-05-15 17:26:12,613 - UQpy: Scientific Machine Learning: Epoch 44 / 1,000 Train Loss 0.4924164775170778 Test Loss 0.6295207142829895\n",
      "[INFO] - 2024-05-15 17:26:15,848 - UQpy: Scientific Machine Learning: Epoch 45 / 1,000 Train Loss 0.4873879410718617 Test Loss 0.6229135990142822\n",
      "[INFO] - 2024-05-15 17:26:19,090 - UQpy: Scientific Machine Learning: Epoch 46 / 1,000 Train Loss 0.4849136471748352 Test Loss 0.621029257774353\n",
      "[INFO] - 2024-05-15 17:26:22,335 - UQpy: Scientific Machine Learning: Epoch 47 / 1,000 Train Loss 0.48192860891944483 Test Loss 0.6182281970977783\n",
      "[INFO] - 2024-05-15 17:26:25,580 - UQpy: Scientific Machine Learning: Epoch 48 / 1,000 Train Loss 0.4761635614068885 Test Loss 0.6084392070770264\n",
      "[INFO] - 2024-05-15 17:26:28,849 - UQpy: Scientific Machine Learning: Epoch 49 / 1,000 Train Loss 0.47314564021010147 Test Loss 0.6054956912994385\n",
      "[INFO] - 2024-05-15 17:26:32,078 - UQpy: Scientific Machine Learning: Epoch 50 / 1,000 Train Loss 0.4705065661355069 Test Loss 0.6007788181304932\n",
      "[INFO] - 2024-05-15 17:26:35,321 - UQpy: Scientific Machine Learning: Epoch 51 / 1,000 Train Loss 0.4678087752116354 Test Loss 0.5968313217163086\n",
      "[INFO] - 2024-05-15 17:26:38,569 - UQpy: Scientific Machine Learning: Epoch 52 / 1,000 Train Loss 0.46515244088674845 Test Loss 0.5932374000549316\n",
      "[INFO] - 2024-05-15 17:26:41,819 - UQpy: Scientific Machine Learning: Epoch 53 / 1,000 Train Loss 0.46176441405948837 Test Loss 0.5878534913063049\n",
      "[INFO] - 2024-05-15 17:26:45,071 - UQpy: Scientific Machine Learning: Epoch 54 / 1,000 Train Loss 0.4592975487834529 Test Loss 0.5850548148155212\n",
      "[INFO] - 2024-05-15 17:26:48,316 - UQpy: Scientific Machine Learning: Epoch 55 / 1,000 Train Loss 0.4555519678090748 Test Loss 0.5786590576171875\n",
      "[INFO] - 2024-05-15 17:26:51,557 - UQpy: Scientific Machine Learning: Epoch 56 / 1,000 Train Loss 0.4529396577885276 Test Loss 0.5777060389518738\n",
      "[INFO] - 2024-05-15 17:26:54,809 - UQpy: Scientific Machine Learning: Epoch 57 / 1,000 Train Loss 0.4504763656540921 Test Loss 0.5712761878967285\n",
      "[INFO] - 2024-05-15 17:26:58,053 - UQpy: Scientific Machine Learning: Epoch 58 / 1,000 Train Loss 0.44767121421663386 Test Loss 0.5673766136169434\n",
      "[INFO] - 2024-05-15 17:27:01,307 - UQpy: Scientific Machine Learning: Epoch 59 / 1,000 Train Loss 0.44397427220093577 Test Loss 0.5624912977218628\n",
      "[INFO] - 2024-05-15 17:27:04,546 - UQpy: Scientific Machine Learning: Epoch 60 / 1,000 Train Loss 0.441586340728559 Test Loss 0.560702919960022\n",
      "[INFO] - 2024-05-15 17:27:07,780 - UQpy: Scientific Machine Learning: Epoch 61 / 1,000 Train Loss 0.43885530766687897 Test Loss 0.5551561117172241\n",
      "[INFO] - 2024-05-15 17:27:11,011 - UQpy: Scientific Machine Learning: Epoch 62 / 1,000 Train Loss 0.43682468408032465 Test Loss 0.5531035661697388\n",
      "[INFO] - 2024-05-15 17:27:14,246 - UQpy: Scientific Machine Learning: Epoch 63 / 1,000 Train Loss 0.4342974173395257 Test Loss 0.54780113697052\n",
      "[INFO] - 2024-05-15 17:27:17,497 - UQpy: Scientific Machine Learning: Epoch 64 / 1,000 Train Loss 0.4310834156839471 Test Loss 0.5445240139961243\n",
      "[INFO] - 2024-05-15 17:27:20,736 - UQpy: Scientific Machine Learning: Epoch 65 / 1,000 Train Loss 0.4280654819388139 Test Loss 0.5418261289596558\n",
      "[INFO] - 2024-05-15 17:27:23,974 - UQpy: Scientific Machine Learning: Epoch 66 / 1,000 Train Loss 0.4252749148168062 Test Loss 0.5389910340309143\n",
      "[INFO] - 2024-05-15 17:27:27,218 - UQpy: Scientific Machine Learning: Epoch 67 / 1,000 Train Loss 0.423214023050509 Test Loss 0.5341712236404419\n",
      "[INFO] - 2024-05-15 17:27:30,480 - UQpy: Scientific Machine Learning: Epoch 68 / 1,000 Train Loss 0.4219413697719574 Test Loss 0.5313392877578735\n",
      "[INFO] - 2024-05-15 17:27:33,732 - UQpy: Scientific Machine Learning: Epoch 69 / 1,000 Train Loss 0.4185808379399149 Test Loss 0.5312181115150452\n",
      "[INFO] - 2024-05-15 17:27:36,952 - UQpy: Scientific Machine Learning: Epoch 70 / 1,000 Train Loss 0.4167068153619766 Test Loss 0.5259696245193481\n",
      "[INFO] - 2024-05-15 17:27:40,221 - UQpy: Scientific Machine Learning: Epoch 71 / 1,000 Train Loss 0.41463282547499003 Test Loss 0.5234771966934204\n",
      "[INFO] - 2024-05-15 17:27:43,487 - UQpy: Scientific Machine Learning: Epoch 72 / 1,000 Train Loss 0.4124635567790584 Test Loss 0.5209327340126038\n",
      "[INFO] - 2024-05-15 17:27:46,736 - UQpy: Scientific Machine Learning: Epoch 73 / 1,000 Train Loss 0.4093656163466604 Test Loss 0.5160505175590515\n",
      "[INFO] - 2024-05-15 17:27:49,996 - UQpy: Scientific Machine Learning: Epoch 74 / 1,000 Train Loss 0.40643503007135895 Test Loss 0.5130046606063843\n",
      "[INFO] - 2024-05-15 17:27:53,248 - UQpy: Scientific Machine Learning: Epoch 75 / 1,000 Train Loss 0.4042075590083474 Test Loss 0.5101861953735352\n",
      "[INFO] - 2024-05-15 17:27:56,483 - UQpy: Scientific Machine Learning: Epoch 76 / 1,000 Train Loss 0.4021150673690595 Test Loss 0.5083803534507751\n",
      "[INFO] - 2024-05-15 17:27:59,766 - UQpy: Scientific Machine Learning: Epoch 77 / 1,000 Train Loss 0.40032641668068736 Test Loss 0.5053192377090454\n",
      "[INFO] - 2024-05-15 17:28:03,036 - UQpy: Scientific Machine Learning: Epoch 78 / 1,000 Train Loss 0.3976577049807498 Test Loss 0.5008211731910706\n",
      "[INFO] - 2024-05-15 17:28:06,289 - UQpy: Scientific Machine Learning: Epoch 79 / 1,000 Train Loss 0.39582412964419317 Test Loss 0.49833863973617554\n",
      "[INFO] - 2024-05-15 17:28:09,546 - UQpy: Scientific Machine Learning: Epoch 80 / 1,000 Train Loss 0.39205740627489594 Test Loss 0.4959082007408142\n",
      "[INFO] - 2024-05-15 17:28:12,832 - UQpy: Scientific Machine Learning: Epoch 81 / 1,000 Train Loss 0.3899973756388614 Test Loss 0.4923118054866791\n",
      "[INFO] - 2024-05-15 17:28:16,102 - UQpy: Scientific Machine Learning: Epoch 82 / 1,000 Train Loss 0.3877695689075871 Test Loss 0.48853808641433716\n",
      "[INFO] - 2024-05-15 17:28:19,365 - UQpy: Scientific Machine Learning: Epoch 83 / 1,000 Train Loss 0.3857334224801314 Test Loss 0.48598527908325195\n",
      "[INFO] - 2024-05-15 17:28:22,620 - UQpy: Scientific Machine Learning: Epoch 84 / 1,000 Train Loss 0.3827226271754817 Test Loss 0.4839179515838623\n",
      "[INFO] - 2024-05-15 17:28:25,868 - UQpy: Scientific Machine Learning: Epoch 85 / 1,000 Train Loss 0.3802340093411897 Test Loss 0.48000001907348633\n",
      "[INFO] - 2024-05-15 17:28:29,113 - UQpy: Scientific Machine Learning: Epoch 86 / 1,000 Train Loss 0.37718460748070165 Test Loss 0.4769904613494873\n",
      "[INFO] - 2024-05-15 17:28:32,352 - UQpy: Scientific Machine Learning: Epoch 87 / 1,000 Train Loss 0.3744219267054608 Test Loss 0.4734361469745636\n",
      "[INFO] - 2024-05-15 17:28:35,611 - UQpy: Scientific Machine Learning: Epoch 88 / 1,000 Train Loss 0.3714063230313753 Test Loss 0.4685726761817932\n",
      "[INFO] - 2024-05-15 17:28:38,872 - UQpy: Scientific Machine Learning: Epoch 89 / 1,000 Train Loss 0.36891857652287735 Test Loss 0.4677767753601074\n",
      "[INFO] - 2024-05-15 17:28:42,135 - UQpy: Scientific Machine Learning: Epoch 90 / 1,000 Train Loss 0.36611017114237737 Test Loss 0.4610280990600586\n",
      "[INFO] - 2024-05-15 17:28:45,403 - UQpy: Scientific Machine Learning: Epoch 91 / 1,000 Train Loss 0.36327683611920003 Test Loss 0.45710158348083496\n",
      "[INFO] - 2024-05-15 17:28:48,651 - UQpy: Scientific Machine Learning: Epoch 92 / 1,000 Train Loss 0.3585320915046491 Test Loss 0.4523407220840454\n",
      "[INFO] - 2024-05-15 17:28:51,928 - UQpy: Scientific Machine Learning: Epoch 93 / 1,000 Train Loss 0.35490555198569046 Test Loss 0.4458215832710266\n",
      "[INFO] - 2024-05-15 17:28:55,212 - UQpy: Scientific Machine Learning: Epoch 94 / 1,000 Train Loss 0.35171384403580114 Test Loss 0.4414675235748291\n",
      "[INFO] - 2024-05-15 17:28:58,478 - UQpy: Scientific Machine Learning: Epoch 95 / 1,000 Train Loss 0.3458734593893352 Test Loss 0.43553054332733154\n",
      "[INFO] - 2024-05-15 17:29:01,745 - UQpy: Scientific Machine Learning: Epoch 96 / 1,000 Train Loss 0.34217447513028193 Test Loss 0.43057096004486084\n",
      "[INFO] - 2024-05-15 17:29:05,015 - UQpy: Scientific Machine Learning: Epoch 97 / 1,000 Train Loss 0.3373924289879046 Test Loss 0.42329055070877075\n",
      "[INFO] - 2024-05-15 17:29:08,270 - UQpy: Scientific Machine Learning: Epoch 98 / 1,000 Train Loss 0.33126071879738256 Test Loss 0.41533738374710083\n",
      "[INFO] - 2024-05-15 17:29:11,551 - UQpy: Scientific Machine Learning: Epoch 99 / 1,000 Train Loss 0.3258436063402577 Test Loss 0.4068155288696289\n",
      "[INFO] - 2024-05-15 17:29:14,813 - UQpy: Scientific Machine Learning: Epoch 100 / 1,000 Train Loss 0.3208898486275422 Test Loss 0.4014633595943451\n",
      "[INFO] - 2024-05-15 17:29:18,070 - UQpy: Scientific Machine Learning: Epoch 101 / 1,000 Train Loss 0.3139738009164208 Test Loss 0.39089858531951904\n",
      "[INFO] - 2024-05-15 17:29:21,323 - UQpy: Scientific Machine Learning: Epoch 102 / 1,000 Train Loss 0.3070036753227836 Test Loss 0.3837626576423645\n",
      "[INFO] - 2024-05-15 17:29:24,620 - UQpy: Scientific Machine Learning: Epoch 103 / 1,000 Train Loss 0.3022377365513852 Test Loss 0.3784137964248657\n",
      "[INFO] - 2024-05-15 17:29:27,892 - UQpy: Scientific Machine Learning: Epoch 104 / 1,000 Train Loss 0.2960416870681863 Test Loss 0.37020614743232727\n",
      "[INFO] - 2024-05-15 17:29:31,179 - UQpy: Scientific Machine Learning: Epoch 105 / 1,000 Train Loss 0.2922627627849579 Test Loss 0.36049747467041016\n",
      "[INFO] - 2024-05-15 17:29:34,458 - UQpy: Scientific Machine Learning: Epoch 106 / 1,000 Train Loss 0.28539929891887467 Test Loss 0.35513412952423096\n",
      "[INFO] - 2024-05-15 17:29:37,732 - UQpy: Scientific Machine Learning: Epoch 107 / 1,000 Train Loss 0.28142814495061574 Test Loss 0.35174453258514404\n",
      "[INFO] - 2024-05-15 17:29:41,017 - UQpy: Scientific Machine Learning: Epoch 108 / 1,000 Train Loss 0.27876641954246323 Test Loss 0.3476985692977905\n",
      "[INFO] - 2024-05-15 17:29:44,286 - UQpy: Scientific Machine Learning: Epoch 109 / 1,000 Train Loss 0.2753936013108806 Test Loss 0.3414885699748993\n",
      "[INFO] - 2024-05-15 17:29:47,541 - UQpy: Scientific Machine Learning: Epoch 110 / 1,000 Train Loss 0.2727174311876297 Test Loss 0.34278082847595215\n",
      "[INFO] - 2024-05-15 17:29:50,802 - UQpy: Scientific Machine Learning: Epoch 111 / 1,000 Train Loss 0.2696721342049147 Test Loss 0.33490657806396484\n",
      "[INFO] - 2024-05-15 17:29:54,069 - UQpy: Scientific Machine Learning: Epoch 112 / 1,000 Train Loss 0.2673406263715343 Test Loss 0.3327234983444214\n",
      "[INFO] - 2024-05-15 17:29:57,343 - UQpy: Scientific Machine Learning: Epoch 113 / 1,000 Train Loss 0.2648464488355737 Test Loss 0.3294287621974945\n",
      "[INFO] - 2024-05-15 17:30:00,620 - UQpy: Scientific Machine Learning: Epoch 114 / 1,000 Train Loss 0.26286767266298594 Test Loss 0.3267543911933899\n",
      "[INFO] - 2024-05-15 17:30:03,874 - UQpy: Scientific Machine Learning: Epoch 115 / 1,000 Train Loss 0.26119727366848994 Test Loss 0.32472774386405945\n",
      "[INFO] - 2024-05-15 17:30:07,121 - UQpy: Scientific Machine Learning: Epoch 116 / 1,000 Train Loss 0.25973268088541535 Test Loss 0.3228710889816284\n",
      "[INFO] - 2024-05-15 17:30:10,392 - UQpy: Scientific Machine Learning: Epoch 117 / 1,000 Train Loss 0.25919209655962494 Test Loss 0.3229994773864746\n",
      "[INFO] - 2024-05-15 17:30:13,673 - UQpy: Scientific Machine Learning: Epoch 118 / 1,000 Train Loss 0.2567694477344814 Test Loss 0.31884250044822693\n",
      "[INFO] - 2024-05-15 17:30:16,940 - UQpy: Scientific Machine Learning: Epoch 119 / 1,000 Train Loss 0.2544116275875192 Test Loss 0.31672921776771545\n",
      "[INFO] - 2024-05-15 17:30:20,214 - UQpy: Scientific Machine Learning: Epoch 120 / 1,000 Train Loss 0.2528170449169059 Test Loss 0.3156658113002777\n",
      "[INFO] - 2024-05-15 17:30:23,506 - UQpy: Scientific Machine Learning: Epoch 121 / 1,000 Train Loss 0.25179227637617213 Test Loss 0.3156856596469879\n",
      "[INFO] - 2024-05-15 17:30:26,772 - UQpy: Scientific Machine Learning: Epoch 122 / 1,000 Train Loss 0.25031710925855133 Test Loss 0.3132089376449585\n",
      "[INFO] - 2024-05-15 17:30:30,058 - UQpy: Scientific Machine Learning: Epoch 123 / 1,000 Train Loss 0.2485900929099635 Test Loss 0.31264030933380127\n",
      "[INFO] - 2024-05-15 17:30:33,337 - UQpy: Scientific Machine Learning: Epoch 124 / 1,000 Train Loss 0.2484000850664942 Test Loss 0.3113029897212982\n",
      "[INFO] - 2024-05-15 17:30:36,621 - UQpy: Scientific Machine Learning: Epoch 125 / 1,000 Train Loss 0.24681177578474345 Test Loss 0.3101658821105957\n",
      "[INFO] - 2024-05-15 17:30:39,884 - UQpy: Scientific Machine Learning: Epoch 126 / 1,000 Train Loss 0.2461396596933666 Test Loss 0.30801141262054443\n",
      "[INFO] - 2024-05-15 17:30:43,176 - UQpy: Scientific Machine Learning: Epoch 127 / 1,000 Train Loss 0.24404642299601906 Test Loss 0.3065894842147827\n",
      "[INFO] - 2024-05-15 17:30:46,446 - UQpy: Scientific Machine Learning: Epoch 128 / 1,000 Train Loss 0.2437596376004972 Test Loss 0.30555203557014465\n",
      "[INFO] - 2024-05-15 17:30:49,704 - UQpy: Scientific Machine Learning: Epoch 129 / 1,000 Train Loss 0.24459427908847206 Test Loss 0.3052895963191986\n",
      "[INFO] - 2024-05-15 17:30:52,954 - UQpy: Scientific Machine Learning: Epoch 130 / 1,000 Train Loss 0.24311743441380954 Test Loss 0.3052067458629608\n",
      "[INFO] - 2024-05-15 17:30:56,216 - UQpy: Scientific Machine Learning: Epoch 131 / 1,000 Train Loss 0.2405875325202942 Test Loss 0.3029225468635559\n",
      "[INFO] - 2024-05-15 17:30:59,532 - UQpy: Scientific Machine Learning: Epoch 132 / 1,000 Train Loss 0.2394879711301703 Test Loss 0.30117109417915344\n",
      "[INFO] - 2024-05-15 17:31:02,787 - UQpy: Scientific Machine Learning: Epoch 133 / 1,000 Train Loss 0.23822263667457982 Test Loss 0.30151671171188354\n",
      "[INFO] - 2024-05-15 17:31:06,062 - UQpy: Scientific Machine Learning: Epoch 134 / 1,000 Train Loss 0.23771794140338898 Test Loss 0.2998073697090149\n",
      "[INFO] - 2024-05-15 17:31:09,399 - UQpy: Scientific Machine Learning: Epoch 135 / 1,000 Train Loss 0.23695515331469083 Test Loss 0.29832467436790466\n",
      "[INFO] - 2024-05-15 17:31:12,676 - UQpy: Scientific Machine Learning: Epoch 136 / 1,000 Train Loss 0.23678360016722427 Test Loss 0.2988651394844055\n",
      "[INFO] - 2024-05-15 17:31:15,971 - UQpy: Scientific Machine Learning: Epoch 137 / 1,000 Train Loss 0.23467490782863215 Test Loss 0.29609403014183044\n",
      "[INFO] - 2024-05-15 17:31:19,267 - UQpy: Scientific Machine Learning: Epoch 138 / 1,000 Train Loss 0.23337805349575846 Test Loss 0.29434579610824585\n",
      "[INFO] - 2024-05-15 17:31:22,517 - UQpy: Scientific Machine Learning: Epoch 139 / 1,000 Train Loss 0.23293125237289228 Test Loss 0.2954988181591034\n",
      "[INFO] - 2024-05-15 17:31:25,786 - UQpy: Scientific Machine Learning: Epoch 140 / 1,000 Train Loss 0.23264745740514053 Test Loss 0.29245156049728394\n",
      "[INFO] - 2024-05-15 17:31:28,996 - UQpy: Scientific Machine Learning: Epoch 141 / 1,000 Train Loss 0.23229964234327016 Test Loss 0.2923467457294464\n",
      "[INFO] - 2024-05-15 17:31:32,209 - UQpy: Scientific Machine Learning: Epoch 142 / 1,000 Train Loss 0.2289037210376639 Test Loss 0.2892324924468994\n",
      "[INFO] - 2024-05-15 17:31:35,376 - UQpy: Scientific Machine Learning: Epoch 143 / 1,000 Train Loss 0.22847671571530795 Test Loss 0.2888149917125702\n",
      "[INFO] - 2024-05-15 17:31:38,499 - UQpy: Scientific Machine Learning: Epoch 144 / 1,000 Train Loss 0.2269329420830074 Test Loss 0.28823375701904297\n",
      "[INFO] - 2024-05-15 17:31:41,631 - UQpy: Scientific Machine Learning: Epoch 145 / 1,000 Train Loss 0.226152943937402 Test Loss 0.2871367633342743\n",
      "[INFO] - 2024-05-15 17:31:44,781 - UQpy: Scientific Machine Learning: Epoch 146 / 1,000 Train Loss 0.22558918360032534 Test Loss 0.28483957052230835\n",
      "[INFO] - 2024-05-15 17:31:47,949 - UQpy: Scientific Machine Learning: Epoch 147 / 1,000 Train Loss 0.2251661302227723 Test Loss 0.28440362215042114\n",
      "[INFO] - 2024-05-15 17:31:51,118 - UQpy: Scientific Machine Learning: Epoch 148 / 1,000 Train Loss 0.2230602973385861 Test Loss 0.28327372670173645\n",
      "[INFO] - 2024-05-15 17:31:54,304 - UQpy: Scientific Machine Learning: Epoch 149 / 1,000 Train Loss 0.2217360280062023 Test Loss 0.2806467115879059\n",
      "[INFO] - 2024-05-15 17:31:57,487 - UQpy: Scientific Machine Learning: Epoch 150 / 1,000 Train Loss 0.22104750495207937 Test Loss 0.27965328097343445\n",
      "[INFO] - 2024-05-15 17:32:00,659 - UQpy: Scientific Machine Learning: Epoch 151 / 1,000 Train Loss 0.2204238717493258 Test Loss 0.2801487147808075\n",
      "[INFO] - 2024-05-15 17:32:03,906 - UQpy: Scientific Machine Learning: Epoch 152 / 1,000 Train Loss 0.21916338879811137 Test Loss 0.27801284193992615\n",
      "[INFO] - 2024-05-15 17:32:07,100 - UQpy: Scientific Machine Learning: Epoch 153 / 1,000 Train Loss 0.2184888453860032 Test Loss 0.28041917085647583\n",
      "[INFO] - 2024-05-15 17:32:10,309 - UQpy: Scientific Machine Learning: Epoch 154 / 1,000 Train Loss 0.21628222340031675 Test Loss 0.274865061044693\n",
      "[INFO] - 2024-05-15 17:32:13,530 - UQpy: Scientific Machine Learning: Epoch 155 / 1,000 Train Loss 0.21553238991059756 Test Loss 0.272832989692688\n",
      "[INFO] - 2024-05-15 17:32:16,688 - UQpy: Scientific Machine Learning: Epoch 156 / 1,000 Train Loss 0.21561506155290103 Test Loss 0.27453312277793884\n",
      "[INFO] - 2024-05-15 17:32:19,849 - UQpy: Scientific Machine Learning: Epoch 157 / 1,000 Train Loss 0.21863143381319547 Test Loss 0.27733826637268066\n",
      "[INFO] - 2024-05-15 17:32:22,987 - UQpy: Scientific Machine Learning: Epoch 158 / 1,000 Train Loss 0.21647551812623678 Test Loss 0.2738429307937622\n",
      "[INFO] - 2024-05-15 17:32:26,155 - UQpy: Scientific Machine Learning: Epoch 159 / 1,000 Train Loss 0.2155597060918808 Test Loss 0.27010422945022583\n",
      "[INFO] - 2024-05-15 17:32:29,314 - UQpy: Scientific Machine Learning: Epoch 160 / 1,000 Train Loss 0.21010041629013262 Test Loss 0.2683987319469452\n",
      "[INFO] - 2024-05-15 17:32:32,496 - UQpy: Scientific Machine Learning: Epoch 161 / 1,000 Train Loss 0.208238989899033 Test Loss 0.2664511799812317\n",
      "[INFO] - 2024-05-15 17:32:35,696 - UQpy: Scientific Machine Learning: Epoch 162 / 1,000 Train Loss 0.20712784481676003 Test Loss 0.26335251331329346\n",
      "[INFO] - 2024-05-15 17:32:38,925 - UQpy: Scientific Machine Learning: Epoch 163 / 1,000 Train Loss 0.20656133011767738 Test Loss 0.266907274723053\n",
      "[INFO] - 2024-05-15 17:32:42,131 - UQpy: Scientific Machine Learning: Epoch 164 / 1,000 Train Loss 0.20690298943143143 Test Loss 0.26411011815071106\n",
      "[INFO] - 2024-05-15 17:32:45,372 - UQpy: Scientific Machine Learning: Epoch 165 / 1,000 Train Loss 0.20811249394165843 Test Loss 0.26212039589881897\n",
      "[INFO] - 2024-05-15 17:32:48,684 - UQpy: Scientific Machine Learning: Epoch 166 / 1,000 Train Loss 0.20358979074578537 Test Loss 0.25804683566093445\n",
      "[INFO] - 2024-05-15 17:32:51,934 - UQpy: Scientific Machine Learning: Epoch 167 / 1,000 Train Loss 0.20177957023444928 Test Loss 0.26013344526290894\n",
      "[INFO] - 2024-05-15 17:32:55,080 - UQpy: Scientific Machine Learning: Epoch 168 / 1,000 Train Loss 0.20288164364664177 Test Loss 0.2608032822608948\n",
      "[INFO] - 2024-05-15 17:32:58,249 - UQpy: Scientific Machine Learning: Epoch 169 / 1,000 Train Loss 0.20004038904842578 Test Loss 0.2543613314628601\n",
      "[INFO] - 2024-05-15 17:33:01,397 - UQpy: Scientific Machine Learning: Epoch 170 / 1,000 Train Loss 0.1992018411034032 Test Loss 0.25547564029693604\n",
      "[INFO] - 2024-05-15 17:33:04,567 - UQpy: Scientific Machine Learning: Epoch 171 / 1,000 Train Loss 0.1976723114126607 Test Loss 0.25407159328460693\n",
      "[INFO] - 2024-05-15 17:33:07,748 - UQpy: Scientific Machine Learning: Epoch 172 / 1,000 Train Loss 0.19742410277065478 Test Loss 0.25356218218803406\n",
      "[INFO] - 2024-05-15 17:33:10,902 - UQpy: Scientific Machine Learning: Epoch 173 / 1,000 Train Loss 0.1955335697061137 Test Loss 0.24999096989631653\n",
      "[INFO] - 2024-05-15 17:33:14,029 - UQpy: Scientific Machine Learning: Epoch 174 / 1,000 Train Loss 0.19381187464061536 Test Loss 0.24999600648880005\n",
      "[INFO] - 2024-05-15 17:33:17,183 - UQpy: Scientific Machine Learning: Epoch 175 / 1,000 Train Loss 0.193547953900538 Test Loss 0.24701854586601257\n",
      "[INFO] - 2024-05-15 17:33:20,338 - UQpy: Scientific Machine Learning: Epoch 176 / 1,000 Train Loss 0.19276678719018636 Test Loss 0.24592626094818115\n",
      "[INFO] - 2024-05-15 17:33:23,495 - UQpy: Scientific Machine Learning: Epoch 177 / 1,000 Train Loss 0.18995634662477592 Test Loss 0.2466842234134674\n",
      "[INFO] - 2024-05-15 17:33:26,685 - UQpy: Scientific Machine Learning: Epoch 178 / 1,000 Train Loss 0.19182891281027542 Test Loss 0.24420993030071259\n",
      "[INFO] - 2024-05-15 17:33:29,842 - UQpy: Scientific Machine Learning: Epoch 179 / 1,000 Train Loss 0.18997797605238462 Test Loss 0.24341323971748352\n",
      "[INFO] - 2024-05-15 17:33:33,020 - UQpy: Scientific Machine Learning: Epoch 180 / 1,000 Train Loss 0.18734749210508247 Test Loss 0.241012841463089\n",
      "[INFO] - 2024-05-15 17:33:36,152 - UQpy: Scientific Machine Learning: Epoch 181 / 1,000 Train Loss 0.18758807762673027 Test Loss 0.24224184453487396\n",
      "[INFO] - 2024-05-15 17:33:39,301 - UQpy: Scientific Machine Learning: Epoch 182 / 1,000 Train Loss 0.18763457395528493 Test Loss 0.24248751997947693\n",
      "[INFO] - 2024-05-15 17:33:42,498 - UQpy: Scientific Machine Learning: Epoch 183 / 1,000 Train Loss 0.18992675997708974 Test Loss 0.25044047832489014\n",
      "[INFO] - 2024-05-15 17:33:45,717 - UQpy: Scientific Machine Learning: Epoch 184 / 1,000 Train Loss 0.18813301387586093 Test Loss 0.2437511682510376\n",
      "[INFO] - 2024-05-15 17:33:48,945 - UQpy: Scientific Machine Learning: Epoch 185 / 1,000 Train Loss 0.1854040442328704 Test Loss 0.24155083298683167\n",
      "[INFO] - 2024-05-15 17:33:52,130 - UQpy: Scientific Machine Learning: Epoch 186 / 1,000 Train Loss 0.18307993757097343 Test Loss 0.2401757687330246\n",
      "[INFO] - 2024-05-15 17:33:55,350 - UQpy: Scientific Machine Learning: Epoch 187 / 1,000 Train Loss 0.18535964896804408 Test Loss 0.24144339561462402\n",
      "[INFO] - 2024-05-15 17:33:58,670 - UQpy: Scientific Machine Learning: Epoch 188 / 1,000 Train Loss 0.18772780110961512 Test Loss 0.2465827465057373\n",
      "[INFO] - 2024-05-15 17:34:02,033 - UQpy: Scientific Machine Learning: Epoch 189 / 1,000 Train Loss 0.18114817220913737 Test Loss 0.23430120944976807\n",
      "[INFO] - 2024-05-15 17:34:05,332 - UQpy: Scientific Machine Learning: Epoch 190 / 1,000 Train Loss 0.17851519114092776 Test Loss 0.23137769103050232\n",
      "[INFO] - 2024-05-15 17:34:08,608 - UQpy: Scientific Machine Learning: Epoch 191 / 1,000 Train Loss 0.17761072281159854 Test Loss 0.2303493171930313\n",
      "[INFO] - 2024-05-15 17:34:11,865 - UQpy: Scientific Machine Learning: Epoch 192 / 1,000 Train Loss 0.1763951809782731 Test Loss 0.22825315594673157\n",
      "[INFO] - 2024-05-15 17:34:15,063 - UQpy: Scientific Machine Learning: Epoch 193 / 1,000 Train Loss 0.17478756371297335 Test Loss 0.22835136950016022\n",
      "[INFO] - 2024-05-15 17:34:18,273 - UQpy: Scientific Machine Learning: Epoch 194 / 1,000 Train Loss 0.17590606761606117 Test Loss 0.22982670366764069\n",
      "[INFO] - 2024-05-15 17:34:21,448 - UQpy: Scientific Machine Learning: Epoch 195 / 1,000 Train Loss 0.17375053700647855 Test Loss 0.22585272789001465\n",
      "[INFO] - 2024-05-15 17:34:24,665 - UQpy: Scientific Machine Learning: Epoch 196 / 1,000 Train Loss 0.1755939529914605 Test Loss 0.2258090078830719\n",
      "[INFO] - 2024-05-15 17:34:27,874 - UQpy: Scientific Machine Learning: Epoch 197 / 1,000 Train Loss 0.17178843209618017 Test Loss 0.22730441391468048\n",
      "[INFO] - 2024-05-15 17:34:31,091 - UQpy: Scientific Machine Learning: Epoch 198 / 1,000 Train Loss 0.1718387007713318 Test Loss 0.22406017780303955\n",
      "[INFO] - 2024-05-15 17:34:34,266 - UQpy: Scientific Machine Learning: Epoch 199 / 1,000 Train Loss 0.17322378095827604 Test Loss 0.2244490385055542\n",
      "[INFO] - 2024-05-15 17:34:37,879 - UQpy: Scientific Machine Learning: Epoch 200 / 1,000 Train Loss 0.17157998994777077 Test Loss 0.2222924381494522\n",
      "[INFO] - 2024-05-15 17:34:41,181 - UQpy: Scientific Machine Learning: Epoch 201 / 1,000 Train Loss 0.17059023246953361 Test Loss 0.22145070135593414\n",
      "[INFO] - 2024-05-15 17:34:44,480 - UQpy: Scientific Machine Learning: Epoch 202 / 1,000 Train Loss 0.16813441013035021 Test Loss 0.21912401914596558\n",
      "[INFO] - 2024-05-15 17:34:47,778 - UQpy: Scientific Machine Learning: Epoch 203 / 1,000 Train Loss 0.1679455661459973 Test Loss 0.21865922212600708\n",
      "[INFO] - 2024-05-15 17:34:51,067 - UQpy: Scientific Machine Learning: Epoch 204 / 1,000 Train Loss 0.1669803959758658 Test Loss 0.21831998229026794\n",
      "[INFO] - 2024-05-15 17:34:54,350 - UQpy: Scientific Machine Learning: Epoch 205 / 1,000 Train Loss 0.16628502034827283 Test Loss 0.22193220257759094\n",
      "[INFO] - 2024-05-15 17:34:57,646 - UQpy: Scientific Machine Learning: Epoch 206 / 1,000 Train Loss 0.16657589375972748 Test Loss 0.2177811861038208\n",
      "[INFO] - 2024-05-15 17:35:00,840 - UQpy: Scientific Machine Learning: Epoch 207 / 1,000 Train Loss 0.16551130461065391 Test Loss 0.21664465963840485\n",
      "[INFO] - 2024-05-15 17:35:04,083 - UQpy: Scientific Machine Learning: Epoch 208 / 1,000 Train Loss 0.16870539949128502 Test Loss 0.2197798192501068\n",
      "[INFO] - 2024-05-15 17:35:07,291 - UQpy: Scientific Machine Learning: Epoch 209 / 1,000 Train Loss 0.16884131494321322 Test Loss 0.2167961448431015\n",
      "[INFO] - 2024-05-15 17:35:10,568 - UQpy: Scientific Machine Learning: Epoch 210 / 1,000 Train Loss 0.16517573006843267 Test Loss 0.21533146500587463\n",
      "[INFO] - 2024-05-15 17:35:13,799 - UQpy: Scientific Machine Learning: Epoch 211 / 1,000 Train Loss 0.16305457447704516 Test Loss 0.2137857973575592\n",
      "[INFO] - 2024-05-15 17:35:16,987 - UQpy: Scientific Machine Learning: Epoch 212 / 1,000 Train Loss 0.16189945685236076 Test Loss 0.2111685574054718\n",
      "[INFO] - 2024-05-15 17:35:20,157 - UQpy: Scientific Machine Learning: Epoch 213 / 1,000 Train Loss 0.16173552721738815 Test Loss 0.21310460567474365\n",
      "[INFO] - 2024-05-15 17:35:23,398 - UQpy: Scientific Machine Learning: Epoch 214 / 1,000 Train Loss 0.16066437449894452 Test Loss 0.2113286256790161\n",
      "[INFO] - 2024-05-15 17:35:26,654 - UQpy: Scientific Machine Learning: Epoch 215 / 1,000 Train Loss 0.15969736285899816 Test Loss 0.20896412432193756\n",
      "[INFO] - 2024-05-15 17:35:29,940 - UQpy: Scientific Machine Learning: Epoch 216 / 1,000 Train Loss 0.16422093424357867 Test Loss 0.21890974044799805\n",
      "[INFO] - 2024-05-15 17:35:33,125 - UQpy: Scientific Machine Learning: Epoch 217 / 1,000 Train Loss 0.16261562469758487 Test Loss 0.20908299088478088\n",
      "[INFO] - 2024-05-15 17:35:36,276 - UQpy: Scientific Machine Learning: Epoch 218 / 1,000 Train Loss 0.15974966632692436 Test Loss 0.20741069316864014\n",
      "[INFO] - 2024-05-15 17:35:39,467 - UQpy: Scientific Machine Learning: Epoch 219 / 1,000 Train Loss 0.1565980201489047 Test Loss 0.21015331149101257\n",
      "[INFO] - 2024-05-15 17:35:42,626 - UQpy: Scientific Machine Learning: Epoch 220 / 1,000 Train Loss 0.15549228026678688 Test Loss 0.2055772840976715\n",
      "[INFO] - 2024-05-15 17:35:46,016 - UQpy: Scientific Machine Learning: Epoch 221 / 1,000 Train Loss 0.15530113797438772 Test Loss 0.20538906753063202\n",
      "[INFO] - 2024-05-15 17:35:49,459 - UQpy: Scientific Machine Learning: Epoch 222 / 1,000 Train Loss 0.1542190928992472 Test Loss 0.2035958170890808\n",
      "[INFO] - 2024-05-15 17:35:52,645 - UQpy: Scientific Machine Learning: Epoch 223 / 1,000 Train Loss 0.15562567938315242 Test Loss 0.2056637853384018\n",
      "[INFO] - 2024-05-15 17:35:55,841 - UQpy: Scientific Machine Learning: Epoch 224 / 1,000 Train Loss 0.1538485943486816 Test Loss 0.2036946564912796\n",
      "[INFO] - 2024-05-15 17:35:59,007 - UQpy: Scientific Machine Learning: Epoch 225 / 1,000 Train Loss 0.15406056611161484 Test Loss 0.202997624874115\n",
      "[INFO] - 2024-05-15 17:36:02,161 - UQpy: Scientific Machine Learning: Epoch 226 / 1,000 Train Loss 0.1521818308453811 Test Loss 0.20058423280715942\n",
      "[INFO] - 2024-05-15 17:36:05,342 - UQpy: Scientific Machine Learning: Epoch 227 / 1,000 Train Loss 0.152097135390106 Test Loss 0.20759402215480804\n",
      "[INFO] - 2024-05-15 17:36:08,539 - UQpy: Scientific Machine Learning: Epoch 228 / 1,000 Train Loss 0.15345621226649536 Test Loss 0.20238080620765686\n",
      "[INFO] - 2024-05-15 17:36:11,729 - UQpy: Scientific Machine Learning: Epoch 229 / 1,000 Train Loss 0.15191276136197543 Test Loss 0.20551656186580658\n",
      "[INFO] - 2024-05-15 17:36:14,940 - UQpy: Scientific Machine Learning: Epoch 230 / 1,000 Train Loss 0.15303628891706467 Test Loss 0.19868957996368408\n",
      "[INFO] - 2024-05-15 17:36:18,117 - UQpy: Scientific Machine Learning: Epoch 231 / 1,000 Train Loss 0.15062908632190605 Test Loss 0.20259389281272888\n",
      "[INFO] - 2024-05-15 17:36:21,340 - UQpy: Scientific Machine Learning: Epoch 232 / 1,000 Train Loss 0.14878460647244202 Test Loss 0.19703635573387146\n",
      "[INFO] - 2024-05-15 17:36:24,599 - UQpy: Scientific Machine Learning: Epoch 233 / 1,000 Train Loss 0.14769568059005236 Test Loss 0.19613847136497498\n",
      "[INFO] - 2024-05-15 17:36:27,775 - UQpy: Scientific Machine Learning: Epoch 234 / 1,000 Train Loss 0.14927283792119278 Test Loss 0.19695931673049927\n",
      "[INFO] - 2024-05-15 17:36:30,933 - UQpy: Scientific Machine Learning: Epoch 235 / 1,000 Train Loss 0.1482159256150848 Test Loss 0.19578711688518524\n",
      "[INFO] - 2024-05-15 17:36:34,114 - UQpy: Scientific Machine Learning: Epoch 236 / 1,000 Train Loss 0.15072757123332275 Test Loss 0.2027040719985962\n",
      "[INFO] - 2024-05-15 17:36:37,319 - UQpy: Scientific Machine Learning: Epoch 237 / 1,000 Train Loss 0.1488496449432875 Test Loss 0.19832906126976013\n",
      "[INFO] - 2024-05-15 17:36:40,453 - UQpy: Scientific Machine Learning: Epoch 238 / 1,000 Train Loss 0.14702464287218295 Test Loss 0.19363905489444733\n",
      "[INFO] - 2024-05-15 17:36:43,654 - UQpy: Scientific Machine Learning: Epoch 239 / 1,000 Train Loss 0.14559468274053775 Test Loss 0.19417770206928253\n",
      "[INFO] - 2024-05-15 17:36:46,872 - UQpy: Scientific Machine Learning: Epoch 240 / 1,000 Train Loss 0.1513361499497765 Test Loss 0.2094152867794037\n",
      "[INFO] - 2024-05-15 17:36:50,302 - UQpy: Scientific Machine Learning: Epoch 241 / 1,000 Train Loss 0.1577789795241858 Test Loss 0.1963777244091034\n",
      "[INFO] - 2024-05-15 17:36:53,601 - UQpy: Scientific Machine Learning: Epoch 242 / 1,000 Train Loss 0.14648324094320597 Test Loss 0.19349905848503113\n",
      "[INFO] - 2024-05-15 17:36:56,875 - UQpy: Scientific Machine Learning: Epoch 243 / 1,000 Train Loss 0.1461266781154432 Test Loss 0.19124306738376617\n",
      "[INFO] - 2024-05-15 17:37:00,145 - UQpy: Scientific Machine Learning: Epoch 244 / 1,000 Train Loss 0.1427040343221865 Test Loss 0.19282755255699158\n",
      "[INFO] - 2024-05-15 17:37:03,415 - UQpy: Scientific Machine Learning: Epoch 245 / 1,000 Train Loss 0.1430229348571677 Test Loss 0.18971866369247437\n",
      "[INFO] - 2024-05-15 17:37:06,701 - UQpy: Scientific Machine Learning: Epoch 246 / 1,000 Train Loss 0.14379324842440455 Test Loss 0.18990616500377655\n",
      "[INFO] - 2024-05-15 17:37:09,969 - UQpy: Scientific Machine Learning: Epoch 247 / 1,000 Train Loss 0.1420652427171406 Test Loss 0.1909441500902176\n",
      "[INFO] - 2024-05-15 17:37:13,240 - UQpy: Scientific Machine Learning: Epoch 248 / 1,000 Train Loss 0.14158195923817785 Test Loss 0.18756751716136932\n",
      "[INFO] - 2024-05-15 17:37:16,517 - UQpy: Scientific Machine Learning: Epoch 249 / 1,000 Train Loss 0.14046604264723628 Test Loss 0.1869352161884308\n",
      "[INFO] - 2024-05-15 17:37:19,815 - UQpy: Scientific Machine Learning: Epoch 250 / 1,000 Train Loss 0.14049744017814336 Test Loss 0.18929234147071838\n",
      "[INFO] - 2024-05-15 17:37:23,090 - UQpy: Scientific Machine Learning: Epoch 251 / 1,000 Train Loss 0.14144394154611387 Test Loss 0.19098404049873352\n",
      "[INFO] - 2024-05-15 17:37:26,365 - UQpy: Scientific Machine Learning: Epoch 252 / 1,000 Train Loss 0.14100593053980878 Test Loss 0.18763159215450287\n",
      "[INFO] - 2024-05-15 17:37:29,756 - UQpy: Scientific Machine Learning: Epoch 253 / 1,000 Train Loss 0.13857213172473407 Test Loss 0.18503370881080627\n",
      "[INFO] - 2024-05-15 17:37:33,029 - UQpy: Scientific Machine Learning: Epoch 254 / 1,000 Train Loss 0.139876919749536 Test Loss 0.1841157078742981\n",
      "[INFO] - 2024-05-15 17:37:36,318 - UQpy: Scientific Machine Learning: Epoch 255 / 1,000 Train Loss 0.13780964675702548 Test Loss 0.18486428260803223\n",
      "[INFO] - 2024-05-15 17:37:39,602 - UQpy: Scientific Machine Learning: Epoch 256 / 1,000 Train Loss 0.14193214554535716 Test Loss 0.19024278223514557\n",
      "[INFO] - 2024-05-15 17:37:42,891 - UQpy: Scientific Machine Learning: Epoch 257 / 1,000 Train Loss 0.14092907544813657 Test Loss 0.19822971522808075\n",
      "[INFO] - 2024-05-15 17:37:46,162 - UQpy: Scientific Machine Learning: Epoch 258 / 1,000 Train Loss 0.1437716519362048 Test Loss 0.18316423892974854\n",
      "[INFO] - 2024-05-15 17:37:49,449 - UQpy: Scientific Machine Learning: Epoch 259 / 1,000 Train Loss 0.13682860332100014 Test Loss 0.18018358945846558\n",
      "[INFO] - 2024-05-15 17:37:52,724 - UQpy: Scientific Machine Learning: Epoch 260 / 1,000 Train Loss 0.13537775058495372 Test Loss 0.18043003976345062\n",
      "[INFO] - 2024-05-15 17:37:56,044 - UQpy: Scientific Machine Learning: Epoch 261 / 1,000 Train Loss 0.13620210517393916 Test Loss 0.17982271313667297\n",
      "[INFO] - 2024-05-15 17:37:59,321 - UQpy: Scientific Machine Learning: Epoch 262 / 1,000 Train Loss 0.13558482496361984 Test Loss 0.18079067766666412\n",
      "[INFO] - 2024-05-15 17:38:02,592 - UQpy: Scientific Machine Learning: Epoch 263 / 1,000 Train Loss 0.13708404451608658 Test Loss 0.17886114120483398\n",
      "[INFO] - 2024-05-15 17:38:05,889 - UQpy: Scientific Machine Learning: Epoch 264 / 1,000 Train Loss 0.1358666172937343 Test Loss 0.1808595210313797\n",
      "[INFO] - 2024-05-15 17:38:09,208 - UQpy: Scientific Machine Learning: Epoch 265 / 1,000 Train Loss 0.13347352570609042 Test Loss 0.17680497467517853\n",
      "[INFO] - 2024-05-15 17:38:12,478 - UQpy: Scientific Machine Learning: Epoch 266 / 1,000 Train Loss 0.13140750362684853 Test Loss 0.1768500804901123\n",
      "[INFO] - 2024-05-15 17:38:15,755 - UQpy: Scientific Machine Learning: Epoch 267 / 1,000 Train Loss 0.13186794989987424 Test Loss 0.17445875704288483\n",
      "[INFO] - 2024-05-15 17:38:19,007 - UQpy: Scientific Machine Learning: Epoch 268 / 1,000 Train Loss 0.13058030487675415 Test Loss 0.17506250739097595\n",
      "[INFO] - 2024-05-15 17:38:22,289 - UQpy: Scientific Machine Learning: Epoch 269 / 1,000 Train Loss 0.13320920341893247 Test Loss 0.1715577095746994\n",
      "[INFO] - 2024-05-15 17:38:25,550 - UQpy: Scientific Machine Learning: Epoch 270 / 1,000 Train Loss 0.13165449625567385 Test Loss 0.17077848315238953\n",
      "[INFO] - 2024-05-15 17:38:28,840 - UQpy: Scientific Machine Learning: Epoch 271 / 1,000 Train Loss 0.1319230751771676 Test Loss 0.17398911714553833\n",
      "[INFO] - 2024-05-15 17:38:32,058 - UQpy: Scientific Machine Learning: Epoch 272 / 1,000 Train Loss 0.1299828102714137 Test Loss 0.17493586242198944\n",
      "[INFO] - 2024-05-15 17:38:35,219 - UQpy: Scientific Machine Learning: Epoch 273 / 1,000 Train Loss 0.12858148585808904 Test Loss 0.1675437092781067\n",
      "[INFO] - 2024-05-15 17:38:38,276 - UQpy: Scientific Machine Learning: Epoch 274 / 1,000 Train Loss 0.12640829149045443 Test Loss 0.16694660484790802\n",
      "[INFO] - 2024-05-15 17:38:41,452 - UQpy: Scientific Machine Learning: Epoch 275 / 1,000 Train Loss 0.12681847732318074 Test Loss 0.1694318652153015\n",
      "[INFO] - 2024-05-15 17:38:44,641 - UQpy: Scientific Machine Learning: Epoch 276 / 1,000 Train Loss 0.12651372936211133 Test Loss 0.1644987165927887\n",
      "[INFO] - 2024-05-15 17:38:48,251 - UQpy: Scientific Machine Learning: Epoch 277 / 1,000 Train Loss 0.12401415799793444 Test Loss 0.1698714941740036\n",
      "[INFO] - 2024-05-15 17:38:51,497 - UQpy: Scientific Machine Learning: Epoch 278 / 1,000 Train Loss 0.1251516397062101 Test Loss 0.16267752647399902\n",
      "[INFO] - 2024-05-15 17:38:54,761 - UQpy: Scientific Machine Learning: Epoch 279 / 1,000 Train Loss 0.12259416007681896 Test Loss 0.16264386475086212\n",
      "[INFO] - 2024-05-15 17:38:58,082 - UQpy: Scientific Machine Learning: Epoch 280 / 1,000 Train Loss 0.12191220098420193 Test Loss 0.16089898347854614\n",
      "[INFO] - 2024-05-15 17:39:01,317 - UQpy: Scientific Machine Learning: Epoch 281 / 1,000 Train Loss 0.12138312976611287 Test Loss 0.15918375551700592\n",
      "[INFO] - 2024-05-15 17:39:04,521 - UQpy: Scientific Machine Learning: Epoch 282 / 1,000 Train Loss 0.12033955674422414 Test Loss 0.15893012285232544\n",
      "[INFO] - 2024-05-15 17:39:07,698 - UQpy: Scientific Machine Learning: Epoch 283 / 1,000 Train Loss 0.12013711192105946 Test Loss 0.1647951602935791\n",
      "[INFO] - 2024-05-15 17:39:10,887 - UQpy: Scientific Machine Learning: Epoch 284 / 1,000 Train Loss 0.12267808145598362 Test Loss 0.15835438668727875\n",
      "[INFO] - 2024-05-15 17:39:14,080 - UQpy: Scientific Machine Learning: Epoch 285 / 1,000 Train Loss 0.12151261811193667 Test Loss 0.16122688353061676\n",
      "[INFO] - 2024-05-15 17:39:17,291 - UQpy: Scientific Machine Learning: Epoch 286 / 1,000 Train Loss 0.11871922369066037 Test Loss 0.16052278876304626\n",
      "[INFO] - 2024-05-15 17:39:20,525 - UQpy: Scientific Machine Learning: Epoch 287 / 1,000 Train Loss 0.11898006106677808 Test Loss 0.15879209339618683\n",
      "[INFO] - 2024-05-15 17:39:23,785 - UQpy: Scientific Machine Learning: Epoch 288 / 1,000 Train Loss 0.11892834619471901 Test Loss 0.15341119468212128\n",
      "[INFO] - 2024-05-15 17:39:26,953 - UQpy: Scientific Machine Learning: Epoch 289 / 1,000 Train Loss 0.11903827127657439 Test Loss 0.1577216535806656\n",
      "[INFO] - 2024-05-15 17:39:30,069 - UQpy: Scientific Machine Learning: Epoch 290 / 1,000 Train Loss 0.1167097287742715 Test Loss 0.15183427929878235\n",
      "[INFO] - 2024-05-15 17:39:33,264 - UQpy: Scientific Machine Learning: Epoch 291 / 1,000 Train Loss 0.1146002427527779 Test Loss 0.15154793858528137\n",
      "[INFO] - 2024-05-15 17:39:36,440 - UQpy: Scientific Machine Learning: Epoch 292 / 1,000 Train Loss 0.11524491717940882 Test Loss 0.15600468218326569\n",
      "[INFO] - 2024-05-15 17:39:39,609 - UQpy: Scientific Machine Learning: Epoch 293 / 1,000 Train Loss 0.1155067807749698 Test Loss 0.1504172384738922\n",
      "[INFO] - 2024-05-15 17:39:42,831 - UQpy: Scientific Machine Learning: Epoch 294 / 1,000 Train Loss 0.11296817503477398 Test Loss 0.15170222520828247\n",
      "[INFO] - 2024-05-15 17:39:46,062 - UQpy: Scientific Machine Learning: Epoch 295 / 1,000 Train Loss 0.11275553350385867 Test Loss 0.14831256866455078\n",
      "[INFO] - 2024-05-15 17:39:49,295 - UQpy: Scientific Machine Learning: Epoch 296 / 1,000 Train Loss 0.11593175326523028 Test Loss 0.15182466804981232\n",
      "[INFO] - 2024-05-15 17:39:52,581 - UQpy: Scientific Machine Learning: Epoch 297 / 1,000 Train Loss 0.1161268086809861 Test Loss 0.15003791451454163\n",
      "[INFO] - 2024-05-15 17:39:55,779 - UQpy: Scientific Machine Learning: Epoch 298 / 1,000 Train Loss 0.11183756825171019 Test Loss 0.14697033166885376\n",
      "[INFO] - 2024-05-15 17:39:58,959 - UQpy: Scientific Machine Learning: Epoch 299 / 1,000 Train Loss 0.11081128567457199 Test Loss 0.14811143279075623\n",
      "[INFO] - 2024-05-15 17:40:02,215 - UQpy: Scientific Machine Learning: Epoch 300 / 1,000 Train Loss 0.11019969064938395 Test Loss 0.1450318992137909\n",
      "[INFO] - 2024-05-15 17:40:05,501 - UQpy: Scientific Machine Learning: Epoch 301 / 1,000 Train Loss 0.11090780284843947 Test Loss 0.1450643241405487\n",
      "[INFO] - 2024-05-15 17:40:08,789 - UQpy: Scientific Machine Learning: Epoch 302 / 1,000 Train Loss 0.11235055484269794 Test Loss 0.14583367109298706\n",
      "[INFO] - 2024-05-15 17:40:12,111 - UQpy: Scientific Machine Learning: Epoch 303 / 1,000 Train Loss 0.11177186550278413 Test Loss 0.15215206146240234\n",
      "[INFO] - 2024-05-15 17:40:15,269 - UQpy: Scientific Machine Learning: Epoch 304 / 1,000 Train Loss 0.11251698081430636 Test Loss 0.14433985948562622\n",
      "[INFO] - 2024-05-15 17:40:18,449 - UQpy: Scientific Machine Learning: Epoch 305 / 1,000 Train Loss 0.11174728878234562 Test Loss 0.1437884122133255\n",
      "[INFO] - 2024-05-15 17:40:21,641 - UQpy: Scientific Machine Learning: Epoch 306 / 1,000 Train Loss 0.1092627750415551 Test Loss 0.1438983827829361\n",
      "[INFO] - 2024-05-15 17:40:24,806 - UQpy: Scientific Machine Learning: Epoch 307 / 1,000 Train Loss 0.10991786773267545 Test Loss 0.14378532767295837\n",
      "[INFO] - 2024-05-15 17:40:27,964 - UQpy: Scientific Machine Learning: Epoch 308 / 1,000 Train Loss 0.11035990558172527 Test Loss 0.14475199580192566\n",
      "[INFO] - 2024-05-15 17:40:31,239 - UQpy: Scientific Machine Learning: Epoch 309 / 1,000 Train Loss 0.10892112592333242 Test Loss 0.14277596771717072\n",
      "[INFO] - 2024-05-15 17:40:34,451 - UQpy: Scientific Machine Learning: Epoch 310 / 1,000 Train Loss 0.1080537530544557 Test Loss 0.14803197979927063\n",
      "[INFO] - 2024-05-15 17:40:37,663 - UQpy: Scientific Machine Learning: Epoch 311 / 1,000 Train Loss 0.1102208012813016 Test Loss 0.14287534356117249\n",
      "[INFO] - 2024-05-15 17:40:40,902 - UQpy: Scientific Machine Learning: Epoch 312 / 1,000 Train Loss 0.1069848474703337 Test Loss 0.13954168558120728\n",
      "[INFO] - 2024-05-15 17:40:44,131 - UQpy: Scientific Machine Learning: Epoch 313 / 1,000 Train Loss 0.10648933288298155 Test Loss 0.13946981728076935\n",
      "[INFO] - 2024-05-15 17:40:47,336 - UQpy: Scientific Machine Learning: Epoch 314 / 1,000 Train Loss 0.10734532930349049 Test Loss 0.13950678706169128\n",
      "[INFO] - 2024-05-15 17:40:50,664 - UQpy: Scientific Machine Learning: Epoch 315 / 1,000 Train Loss 0.1073645525857022 Test Loss 0.1402398943901062\n",
      "[INFO] - 2024-05-15 17:40:53,953 - UQpy: Scientific Machine Learning: Epoch 316 / 1,000 Train Loss 0.11000359136807292 Test Loss 0.14423947036266327\n",
      "[INFO] - 2024-05-15 17:40:57,163 - UQpy: Scientific Machine Learning: Epoch 317 / 1,000 Train Loss 0.11045712311016886 Test Loss 0.13955824077129364\n",
      "[INFO] - 2024-05-15 17:41:00,326 - UQpy: Scientific Machine Learning: Epoch 318 / 1,000 Train Loss 0.10597371309995651 Test Loss 0.13734948635101318\n",
      "[INFO] - 2024-05-15 17:41:03,494 - UQpy: Scientific Machine Learning: Epoch 319 / 1,000 Train Loss 0.10417790710926056 Test Loss 0.13761483132839203\n",
      "[INFO] - 2024-05-15 17:41:06,667 - UQpy: Scientific Machine Learning: Epoch 320 / 1,000 Train Loss 0.10582663550188667 Test Loss 0.1378096342086792\n",
      "[INFO] - 2024-05-15 17:41:09,839 - UQpy: Scientific Machine Learning: Epoch 321 / 1,000 Train Loss 0.1054548201592345 Test Loss 0.13654698431491852\n",
      "[INFO] - 2024-05-15 17:41:12,988 - UQpy: Scientific Machine Learning: Epoch 322 / 1,000 Train Loss 0.10528036206960678 Test Loss 0.1407965123653412\n",
      "[INFO] - 2024-05-15 17:41:16,152 - UQpy: Scientific Machine Learning: Epoch 323 / 1,000 Train Loss 0.10484487445730913 Test Loss 0.13525551557540894\n",
      "[INFO] - 2024-05-15 17:41:19,336 - UQpy: Scientific Machine Learning: Epoch 324 / 1,000 Train Loss 0.10401285478943273 Test Loss 0.13646596670150757\n",
      "[INFO] - 2024-05-15 17:41:22,503 - UQpy: Scientific Machine Learning: Epoch 325 / 1,000 Train Loss 0.10779361034694471 Test Loss 0.14066186547279358\n",
      "[INFO] - 2024-05-15 17:41:25,687 - UQpy: Scientific Machine Learning: Epoch 326 / 1,000 Train Loss 0.1091269682112493 Test Loss 0.1377873569726944\n",
      "[INFO] - 2024-05-15 17:41:28,858 - UQpy: Scientific Machine Learning: Epoch 327 / 1,000 Train Loss 0.10626249211399179 Test Loss 0.13651743531227112\n",
      "[INFO] - 2024-05-15 17:41:32,033 - UQpy: Scientific Machine Learning: Epoch 328 / 1,000 Train Loss 0.10360940937933169 Test Loss 0.14292582869529724\n",
      "[INFO] - 2024-05-15 17:41:35,214 - UQpy: Scientific Machine Learning: Epoch 329 / 1,000 Train Loss 0.10490585314600091 Test Loss 0.13674157857894897\n",
      "[INFO] - 2024-05-15 17:41:38,381 - UQpy: Scientific Machine Learning: Epoch 330 / 1,000 Train Loss 0.10391219822983992 Test Loss 0.1349063664674759\n",
      "[INFO] - 2024-05-15 17:41:41,538 - UQpy: Scientific Machine Learning: Epoch 331 / 1,000 Train Loss 0.10198370837851574 Test Loss 0.13582681119441986\n",
      "[INFO] - 2024-05-15 17:41:44,758 - UQpy: Scientific Machine Learning: Epoch 332 / 1,000 Train Loss 0.10439174426229376 Test Loss 0.1354527473449707\n",
      "[INFO] - 2024-05-15 17:41:47,909 - UQpy: Scientific Machine Learning: Epoch 333 / 1,000 Train Loss 0.10155117943098671 Test Loss 0.13334345817565918\n",
      "[INFO] - 2024-05-15 17:41:51,076 - UQpy: Scientific Machine Learning: Epoch 334 / 1,000 Train Loss 0.10115096561218563 Test Loss 0.1318228840827942\n",
      "[INFO] - 2024-05-15 17:41:54,252 - UQpy: Scientific Machine Learning: Epoch 335 / 1,000 Train Loss 0.10147961346726668 Test Loss 0.134924054145813\n",
      "[INFO] - 2024-05-15 17:41:57,429 - UQpy: Scientific Machine Learning: Epoch 336 / 1,000 Train Loss 0.10231969858470716 Test Loss 0.14755389094352722\n",
      "[INFO] - 2024-05-15 17:42:00,595 - UQpy: Scientific Machine Learning: Epoch 337 / 1,000 Train Loss 0.10453807052813079 Test Loss 0.1334170401096344\n",
      "[INFO] - 2024-05-15 17:42:03,775 - UQpy: Scientific Machine Learning: Epoch 338 / 1,000 Train Loss 0.10120302125027306 Test Loss 0.13362374901771545\n",
      "[INFO] - 2024-05-15 17:42:07,043 - UQpy: Scientific Machine Learning: Epoch 339 / 1,000 Train Loss 0.10043050231117952 Test Loss 0.1303623765707016\n",
      "[INFO] - 2024-05-15 17:42:10,307 - UQpy: Scientific Machine Learning: Epoch 340 / 1,000 Train Loss 0.10127966223578704 Test Loss 0.13091091811656952\n",
      "[INFO] - 2024-05-15 17:42:13,568 - UQpy: Scientific Machine Learning: Epoch 341 / 1,000 Train Loss 0.10015922235815149 Test Loss 0.13161176443099976\n",
      "[INFO] - 2024-05-15 17:42:16,831 - UQpy: Scientific Machine Learning: Epoch 342 / 1,000 Train Loss 0.09950912077175944 Test Loss 0.12923790514469147\n",
      "[INFO] - 2024-05-15 17:42:20,088 - UQpy: Scientific Machine Learning: Epoch 343 / 1,000 Train Loss 0.09937697845069986 Test Loss 0.13043542206287384\n",
      "[INFO] - 2024-05-15 17:42:23,360 - UQpy: Scientific Machine Learning: Epoch 344 / 1,000 Train Loss 0.09978926260220378 Test Loss 0.13512742519378662\n",
      "[INFO] - 2024-05-15 17:42:26,616 - UQpy: Scientific Machine Learning: Epoch 345 / 1,000 Train Loss 0.09976909858615775 Test Loss 0.12894918024539948\n",
      "[INFO] - 2024-05-15 17:42:29,889 - UQpy: Scientific Machine Learning: Epoch 346 / 1,000 Train Loss 0.09948097796816575 Test Loss 0.12855693697929382\n",
      "[INFO] - 2024-05-15 17:42:33,154 - UQpy: Scientific Machine Learning: Epoch 347 / 1,000 Train Loss 0.10126256864321859 Test Loss 0.13390852510929108\n",
      "[INFO] - 2024-05-15 17:42:36,398 - UQpy: Scientific Machine Learning: Epoch 348 / 1,000 Train Loss 0.10281770127384286 Test Loss 0.13696818053722382\n",
      "[INFO] - 2024-05-15 17:42:39,652 - UQpy: Scientific Machine Learning: Epoch 349 / 1,000 Train Loss 0.10425586371045363 Test Loss 0.13433422148227692\n",
      "[INFO] - 2024-05-15 17:42:42,923 - UQpy: Scientific Machine Learning: Epoch 350 / 1,000 Train Loss 0.09998034568209398 Test Loss 0.12971678376197815\n",
      "[INFO] - 2024-05-15 17:42:46,167 - UQpy: Scientific Machine Learning: Epoch 351 / 1,000 Train Loss 0.09900809197049391 Test Loss 0.12790730595588684\n",
      "[INFO] - 2024-05-15 17:42:49,562 - UQpy: Scientific Machine Learning: Epoch 352 / 1,000 Train Loss 0.09831848936645608 Test Loss 0.12745143473148346\n",
      "[INFO] - 2024-05-15 17:42:52,897 - UQpy: Scientific Machine Learning: Epoch 353 / 1,000 Train Loss 0.09821952369652297 Test Loss 0.12973381578922272\n",
      "[INFO] - 2024-05-15 17:42:56,232 - UQpy: Scientific Machine Learning: Epoch 354 / 1,000 Train Loss 0.09973529765480443 Test Loss 0.12827570736408234\n",
      "[INFO] - 2024-05-15 17:42:59,597 - UQpy: Scientific Machine Learning: Epoch 355 / 1,000 Train Loss 0.0978076591303474 Test Loss 0.12880970537662506\n",
      "[INFO] - 2024-05-15 17:43:02,989 - UQpy: Scientific Machine Learning: Epoch 356 / 1,000 Train Loss 0.09803476263033717 Test Loss 0.12592655420303345\n",
      "[INFO] - 2024-05-15 17:43:06,263 - UQpy: Scientific Machine Learning: Epoch 357 / 1,000 Train Loss 0.09588194246354856 Test Loss 0.12530232965946198\n",
      "[INFO] - 2024-05-15 17:43:09,561 - UQpy: Scientific Machine Learning: Epoch 358 / 1,000 Train Loss 0.10008191003611214 Test Loss 0.12604199349880219\n",
      "[INFO] - 2024-05-15 17:43:12,854 - UQpy: Scientific Machine Learning: Epoch 359 / 1,000 Train Loss 0.09603199362754822 Test Loss 0.12620410323143005\n",
      "[INFO] - 2024-05-15 17:43:16,139 - UQpy: Scientific Machine Learning: Epoch 360 / 1,000 Train Loss 0.09636384836937252 Test Loss 0.12622526288032532\n",
      "[INFO] - 2024-05-15 17:43:19,467 - UQpy: Scientific Machine Learning: Epoch 361 / 1,000 Train Loss 0.09563002735376358 Test Loss 0.1258063018321991\n",
      "[INFO] - 2024-05-15 17:43:22,781 - UQpy: Scientific Machine Learning: Epoch 362 / 1,000 Train Loss 0.09616448102813017 Test Loss 0.124647356569767\n",
      "[INFO] - 2024-05-15 17:43:26,073 - UQpy: Scientific Machine Learning: Epoch 363 / 1,000 Train Loss 0.09648727076618295 Test Loss 0.12749803066253662\n",
      "[INFO] - 2024-05-15 17:43:29,383 - UQpy: Scientific Machine Learning: Epoch 364 / 1,000 Train Loss 0.0989401281664246 Test Loss 0.1261986643075943\n",
      "[INFO] - 2024-05-15 17:43:32,673 - UQpy: Scientific Machine Learning: Epoch 365 / 1,000 Train Loss 0.09675065859367973 Test Loss 0.1262928694486618\n",
      "[INFO] - 2024-05-15 17:43:35,945 - UQpy: Scientific Machine Learning: Epoch 366 / 1,000 Train Loss 0.10182136689361773 Test Loss 0.14733509719371796\n",
      "[INFO] - 2024-05-15 17:43:39,213 - UQpy: Scientific Machine Learning: Epoch 367 / 1,000 Train Loss 0.09801370415248369 Test Loss 0.12724915146827698\n",
      "[INFO] - 2024-05-15 17:43:42,486 - UQpy: Scientific Machine Learning: Epoch 368 / 1,000 Train Loss 0.09606855735182762 Test Loss 0.12274961918592453\n",
      "[INFO] - 2024-05-15 17:43:45,749 - UQpy: Scientific Machine Learning: Epoch 369 / 1,000 Train Loss 0.0937574858728208 Test Loss 0.12252699583768845\n",
      "[INFO] - 2024-05-15 17:43:49,058 - UQpy: Scientific Machine Learning: Epoch 370 / 1,000 Train Loss 0.09537400030776073 Test Loss 0.12223358452320099\n",
      "[INFO] - 2024-05-15 17:43:52,334 - UQpy: Scientific Machine Learning: Epoch 371 / 1,000 Train Loss 0.09529119298646324 Test Loss 0.12178590893745422\n",
      "[INFO] - 2024-05-15 17:43:55,608 - UQpy: Scientific Machine Learning: Epoch 372 / 1,000 Train Loss 0.09349172170224943 Test Loss 0.12089353799819946\n",
      "[INFO] - 2024-05-15 17:43:58,909 - UQpy: Scientific Machine Learning: Epoch 373 / 1,000 Train Loss 0.0926705751764147 Test Loss 0.12201789021492004\n",
      "[INFO] - 2024-05-15 17:44:02,198 - UQpy: Scientific Machine Learning: Epoch 374 / 1,000 Train Loss 0.09421848388094652 Test Loss 0.12179075926542282\n",
      "[INFO] - 2024-05-15 17:44:05,476 - UQpy: Scientific Machine Learning: Epoch 375 / 1,000 Train Loss 0.09307524836377094 Test Loss 0.12120235711336136\n",
      "[INFO] - 2024-05-15 17:44:08,752 - UQpy: Scientific Machine Learning: Epoch 376 / 1,000 Train Loss 0.09332294526853059 Test Loss 0.12170663475990295\n",
      "[INFO] - 2024-05-15 17:44:12,027 - UQpy: Scientific Machine Learning: Epoch 377 / 1,000 Train Loss 0.0948540709520641 Test Loss 0.13170528411865234\n",
      "[INFO] - 2024-05-15 17:44:15,286 - UQpy: Scientific Machine Learning: Epoch 378 / 1,000 Train Loss 0.09759722846119027 Test Loss 0.12284120917320251\n",
      "[INFO] - 2024-05-15 17:44:18,563 - UQpy: Scientific Machine Learning: Epoch 379 / 1,000 Train Loss 0.09852066989007749 Test Loss 0.12372155487537384\n",
      "[INFO] - 2024-05-15 17:44:21,835 - UQpy: Scientific Machine Learning: Epoch 380 / 1,000 Train Loss 0.09444233971206765 Test Loss 0.12054133415222168\n",
      "[INFO] - 2024-05-15 17:44:25,134 - UQpy: Scientific Machine Learning: Epoch 381 / 1,000 Train Loss 0.09251449845339123 Test Loss 0.12037847191095352\n",
      "[INFO] - 2024-05-15 17:44:28,456 - UQpy: Scientific Machine Learning: Epoch 382 / 1,000 Train Loss 0.09277961599199396 Test Loss 0.12394946813583374\n",
      "[INFO] - 2024-05-15 17:44:31,754 - UQpy: Scientific Machine Learning: Epoch 383 / 1,000 Train Loss 0.09233973881131724 Test Loss 0.1186521053314209\n",
      "[INFO] - 2024-05-15 17:44:35,039 - UQpy: Scientific Machine Learning: Epoch 384 / 1,000 Train Loss 0.0934139086227668 Test Loss 0.12194710969924927\n",
      "[INFO] - 2024-05-15 17:44:38,289 - UQpy: Scientific Machine Learning: Epoch 385 / 1,000 Train Loss 0.09426306737096686 Test Loss 0.12089845538139343\n",
      "[INFO] - 2024-05-15 17:44:41,493 - UQpy: Scientific Machine Learning: Epoch 386 / 1,000 Train Loss 0.09260690055395428 Test Loss 0.11958449333906174\n",
      "[INFO] - 2024-05-15 17:44:44,854 - UQpy: Scientific Machine Learning: Epoch 387 / 1,000 Train Loss 0.09285010240579906 Test Loss 0.11878856271505356\n",
      "[INFO] - 2024-05-15 17:44:48,012 - UQpy: Scientific Machine Learning: Epoch 388 / 1,000 Train Loss 0.09152497194315258 Test Loss 0.1186540424823761\n",
      "[INFO] - 2024-05-15 17:44:51,237 - UQpy: Scientific Machine Learning: Epoch 389 / 1,000 Train Loss 0.09185736351891567 Test Loss 0.12076524645090103\n",
      "[INFO] - 2024-05-15 17:44:54,409 - UQpy: Scientific Machine Learning: Epoch 390 / 1,000 Train Loss 0.09090566870413329 Test Loss 0.11838115751743317\n",
      "[INFO] - 2024-05-15 17:44:57,544 - UQpy: Scientific Machine Learning: Epoch 391 / 1,000 Train Loss 0.09358102436128415 Test Loss 0.11717749387025833\n",
      "[INFO] - 2024-05-15 17:45:00,813 - UQpy: Scientific Machine Learning: Epoch 392 / 1,000 Train Loss 0.09059262236482218 Test Loss 0.11747553944587708\n",
      "[INFO] - 2024-05-15 17:45:04,063 - UQpy: Scientific Machine Learning: Epoch 393 / 1,000 Train Loss 0.09054838434646004 Test Loss 0.11576325446367264\n",
      "[INFO] - 2024-05-15 17:45:07,243 - UQpy: Scientific Machine Learning: Epoch 394 / 1,000 Train Loss 0.09079106152057648 Test Loss 0.11603949964046478\n",
      "[INFO] - 2024-05-15 17:45:10,493 - UQpy: Scientific Machine Learning: Epoch 395 / 1,000 Train Loss 0.0900932495531283 Test Loss 0.11596743762493134\n",
      "[INFO] - 2024-05-15 17:45:13,724 - UQpy: Scientific Machine Learning: Epoch 396 / 1,000 Train Loss 0.08936464315966557 Test Loss 0.11730688810348511\n",
      "[INFO] - 2024-05-15 17:45:16,917 - UQpy: Scientific Machine Learning: Epoch 397 / 1,000 Train Loss 0.08977964794949482 Test Loss 0.11758462339639664\n",
      "[INFO] - 2024-05-15 17:45:20,069 - UQpy: Scientific Machine Learning: Epoch 398 / 1,000 Train Loss 0.08969583911331076 Test Loss 0.11589387059211731\n",
      "[INFO] - 2024-05-15 17:45:23,317 - UQpy: Scientific Machine Learning: Epoch 399 / 1,000 Train Loss 0.09243735200480412 Test Loss 0.11680243164300919\n",
      "[INFO] - 2024-05-15 17:45:26,576 - UQpy: Scientific Machine Learning: Epoch 400 / 1,000 Train Loss 0.09872848697398838 Test Loss 0.12095877528190613\n",
      "[INFO] - 2024-05-15 17:45:29,753 - UQpy: Scientific Machine Learning: Epoch 401 / 1,000 Train Loss 0.09632352425863869 Test Loss 0.11794650554656982\n",
      "[INFO] - 2024-05-15 17:45:32,966 - UQpy: Scientific Machine Learning: Epoch 402 / 1,000 Train Loss 0.09047019246377443 Test Loss 0.11966151744127274\n",
      "[INFO] - 2024-05-15 17:45:36,223 - UQpy: Scientific Machine Learning: Epoch 403 / 1,000 Train Loss 0.08922167710567776 Test Loss 0.11635071784257889\n",
      "[INFO] - 2024-05-15 17:45:39,440 - UQpy: Scientific Machine Learning: Epoch 404 / 1,000 Train Loss 0.08895590470025413 Test Loss 0.1147514134645462\n",
      "[INFO] - 2024-05-15 17:45:42,644 - UQpy: Scientific Machine Learning: Epoch 405 / 1,000 Train Loss 0.08813678512447759 Test Loss 0.11444313079118729\n",
      "[INFO] - 2024-05-15 17:45:45,861 - UQpy: Scientific Machine Learning: Epoch 406 / 1,000 Train Loss 0.08712110786061537 Test Loss 0.11210106313228607\n",
      "[INFO] - 2024-05-15 17:45:49,209 - UQpy: Scientific Machine Learning: Epoch 407 / 1,000 Train Loss 0.08633370070081008 Test Loss 0.1127387136220932\n",
      "[INFO] - 2024-05-15 17:45:52,415 - UQpy: Scientific Machine Learning: Epoch 408 / 1,000 Train Loss 0.08709280859482915 Test Loss 0.11263945698738098\n",
      "[INFO] - 2024-05-15 17:45:55,666 - UQpy: Scientific Machine Learning: Epoch 409 / 1,000 Train Loss 0.08690841927340157 Test Loss 0.11383888125419617\n",
      "[INFO] - 2024-05-15 17:45:58,933 - UQpy: Scientific Machine Learning: Epoch 410 / 1,000 Train Loss 0.0864352866222984 Test Loss 0.11215396225452423\n",
      "[INFO] - 2024-05-15 17:46:02,122 - UQpy: Scientific Machine Learning: Epoch 411 / 1,000 Train Loss 0.0858940472336192 Test Loss 0.1122887134552002\n",
      "[INFO] - 2024-05-15 17:46:05,290 - UQpy: Scientific Machine Learning: Epoch 412 / 1,000 Train Loss 0.08768019040948466 Test Loss 0.11247646063566208\n",
      "[INFO] - 2024-05-15 17:46:08,503 - UQpy: Scientific Machine Learning: Epoch 413 / 1,000 Train Loss 0.08683708310127258 Test Loss 0.11165747046470642\n",
      "[INFO] - 2024-05-15 17:46:11,780 - UQpy: Scientific Machine Learning: Epoch 414 / 1,000 Train Loss 0.08973454546771552 Test Loss 0.11378315091133118\n",
      "[INFO] - 2024-05-15 17:46:15,116 - UQpy: Scientific Machine Learning: Epoch 415 / 1,000 Train Loss 0.08950011236102957 Test Loss 0.1140437126159668\n",
      "[INFO] - 2024-05-15 17:46:18,385 - UQpy: Scientific Machine Learning: Epoch 416 / 1,000 Train Loss 0.0865276224519077 Test Loss 0.11105266958475113\n",
      "[INFO] - 2024-05-15 17:46:21,666 - UQpy: Scientific Machine Learning: Epoch 417 / 1,000 Train Loss 0.0850459074503497 Test Loss 0.11015257239341736\n",
      "[INFO] - 2024-05-15 17:46:24,949 - UQpy: Scientific Machine Learning: Epoch 418 / 1,000 Train Loss 0.08789025600019254 Test Loss 0.11387874186038971\n",
      "[INFO] - 2024-05-15 17:46:28,224 - UQpy: Scientific Machine Learning: Epoch 419 / 1,000 Train Loss 0.08912204833407152 Test Loss 0.11286826431751251\n",
      "[INFO] - 2024-05-15 17:46:31,385 - UQpy: Scientific Machine Learning: Epoch 420 / 1,000 Train Loss 0.08590272970889744 Test Loss 0.11065662652254105\n",
      "[INFO] - 2024-05-15 17:46:34,588 - UQpy: Scientific Machine Learning: Epoch 421 / 1,000 Train Loss 0.08662014885952599 Test Loss 0.11043841391801834\n",
      "[INFO] - 2024-05-15 17:46:37,925 - UQpy: Scientific Machine Learning: Epoch 422 / 1,000 Train Loss 0.08479279769878638 Test Loss 0.10892155766487122\n",
      "[INFO] - 2024-05-15 17:46:41,111 - UQpy: Scientific Machine Learning: Epoch 423 / 1,000 Train Loss 0.08437228359674152 Test Loss 0.10894587635993958\n",
      "[INFO] - 2024-05-15 17:46:49,574 - UQpy: Scientific Machine Learning: Epoch 424 / 1,000 Train Loss 0.08445906717526286 Test Loss 0.11201189458370209\n",
      "[INFO] - 2024-05-15 17:46:53,038 - UQpy: Scientific Machine Learning: Epoch 425 / 1,000 Train Loss 0.08757211581656807 Test Loss 0.11182720214128494\n",
      "[INFO] - 2024-05-15 17:46:56,318 - UQpy: Scientific Machine Learning: Epoch 426 / 1,000 Train Loss 0.08734399041062907 Test Loss 0.10872745513916016\n",
      "[INFO] - 2024-05-15 17:46:59,619 - UQpy: Scientific Machine Learning: Epoch 427 / 1,000 Train Loss 0.08441460916870519 Test Loss 0.1105356514453888\n",
      "[INFO] - 2024-05-15 17:47:02,869 - UQpy: Scientific Machine Learning: Epoch 428 / 1,000 Train Loss 0.08869284901179765 Test Loss 0.11098168045282364\n",
      "[INFO] - 2024-05-15 17:47:06,082 - UQpy: Scientific Machine Learning: Epoch 429 / 1,000 Train Loss 0.09237935825398094 Test Loss 0.11282920837402344\n",
      "[INFO] - 2024-05-15 17:47:09,812 - UQpy: Scientific Machine Learning: Epoch 430 / 1,000 Train Loss 0.09404831065943367 Test Loss 0.12015786021947861\n",
      "[INFO] - 2024-05-15 17:47:13,103 - UQpy: Scientific Machine Learning: Epoch 431 / 1,000 Train Loss 0.09088463846005891 Test Loss 0.11121624708175659\n",
      "[INFO] - 2024-05-15 17:47:16,403 - UQpy: Scientific Machine Learning: Epoch 432 / 1,000 Train Loss 0.08431328872316762 Test Loss 0.10714886337518692\n",
      "[INFO] - 2024-05-15 17:47:20,140 - UQpy: Scientific Machine Learning: Epoch 433 / 1,000 Train Loss 0.08506849506183674 Test Loss 0.11092197895050049\n",
      "[INFO] - 2024-05-15 17:47:23,450 - UQpy: Scientific Machine Learning: Epoch 434 / 1,000 Train Loss 0.08617809532504332 Test Loss 0.10935719311237335\n",
      "[INFO] - 2024-05-15 17:47:27,035 - UQpy: Scientific Machine Learning: Epoch 435 / 1,000 Train Loss 0.08624016258277391 Test Loss 0.10772426426410675\n",
      "[INFO] - 2024-05-15 17:47:30,339 - UQpy: Scientific Machine Learning: Epoch 436 / 1,000 Train Loss 0.08187826566006008 Test Loss 0.1050419807434082\n",
      "[INFO] - 2024-05-15 17:47:33,660 - UQpy: Scientific Machine Learning: Epoch 437 / 1,000 Train Loss 0.0818295745473159 Test Loss 0.10491973906755447\n",
      "[INFO] - 2024-05-15 17:47:36,968 - UQpy: Scientific Machine Learning: Epoch 438 / 1,000 Train Loss 0.08221565088943432 Test Loss 0.10469481348991394\n",
      "[INFO] - 2024-05-15 17:47:40,261 - UQpy: Scientific Machine Learning: Epoch 439 / 1,000 Train Loss 0.08124742068742451 Test Loss 0.10426974296569824\n",
      "[INFO] - 2024-05-15 17:47:43,541 - UQpy: Scientific Machine Learning: Epoch 440 / 1,000 Train Loss 0.08156841169846685 Test Loss 0.10427737236022949\n",
      "[INFO] - 2024-05-15 17:47:46,831 - UQpy: Scientific Machine Learning: Epoch 441 / 1,000 Train Loss 0.0823084196369899 Test Loss 0.1080562174320221\n",
      "[INFO] - 2024-05-15 17:47:50,117 - UQpy: Scientific Machine Learning: Epoch 442 / 1,000 Train Loss 0.08210974421940352 Test Loss 0.10602199286222458\n",
      "[INFO] - 2024-05-15 17:47:53,416 - UQpy: Scientific Machine Learning: Epoch 443 / 1,000 Train Loss 0.08078991562912338 Test Loss 0.10411247611045837\n",
      "[INFO] - 2024-05-15 17:47:56,713 - UQpy: Scientific Machine Learning: Epoch 444 / 1,000 Train Loss 0.08042348960512563 Test Loss 0.1028890460729599\n",
      "[INFO] - 2024-05-15 17:48:00,041 - UQpy: Scientific Machine Learning: Epoch 445 / 1,000 Train Loss 0.08121562553079505 Test Loss 0.10401693731546402\n",
      "[INFO] - 2024-05-15 17:48:03,326 - UQpy: Scientific Machine Learning: Epoch 446 / 1,000 Train Loss 0.08181213999265119 Test Loss 0.1052437424659729\n",
      "[INFO] - 2024-05-15 17:48:06,633 - UQpy: Scientific Machine Learning: Epoch 447 / 1,000 Train Loss 0.08110640570521355 Test Loss 0.10254242271184921\n",
      "[INFO] - 2024-05-15 17:48:09,914 - UQpy: Scientific Machine Learning: Epoch 448 / 1,000 Train Loss 0.08065849738685708 Test Loss 0.10373789072036743\n",
      "[INFO] - 2024-05-15 17:48:13,201 - UQpy: Scientific Machine Learning: Epoch 449 / 1,000 Train Loss 0.0801261537953427 Test Loss 0.10277198255062103\n",
      "[INFO] - 2024-05-15 17:48:16,470 - UQpy: Scientific Machine Learning: Epoch 450 / 1,000 Train Loss 0.08096253558209068 Test Loss 0.10370273888111115\n",
      "[INFO] - 2024-05-15 17:48:19,779 - UQpy: Scientific Machine Learning: Epoch 451 / 1,000 Train Loss 0.08373956774410449 Test Loss 0.10527735203504562\n",
      "[INFO] - 2024-05-15 17:48:23,423 - UQpy: Scientific Machine Learning: Epoch 452 / 1,000 Train Loss 0.08418136639030356 Test Loss 0.10575464367866516\n",
      "[INFO] - 2024-05-15 17:48:26,714 - UQpy: Scientific Machine Learning: Epoch 453 / 1,000 Train Loss 0.08074980248746119 Test Loss 0.10173138976097107\n",
      "[INFO] - 2024-05-15 17:48:30,006 - UQpy: Scientific Machine Learning: Epoch 454 / 1,000 Train Loss 0.08051687910368568 Test Loss 0.10068528354167938\n",
      "[INFO] - 2024-05-15 17:48:33,682 - UQpy: Scientific Machine Learning: Epoch 455 / 1,000 Train Loss 0.07891841271990224 Test Loss 0.10253653675317764\n",
      "[INFO] - 2024-05-15 17:48:37,095 - UQpy: Scientific Machine Learning: Epoch 456 / 1,000 Train Loss 0.07957771126376956 Test Loss 0.10397741198539734\n",
      "[INFO] - 2024-05-15 17:48:40,385 - UQpy: Scientific Machine Learning: Epoch 457 / 1,000 Train Loss 0.08123143801563665 Test Loss 0.10141532868146896\n",
      "[INFO] - 2024-05-15 17:48:43,795 - UQpy: Scientific Machine Learning: Epoch 458 / 1,000 Train Loss 0.07833008330903556 Test Loss 0.10002003610134125\n",
      "[INFO] - 2024-05-15 17:48:47,084 - UQpy: Scientific Machine Learning: Epoch 459 / 1,000 Train Loss 0.07826717786098782 Test Loss 0.1002267450094223\n",
      "[INFO] - 2024-05-15 17:48:50,382 - UQpy: Scientific Machine Learning: Epoch 460 / 1,000 Train Loss 0.07772262472855418 Test Loss 0.09962071478366852\n",
      "[INFO] - 2024-05-15 17:48:53,681 - UQpy: Scientific Machine Learning: Epoch 461 / 1,000 Train Loss 0.07711342330041684 Test Loss 0.09912173449993134\n",
      "[INFO] - 2024-05-15 17:48:56,998 - UQpy: Scientific Machine Learning: Epoch 462 / 1,000 Train Loss 0.0769278334551736 Test Loss 0.09860672056674957\n",
      "[INFO] - 2024-05-15 17:49:00,312 - UQpy: Scientific Machine Learning: Epoch 463 / 1,000 Train Loss 0.07966849051023785 Test Loss 0.10151819884777069\n",
      "[INFO] - 2024-05-15 17:49:03,612 - UQpy: Scientific Machine Learning: Epoch 464 / 1,000 Train Loss 0.07796110702972663 Test Loss 0.09876090288162231\n",
      "[INFO] - 2024-05-15 17:49:06,922 - UQpy: Scientific Machine Learning: Epoch 465 / 1,000 Train Loss 0.07893294017565877 Test Loss 0.09906208515167236\n",
      "[INFO] - 2024-05-15 17:49:10,238 - UQpy: Scientific Machine Learning: Epoch 466 / 1,000 Train Loss 0.07684808165619247 Test Loss 0.0988268181681633\n",
      "[INFO] - 2024-05-15 17:49:13,529 - UQpy: Scientific Machine Learning: Epoch 467 / 1,000 Train Loss 0.07655974006966541 Test Loss 0.09705277532339096\n",
      "[INFO] - 2024-05-15 17:49:16,842 - UQpy: Scientific Machine Learning: Epoch 468 / 1,000 Train Loss 0.07624802758034907 Test Loss 0.09701928496360779\n",
      "[INFO] - 2024-05-15 17:49:20,124 - UQpy: Scientific Machine Learning: Epoch 469 / 1,000 Train Loss 0.07611750301561858 Test Loss 0.09782298654317856\n",
      "[INFO] - 2024-05-15 17:49:23,444 - UQpy: Scientific Machine Learning: Epoch 470 / 1,000 Train Loss 0.08853610840282942 Test Loss 0.09920534491539001\n",
      "[INFO] - 2024-05-15 17:49:26,720 - UQpy: Scientific Machine Learning: Epoch 471 / 1,000 Train Loss 0.08080843816462316 Test Loss 0.09842899441719055\n",
      "[INFO] - 2024-05-15 17:49:30,025 - UQpy: Scientific Machine Learning: Epoch 472 / 1,000 Train Loss 0.07757318823745377 Test Loss 0.09699183702468872\n",
      "[INFO] - 2024-05-15 17:49:33,297 - UQpy: Scientific Machine Learning: Epoch 473 / 1,000 Train Loss 0.07599624520853947 Test Loss 0.0962601900100708\n",
      "[INFO] - 2024-05-15 17:49:36,593 - UQpy: Scientific Machine Learning: Epoch 474 / 1,000 Train Loss 0.07612318780861403 Test Loss 0.09675225615501404\n",
      "[INFO] - 2024-05-15 17:49:39,878 - UQpy: Scientific Machine Learning: Epoch 475 / 1,000 Train Loss 0.07739582657814026 Test Loss 0.09844110906124115\n",
      "[INFO] - 2024-05-15 17:49:43,186 - UQpy: Scientific Machine Learning: Epoch 476 / 1,000 Train Loss 0.0759840488041702 Test Loss 0.0960361510515213\n",
      "[INFO] - 2024-05-15 17:49:46,480 - UQpy: Scientific Machine Learning: Epoch 477 / 1,000 Train Loss 0.07655575224443485 Test Loss 0.09671671688556671\n",
      "[INFO] - 2024-05-15 17:49:49,764 - UQpy: Scientific Machine Learning: Epoch 478 / 1,000 Train Loss 0.07616351721318144 Test Loss 0.09659320116043091\n",
      "[INFO] - 2024-05-15 17:49:53,063 - UQpy: Scientific Machine Learning: Epoch 479 / 1,000 Train Loss 0.08527893220123492 Test Loss 0.10373617708683014\n",
      "[INFO] - 2024-05-15 17:49:56,366 - UQpy: Scientific Machine Learning: Epoch 480 / 1,000 Train Loss 0.08328614187868018 Test Loss 0.09628738462924957\n",
      "[INFO] - 2024-05-15 17:49:59,670 - UQpy: Scientific Machine Learning: Epoch 481 / 1,000 Train Loss 0.07598608830257465 Test Loss 0.09677361696958542\n",
      "[INFO] - 2024-05-15 17:50:02,990 - UQpy: Scientific Machine Learning: Epoch 482 / 1,000 Train Loss 0.07487915044552401 Test Loss 0.09763902425765991\n",
      "[INFO] - 2024-05-15 17:50:06,479 - UQpy: Scientific Machine Learning: Epoch 483 / 1,000 Train Loss 0.07525664371879477 Test Loss 0.09520024061203003\n",
      "[INFO] - 2024-05-15 17:50:09,808 - UQpy: Scientific Machine Learning: Epoch 484 / 1,000 Train Loss 0.07420771470979641 Test Loss 0.095293328166008\n",
      "[INFO] - 2024-05-15 17:50:13,104 - UQpy: Scientific Machine Learning: Epoch 485 / 1,000 Train Loss 0.07579204577364419 Test Loss 0.09744643419981003\n",
      "[INFO] - 2024-05-15 17:50:16,504 - UQpy: Scientific Machine Learning: Epoch 486 / 1,000 Train Loss 0.0780595744910993 Test Loss 0.09812134504318237\n",
      "[INFO] - 2024-05-15 17:50:19,794 - UQpy: Scientific Machine Learning: Epoch 487 / 1,000 Train Loss 0.07947917988425807 Test Loss 0.09927491843700409\n",
      "[INFO] - 2024-05-15 17:50:23,172 - UQpy: Scientific Machine Learning: Epoch 488 / 1,000 Train Loss 0.07558225663869005 Test Loss 0.09472765028476715\n",
      "[INFO] - 2024-05-15 17:50:26,479 - UQpy: Scientific Machine Learning: Epoch 489 / 1,000 Train Loss 0.0737166140032442 Test Loss 0.09262637048959732\n",
      "[INFO] - 2024-05-15 17:50:29,799 - UQpy: Scientific Machine Learning: Epoch 490 / 1,000 Train Loss 0.07333531015013393 Test Loss 0.09373247623443604\n",
      "[INFO] - 2024-05-15 17:50:33,096 - UQpy: Scientific Machine Learning: Epoch 491 / 1,000 Train Loss 0.07314082548806541 Test Loss 0.09269179403781891\n",
      "[INFO] - 2024-05-15 17:50:36,386 - UQpy: Scientific Machine Learning: Epoch 492 / 1,000 Train Loss 0.07291409765419207 Test Loss 0.09246170520782471\n",
      "[INFO] - 2024-05-15 17:50:39,674 - UQpy: Scientific Machine Learning: Epoch 493 / 1,000 Train Loss 0.0731919645086715 Test Loss 0.09445811808109283\n",
      "[INFO] - 2024-05-15 17:50:42,988 - UQpy: Scientific Machine Learning: Epoch 494 / 1,000 Train Loss 0.07357541981496309 Test Loss 0.09411684423685074\n",
      "[INFO] - 2024-05-15 17:50:46,477 - UQpy: Scientific Machine Learning: Epoch 495 / 1,000 Train Loss 0.07290929320611451 Test Loss 0.09229560196399689\n",
      "[INFO] - 2024-05-15 17:50:49,779 - UQpy: Scientific Machine Learning: Epoch 496 / 1,000 Train Loss 0.07328643238073901 Test Loss 0.09285096824169159\n",
      "[INFO] - 2024-05-15 17:50:53,068 - UQpy: Scientific Machine Learning: Epoch 497 / 1,000 Train Loss 0.07584535252106817 Test Loss 0.1026940643787384\n",
      "[INFO] - 2024-05-15 17:50:56,595 - UQpy: Scientific Machine Learning: Epoch 498 / 1,000 Train Loss 0.08116584761362326 Test Loss 0.10309331119060516\n",
      "[INFO] - 2024-05-15 17:51:00,164 - UQpy: Scientific Machine Learning: Epoch 499 / 1,000 Train Loss 0.07682803606516436 Test Loss 0.09396438300609589\n",
      "[INFO] - 2024-05-15 17:51:04,086 - UQpy: Scientific Machine Learning: Epoch 500 / 1,000 Train Loss 0.07299554975409257 Test Loss 0.09225620329380035\n",
      "[INFO] - 2024-05-15 17:51:07,675 - UQpy: Scientific Machine Learning: Epoch 501 / 1,000 Train Loss 0.07481842958613445 Test Loss 0.09538882970809937\n",
      "[INFO] - 2024-05-15 17:51:11,785 - UQpy: Scientific Machine Learning: Epoch 502 / 1,000 Train Loss 0.07530702023129714 Test Loss 0.09602824598550797\n",
      "[INFO] - 2024-05-15 17:51:15,876 - UQpy: Scientific Machine Learning: Epoch 503 / 1,000 Train Loss 0.07358789091047488 Test Loss 0.0918472409248352\n",
      "[INFO] - 2024-05-15 17:51:19,175 - UQpy: Scientific Machine Learning: Epoch 504 / 1,000 Train Loss 0.07207526619497098 Test Loss 0.09040198475122452\n",
      "[INFO] - 2024-05-15 17:51:22,721 - UQpy: Scientific Machine Learning: Epoch 505 / 1,000 Train Loss 0.07125084259008106 Test Loss 0.09082148969173431\n",
      "[INFO] - 2024-05-15 17:51:26,005 - UQpy: Scientific Machine Learning: Epoch 506 / 1,000 Train Loss 0.07077916809602787 Test Loss 0.09231351315975189\n",
      "[INFO] - 2024-05-15 17:51:29,572 - UQpy: Scientific Machine Learning: Epoch 507 / 1,000 Train Loss 0.07268652986539037 Test Loss 0.09352913498878479\n",
      "[INFO] - 2024-05-15 17:51:32,873 - UQpy: Scientific Machine Learning: Epoch 508 / 1,000 Train Loss 0.07188766488903447 Test Loss 0.09198606014251709\n",
      "[INFO] - 2024-05-15 17:51:36,179 - UQpy: Scientific Machine Learning: Epoch 509 / 1,000 Train Loss 0.07223789178227123 Test Loss 0.09015706181526184\n",
      "[INFO] - 2024-05-15 17:51:39,773 - UQpy: Scientific Machine Learning: Epoch 510 / 1,000 Train Loss 0.07143443822860718 Test Loss 0.0920381098985672\n",
      "[INFO] - 2024-05-15 17:51:43,413 - UQpy: Scientific Machine Learning: Epoch 511 / 1,000 Train Loss 0.07194734658849866 Test Loss 0.08970701694488525\n",
      "[INFO] - 2024-05-15 17:51:46,702 - UQpy: Scientific Machine Learning: Epoch 512 / 1,000 Train Loss 0.07228800046600793 Test Loss 0.09061083197593689\n",
      "[INFO] - 2024-05-15 17:51:50,008 - UQpy: Scientific Machine Learning: Epoch 513 / 1,000 Train Loss 0.07089206164604739 Test Loss 0.09218773990869522\n",
      "[INFO] - 2024-05-15 17:51:53,317 - UQpy: Scientific Machine Learning: Epoch 514 / 1,000 Train Loss 0.07170859959564711 Test Loss 0.08872121572494507\n",
      "[INFO] - 2024-05-15 17:51:56,626 - UQpy: Scientific Machine Learning: Epoch 515 / 1,000 Train Loss 0.07133247526852708 Test Loss 0.08831427991390228\n",
      "[INFO] - 2024-05-15 17:52:00,128 - UQpy: Scientific Machine Learning: Epoch 516 / 1,000 Train Loss 0.07053419731949505 Test Loss 0.09081384539604187\n",
      "[INFO] - 2024-05-15 17:52:03,453 - UQpy: Scientific Machine Learning: Epoch 517 / 1,000 Train Loss 0.071379787631725 Test Loss 0.09079466760158539\n",
      "[INFO] - 2024-05-15 17:52:06,965 - UQpy: Scientific Machine Learning: Epoch 518 / 1,000 Train Loss 0.0725389091592086 Test Loss 0.08894103765487671\n",
      "[INFO] - 2024-05-15 17:52:10,405 - UQpy: Scientific Machine Learning: Epoch 519 / 1,000 Train Loss 0.07088102890472663 Test Loss 0.09137409925460815\n",
      "[INFO] - 2024-05-15 17:52:13,896 - UQpy: Scientific Machine Learning: Epoch 520 / 1,000 Train Loss 0.07148470082565357 Test Loss 0.09428128600120544\n",
      "[INFO] - 2024-05-15 17:52:17,661 - UQpy: Scientific Machine Learning: Epoch 521 / 1,000 Train Loss 0.07166785903667149 Test Loss 0.08840450644493103\n",
      "[INFO] - 2024-05-15 17:52:20,970 - UQpy: Scientific Machine Learning: Epoch 522 / 1,000 Train Loss 0.0704032210142989 Test Loss 0.08800403773784637\n",
      "[INFO] - 2024-05-15 17:52:24,735 - UQpy: Scientific Machine Learning: Epoch 523 / 1,000 Train Loss 0.06994476561483584 Test Loss 0.08814959973096848\n",
      "[INFO] - 2024-05-15 17:52:28,487 - UQpy: Scientific Machine Learning: Epoch 524 / 1,000 Train Loss 0.06970893101472604 Test Loss 0.08773760497570038\n",
      "[INFO] - 2024-05-15 17:52:32,098 - UQpy: Scientific Machine Learning: Epoch 525 / 1,000 Train Loss 0.07014369533250206 Test Loss 0.08735435456037521\n",
      "[INFO] - 2024-05-15 17:52:35,813 - UQpy: Scientific Machine Learning: Epoch 526 / 1,000 Train Loss 0.0707184468445025 Test Loss 0.087801493704319\n",
      "[INFO] - 2024-05-15 17:52:39,715 - UQpy: Scientific Machine Learning: Epoch 527 / 1,000 Train Loss 0.07232288271188736 Test Loss 0.09022301435470581\n",
      "[INFO] - 2024-05-15 17:52:43,610 - UQpy: Scientific Machine Learning: Epoch 528 / 1,000 Train Loss 0.07028424249667871 Test Loss 0.08759672939777374\n",
      "[INFO] - 2024-05-15 17:52:47,057 - UQpy: Scientific Machine Learning: Epoch 529 / 1,000 Train Loss 0.06989967979882893 Test Loss 0.08796882629394531\n",
      "[INFO] - 2024-05-15 17:52:50,707 - UQpy: Scientific Machine Learning: Epoch 530 / 1,000 Train Loss 0.06916413828730583 Test Loss 0.08596426248550415\n",
      "[INFO] - 2024-05-15 17:52:54,016 - UQpy: Scientific Machine Learning: Epoch 531 / 1,000 Train Loss 0.06895550634515912 Test Loss 0.08680009096860886\n",
      "[INFO] - 2024-05-15 17:52:57,311 - UQpy: Scientific Machine Learning: Epoch 532 / 1,000 Train Loss 0.07026698244245429 Test Loss 0.09204570949077606\n",
      "[INFO] - 2024-05-15 17:53:00,979 - UQpy: Scientific Machine Learning: Epoch 533 / 1,000 Train Loss 0.0700372600633847 Test Loss 0.09017696976661682\n",
      "[INFO] - 2024-05-15 17:53:04,268 - UQpy: Scientific Machine Learning: Epoch 534 / 1,000 Train Loss 0.06854238361120224 Test Loss 0.0862489715218544\n",
      "[INFO] - 2024-05-15 17:53:07,565 - UQpy: Scientific Machine Learning: Epoch 535 / 1,000 Train Loss 0.06849658293159384 Test Loss 0.0863465964794159\n",
      "[INFO] - 2024-05-15 17:53:10,863 - UQpy: Scientific Machine Learning: Epoch 536 / 1,000 Train Loss 0.069833287086926 Test Loss 0.0868556797504425\n",
      "[INFO] - 2024-05-15 17:53:14,151 - UQpy: Scientific Machine Learning: Epoch 537 / 1,000 Train Loss 0.07164297429354567 Test Loss 0.08729559183120728\n",
      "[INFO] - 2024-05-15 17:53:17,421 - UQpy: Scientific Machine Learning: Epoch 538 / 1,000 Train Loss 0.07016362838054958 Test Loss 0.08745604753494263\n",
      "[INFO] - 2024-05-15 17:53:20,710 - UQpy: Scientific Machine Learning: Epoch 539 / 1,000 Train Loss 0.06943119905496899 Test Loss 0.09345822781324387\n",
      "[INFO] - 2024-05-15 17:53:24,773 - UQpy: Scientific Machine Learning: Epoch 540 / 1,000 Train Loss 0.06917470477913555 Test Loss 0.08494941145181656\n",
      "[INFO] - 2024-05-15 17:53:28,485 - UQpy: Scientific Machine Learning: Epoch 541 / 1,000 Train Loss 0.067308144153733 Test Loss 0.08592052757740021\n",
      "[INFO] - 2024-05-15 17:53:31,764 - UQpy: Scientific Machine Learning: Epoch 542 / 1,000 Train Loss 0.07006900639910447 Test Loss 0.08674395084381104\n",
      "[INFO] - 2024-05-15 17:53:35,516 - UQpy: Scientific Machine Learning: Epoch 543 / 1,000 Train Loss 0.0678084349553836 Test Loss 0.08890140801668167\n",
      "[INFO] - 2024-05-15 17:53:39,198 - UQpy: Scientific Machine Learning: Epoch 544 / 1,000 Train Loss 0.06717783173448161 Test Loss 0.08807437121868134\n",
      "[INFO] - 2024-05-15 17:53:42,500 - UQpy: Scientific Machine Learning: Epoch 545 / 1,000 Train Loss 0.06733670752299459 Test Loss 0.08573134243488312\n",
      "[INFO] - 2024-05-15 17:53:45,832 - UQpy: Scientific Machine Learning: Epoch 546 / 1,000 Train Loss 0.06796407248628766 Test Loss 0.08474408090114594\n",
      "[INFO] - 2024-05-15 17:53:49,134 - UQpy: Scientific Machine Learning: Epoch 547 / 1,000 Train Loss 0.06700676856072325 Test Loss 0.08443880081176758\n",
      "[INFO] - 2024-05-15 17:53:52,446 - UQpy: Scientific Machine Learning: Epoch 548 / 1,000 Train Loss 0.06852974605403449 Test Loss 0.08923099935054779\n",
      "[INFO] - 2024-05-15 17:53:55,730 - UQpy: Scientific Machine Learning: Epoch 549 / 1,000 Train Loss 0.06852022480023534 Test Loss 0.08572366833686829\n",
      "[INFO] - 2024-05-15 17:53:59,016 - UQpy: Scientific Machine Learning: Epoch 550 / 1,000 Train Loss 0.06934107957701934 Test Loss 0.08602910488843918\n",
      "[INFO] - 2024-05-15 17:54:02,309 - UQpy: Scientific Machine Learning: Epoch 551 / 1,000 Train Loss 0.07065851260956965 Test Loss 0.09099309146404266\n",
      "[INFO] - 2024-05-15 17:54:05,842 - UQpy: Scientific Machine Learning: Epoch 552 / 1,000 Train Loss 0.0744657683137216 Test Loss 0.08973325788974762\n",
      "[INFO] - 2024-05-15 17:54:09,471 - UQpy: Scientific Machine Learning: Epoch 553 / 1,000 Train Loss 0.06795901275779072 Test Loss 0.08428768813610077\n",
      "[INFO] - 2024-05-15 17:54:12,780 - UQpy: Scientific Machine Learning: Epoch 554 / 1,000 Train Loss 0.06707066944555233 Test Loss 0.08370786160230637\n",
      "[INFO] - 2024-05-15 17:54:16,468 - UQpy: Scientific Machine Learning: Epoch 555 / 1,000 Train Loss 0.06634272909478138 Test Loss 0.0833357647061348\n",
      "[INFO] - 2024-05-15 17:54:19,773 - UQpy: Scientific Machine Learning: Epoch 556 / 1,000 Train Loss 0.06547020434548981 Test Loss 0.08240973949432373\n",
      "[INFO] - 2024-05-15 17:54:23,583 - UQpy: Scientific Machine Learning: Epoch 557 / 1,000 Train Loss 0.06497552520350407 Test Loss 0.08340548723936081\n",
      "[INFO] - 2024-05-15 17:54:27,168 - UQpy: Scientific Machine Learning: Epoch 558 / 1,000 Train Loss 0.06502879234521013 Test Loss 0.08204425126314163\n",
      "[INFO] - 2024-05-15 17:54:30,451 - UQpy: Scientific Machine Learning: Epoch 559 / 1,000 Train Loss 0.06424665960826371 Test Loss 0.08219512552022934\n",
      "[INFO] - 2024-05-15 17:54:33,730 - UQpy: Scientific Machine Learning: Epoch 560 / 1,000 Train Loss 0.06416898926621989 Test Loss 0.08127371966838837\n",
      "[INFO] - 2024-05-15 17:54:37,019 - UQpy: Scientific Machine Learning: Epoch 561 / 1,000 Train Loss 0.06428425308120878 Test Loss 0.0814598947763443\n",
      "[INFO] - 2024-05-15 17:54:40,322 - UQpy: Scientific Machine Learning: Epoch 562 / 1,000 Train Loss 0.06406292044802715 Test Loss 0.08114419132471085\n",
      "[INFO] - 2024-05-15 17:54:44,038 - UQpy: Scientific Machine Learning: Epoch 563 / 1,000 Train Loss 0.0640647436835264 Test Loss 0.08267909288406372\n",
      "[INFO] - 2024-05-15 17:54:47,341 - UQpy: Scientific Machine Learning: Epoch 564 / 1,000 Train Loss 0.067336883984114 Test Loss 0.08276282250881195\n",
      "[INFO] - 2024-05-15 17:54:50,742 - UQpy: Scientific Machine Learning: Epoch 565 / 1,000 Train Loss 0.06593329733923863 Test Loss 0.08090300858020782\n",
      "[INFO] - 2024-05-15 17:54:54,393 - UQpy: Scientific Machine Learning: Epoch 566 / 1,000 Train Loss 0.06384816079547531 Test Loss 0.08241643011569977\n",
      "[INFO] - 2024-05-15 17:54:57,679 - UQpy: Scientific Machine Learning: Epoch 567 / 1,000 Train Loss 0.06509621480577871 Test Loss 0.08332648128271103\n",
      "[INFO] - 2024-05-15 17:55:00,958 - UQpy: Scientific Machine Learning: Epoch 568 / 1,000 Train Loss 0.06667026877403259 Test Loss 0.08090490102767944\n",
      "[INFO] - 2024-05-15 17:55:04,238 - UQpy: Scientific Machine Learning: Epoch 569 / 1,000 Train Loss 0.06363684723251745 Test Loss 0.08132757246494293\n",
      "[INFO] - 2024-05-15 17:55:07,524 - UQpy: Scientific Machine Learning: Epoch 570 / 1,000 Train Loss 0.06561976612398498 Test Loss 0.0934281200170517\n",
      "[INFO] - 2024-05-15 17:55:10,784 - UQpy: Scientific Machine Learning: Epoch 571 / 1,000 Train Loss 0.07064168233620494 Test Loss 0.08214186877012253\n",
      "[INFO] - 2024-05-15 17:55:14,057 - UQpy: Scientific Machine Learning: Epoch 572 / 1,000 Train Loss 0.06574357065715287 Test Loss 0.0840531438589096\n",
      "[INFO] - 2024-05-15 17:55:17,361 - UQpy: Scientific Machine Learning: Epoch 573 / 1,000 Train Loss 0.06370960450486134 Test Loss 0.07951460033655167\n",
      "[INFO] - 2024-05-15 17:55:20,642 - UQpy: Scientific Machine Learning: Epoch 574 / 1,000 Train Loss 0.062126180647235164 Test Loss 0.07931508123874664\n",
      "[INFO] - 2024-05-15 17:55:23,934 - UQpy: Scientific Machine Learning: Epoch 575 / 1,000 Train Loss 0.06248329383762259 Test Loss 0.07851216942071915\n",
      "[INFO] - 2024-05-15 17:55:27,206 - UQpy: Scientific Machine Learning: Epoch 576 / 1,000 Train Loss 0.06303906930904639 Test Loss 0.08169907331466675\n",
      "[INFO] - 2024-05-15 17:55:30,564 - UQpy: Scientific Machine Learning: Epoch 577 / 1,000 Train Loss 0.06363631804522715 Test Loss 0.08183950930833817\n",
      "[INFO] - 2024-05-15 17:55:33,864 - UQpy: Scientific Machine Learning: Epoch 578 / 1,000 Train Loss 0.06167310339055563 Test Loss 0.07824546843767166\n",
      "[INFO] - 2024-05-15 17:55:37,177 - UQpy: Scientific Machine Learning: Epoch 579 / 1,000 Train Loss 0.06127079497826727 Test Loss 0.07756612449884415\n",
      "[INFO] - 2024-05-15 17:55:40,803 - UQpy: Scientific Machine Learning: Epoch 580 / 1,000 Train Loss 0.0605927227359069 Test Loss 0.07669785618782043\n",
      "[INFO] - 2024-05-15 17:55:44,105 - UQpy: Scientific Machine Learning: Epoch 581 / 1,000 Train Loss 0.060697608284260095 Test Loss 0.07892494648694992\n",
      "[INFO] - 2024-05-15 17:55:47,382 - UQpy: Scientific Machine Learning: Epoch 582 / 1,000 Train Loss 0.06470767859565585 Test Loss 0.07971418648958206\n",
      "[INFO] - 2024-05-15 17:55:50,711 - UQpy: Scientific Machine Learning: Epoch 583 / 1,000 Train Loss 0.06514777752913926 Test Loss 0.07973495125770569\n",
      "[INFO] - 2024-05-15 17:55:54,018 - UQpy: Scientific Machine Learning: Epoch 584 / 1,000 Train Loss 0.06373625385918115 Test Loss 0.07866162806749344\n",
      "[INFO] - 2024-05-15 17:55:57,341 - UQpy: Scientific Machine Learning: Epoch 585 / 1,000 Train Loss 0.061880536573497874 Test Loss 0.0788867399096489\n",
      "[INFO] - 2024-05-15 17:56:00,680 - UQpy: Scientific Machine Learning: Epoch 586 / 1,000 Train Loss 0.061806645636495794 Test Loss 0.08166175335645676\n",
      "[INFO] - 2024-05-15 17:56:04,052 - UQpy: Scientific Machine Learning: Epoch 587 / 1,000 Train Loss 0.06331896232931238 Test Loss 0.08032771199941635\n",
      "[INFO] - 2024-05-15 17:56:07,363 - UQpy: Scientific Machine Learning: Epoch 588 / 1,000 Train Loss 0.06255162036732624 Test Loss 0.08103031665086746\n",
      "[INFO] - 2024-05-15 17:56:10,672 - UQpy: Scientific Machine Learning: Epoch 589 / 1,000 Train Loss 0.06257551104614609 Test Loss 0.07866568863391876\n",
      "[INFO] - 2024-05-15 17:56:14,056 - UQpy: Scientific Machine Learning: Epoch 590 / 1,000 Train Loss 0.06143366819933841 Test Loss 0.07595475018024445\n",
      "[INFO] - 2024-05-15 17:56:17,464 - UQpy: Scientific Machine Learning: Epoch 591 / 1,000 Train Loss 0.05918477240361666 Test Loss 0.07592962682247162\n",
      "[INFO] - 2024-05-15 17:56:20,870 - UQpy: Scientific Machine Learning: Epoch 592 / 1,000 Train Loss 0.058486137931284155 Test Loss 0.07630699872970581\n",
      "[INFO] - 2024-05-15 17:56:24,167 - UQpy: Scientific Machine Learning: Epoch 593 / 1,000 Train Loss 0.05929123669078475 Test Loss 0.07782483845949173\n",
      "[INFO] - 2024-05-15 17:56:27,446 - UQpy: Scientific Machine Learning: Epoch 594 / 1,000 Train Loss 0.063460271413389 Test Loss 0.08463219553232193\n",
      "[INFO] - 2024-05-15 17:56:30,638 - UQpy: Scientific Machine Learning: Epoch 595 / 1,000 Train Loss 0.06527356724990041 Test Loss 0.07980188727378845\n",
      "[INFO] - 2024-05-15 17:56:33,806 - UQpy: Scientific Machine Learning: Epoch 596 / 1,000 Train Loss 0.06535016745328903 Test Loss 0.08114703744649887\n",
      "[INFO] - 2024-05-15 17:56:37,040 - UQpy: Scientific Machine Learning: Epoch 597 / 1,000 Train Loss 0.06241168728784511 Test Loss 0.08772772550582886\n",
      "[INFO] - 2024-05-15 17:56:40,316 - UQpy: Scientific Machine Learning: Epoch 598 / 1,000 Train Loss 0.06970594568472159 Test Loss 0.084572434425354\n",
      "[INFO] - 2024-05-15 17:56:43,554 - UQpy: Scientific Machine Learning: Epoch 599 / 1,000 Train Loss 0.07361325700032084 Test Loss 0.09171275049448013\n",
      "[INFO] - 2024-05-15 17:56:46,862 - UQpy: Scientific Machine Learning: Epoch 600 / 1,000 Train Loss 0.061999429801577015 Test Loss 0.0753263607621193\n",
      "[INFO] - 2024-05-15 17:56:50,086 - UQpy: Scientific Machine Learning: Epoch 601 / 1,000 Train Loss 0.05770171963070568 Test Loss 0.0714300349354744\n",
      "[INFO] - 2024-05-15 17:56:53,311 - UQpy: Scientific Machine Learning: Epoch 602 / 1,000 Train Loss 0.056126054376363754 Test Loss 0.07105717062950134\n",
      "[INFO] - 2024-05-15 17:56:56,514 - UQpy: Scientific Machine Learning: Epoch 603 / 1,000 Train Loss 0.05540729412122777 Test Loss 0.07109680026769638\n",
      "[INFO] - 2024-05-15 17:56:59,774 - UQpy: Scientific Machine Learning: Epoch 604 / 1,000 Train Loss 0.05503319811664129 Test Loss 0.07040339708328247\n",
      "[INFO] - 2024-05-15 17:57:02,973 - UQpy: Scientific Machine Learning: Epoch 605 / 1,000 Train Loss 0.054820287188417034 Test Loss 0.07034225761890411\n",
      "[INFO] - 2024-05-15 17:57:06,178 - UQpy: Scientific Machine Learning: Epoch 606 / 1,000 Train Loss 0.054583280886474406 Test Loss 0.06970427930355072\n",
      "[INFO] - 2024-05-15 17:57:09,403 - UQpy: Scientific Machine Learning: Epoch 607 / 1,000 Train Loss 0.054231186642458566 Test Loss 0.0699671059846878\n",
      "[INFO] - 2024-05-15 17:57:12,593 - UQpy: Scientific Machine Learning: Epoch 608 / 1,000 Train Loss 0.054022486272611116 Test Loss 0.06900732964277267\n",
      "[INFO] - 2024-05-15 17:57:15,765 - UQpy: Scientific Machine Learning: Epoch 609 / 1,000 Train Loss 0.054588931563653444 Test Loss 0.07014936208724976\n",
      "[INFO] - 2024-05-15 17:57:19,018 - UQpy: Scientific Machine Learning: Epoch 610 / 1,000 Train Loss 0.05462824416003729 Test Loss 0.07073138654232025\n",
      "[INFO] - 2024-05-15 17:57:22,271 - UQpy: Scientific Machine Learning: Epoch 611 / 1,000 Train Loss 0.05375040342148982 Test Loss 0.06885527074337006\n",
      "[INFO] - 2024-05-15 17:57:25,508 - UQpy: Scientific Machine Learning: Epoch 612 / 1,000 Train Loss 0.05405968368837708 Test Loss 0.06986942142248154\n",
      "[INFO] - 2024-05-15 17:57:28,711 - UQpy: Scientific Machine Learning: Epoch 613 / 1,000 Train Loss 0.05466411007862342 Test Loss 0.06859622150659561\n",
      "[INFO] - 2024-05-15 17:57:31,914 - UQpy: Scientific Machine Learning: Epoch 614 / 1,000 Train Loss 0.05389421982200522 Test Loss 0.07000096142292023\n",
      "[INFO] - 2024-05-15 17:57:35,140 - UQpy: Scientific Machine Learning: Epoch 615 / 1,000 Train Loss 0.054810297920515665 Test Loss 0.07086263597011566\n",
      "[INFO] - 2024-05-15 17:57:38,403 - UQpy: Scientific Machine Learning: Epoch 616 / 1,000 Train Loss 0.0546789141862016 Test Loss 0.06890006363391876\n",
      "[INFO] - 2024-05-15 17:57:41,623 - UQpy: Scientific Machine Learning: Epoch 617 / 1,000 Train Loss 0.05430198009861143 Test Loss 0.06755736470222473\n",
      "[INFO] - 2024-05-15 17:57:44,868 - UQpy: Scientific Machine Learning: Epoch 618 / 1,000 Train Loss 0.053001097550517635 Test Loss 0.067869633436203\n",
      "[INFO] - 2024-05-15 17:57:48,093 - UQpy: Scientific Machine Learning: Epoch 619 / 1,000 Train Loss 0.05285641864726418 Test Loss 0.06821781396865845\n",
      "[INFO] - 2024-05-15 17:57:51,289 - UQpy: Scientific Machine Learning: Epoch 620 / 1,000 Train Loss 0.05329526882422598 Test Loss 0.0678526759147644\n",
      "[INFO] - 2024-05-15 17:57:54,498 - UQpy: Scientific Machine Learning: Epoch 621 / 1,000 Train Loss 0.051737473787445774 Test Loss 0.06674566864967346\n",
      "[INFO] - 2024-05-15 17:57:57,711 - UQpy: Scientific Machine Learning: Epoch 622 / 1,000 Train Loss 0.05222608070624502 Test Loss 0.06801716983318329\n",
      "[INFO] - 2024-05-15 17:58:00,983 - UQpy: Scientific Machine Learning: Epoch 623 / 1,000 Train Loss 0.05240429526096896 Test Loss 0.06650029122829437\n",
      "[INFO] - 2024-05-15 17:58:04,169 - UQpy: Scientific Machine Learning: Epoch 624 / 1,000 Train Loss 0.05235482399400912 Test Loss 0.07022741436958313\n",
      "[INFO] - 2024-05-15 17:58:07,393 - UQpy: Scientific Machine Learning: Epoch 625 / 1,000 Train Loss 0.05280926039344386 Test Loss 0.07045301049947739\n",
      "[INFO] - 2024-05-15 17:58:10,715 - UQpy: Scientific Machine Learning: Epoch 626 / 1,000 Train Loss 0.055000363996154385 Test Loss 0.06921796500682831\n",
      "[INFO] - 2024-05-15 17:58:14,080 - UQpy: Scientific Machine Learning: Epoch 627 / 1,000 Train Loss 0.053223005643016415 Test Loss 0.0675792545080185\n",
      "[INFO] - 2024-05-15 17:58:18,032 - UQpy: Scientific Machine Learning: Epoch 628 / 1,000 Train Loss 0.05117919139171902 Test Loss 0.06482250243425369\n",
      "[INFO] - 2024-05-15 17:58:21,973 - UQpy: Scientific Machine Learning: Epoch 629 / 1,000 Train Loss 0.05086745360964223 Test Loss 0.06503663957118988\n",
      "[INFO] - 2024-05-15 17:58:25,646 - UQpy: Scientific Machine Learning: Epoch 630 / 1,000 Train Loss 0.05096486112789104 Test Loss 0.0665469691157341\n",
      "[INFO] - 2024-05-15 17:58:29,150 - UQpy: Scientific Machine Learning: Epoch 631 / 1,000 Train Loss 0.05175515421127018 Test Loss 0.06786742061376572\n",
      "[INFO] - 2024-05-15 17:58:32,463 - UQpy: Scientific Machine Learning: Epoch 632 / 1,000 Train Loss 0.051267249999861965 Test Loss 0.06358076632022858\n",
      "[INFO] - 2024-05-15 17:58:35,748 - UQpy: Scientific Machine Learning: Epoch 633 / 1,000 Train Loss 0.05013492389729148 Test Loss 0.06684328615665436\n",
      "[INFO] - 2024-05-15 17:58:39,071 - UQpy: Scientific Machine Learning: Epoch 634 / 1,000 Train Loss 0.05506173756561781 Test Loss 0.07179516553878784\n",
      "[INFO] - 2024-05-15 17:58:42,388 - UQpy: Scientific Machine Learning: Epoch 635 / 1,000 Train Loss 0.0657552764996102 Test Loss 0.07689768075942993\n",
      "[INFO] - 2024-05-15 17:58:45,856 - UQpy: Scientific Machine Learning: Epoch 636 / 1,000 Train Loss 0.056154447166543255 Test Loss 0.06740690767765045\n",
      "[INFO] - 2024-05-15 17:58:49,958 - UQpy: Scientific Machine Learning: Epoch 637 / 1,000 Train Loss 0.05106463793076967 Test Loss 0.06370618939399719\n",
      "[INFO] - 2024-05-15 17:58:53,241 - UQpy: Scientific Machine Learning: Epoch 638 / 1,000 Train Loss 0.049566421853868586 Test Loss 0.06305421888828278\n",
      "[INFO] - 2024-05-15 17:58:56,519 - UQpy: Scientific Machine Learning: Epoch 639 / 1,000 Train Loss 0.04847721500616325 Test Loss 0.06146692484617233\n",
      "[INFO] - 2024-05-15 17:58:59,806 - UQpy: Scientific Machine Learning: Epoch 640 / 1,000 Train Loss 0.04725892979063486 Test Loss 0.06123502552509308\n",
      "[INFO] - 2024-05-15 17:59:03,593 - UQpy: Scientific Machine Learning: Epoch 641 / 1,000 Train Loss 0.047089716713679466 Test Loss 0.0610087588429451\n",
      "[INFO] - 2024-05-15 17:59:07,295 - UQpy: Scientific Machine Learning: Epoch 642 / 1,000 Train Loss 0.04710950643608445 Test Loss 0.06066388636827469\n",
      "[INFO] - 2024-05-15 17:59:10,607 - UQpy: Scientific Machine Learning: Epoch 643 / 1,000 Train Loss 0.04697694472576443 Test Loss 0.06157034635543823\n",
      "[INFO] - 2024-05-15 17:59:13,912 - UQpy: Scientific Machine Learning: Epoch 644 / 1,000 Train Loss 0.046964696755534725 Test Loss 0.06061355024576187\n",
      "[INFO] - 2024-05-15 17:59:17,244 - UQpy: Scientific Machine Learning: Epoch 645 / 1,000 Train Loss 0.04686021079358302 Test Loss 0.06163320690393448\n",
      "[INFO] - 2024-05-15 17:59:20,546 - UQpy: Scientific Machine Learning: Epoch 646 / 1,000 Train Loss 0.04681628393499475 Test Loss 0.06058608740568161\n",
      "[INFO] - 2024-05-15 17:59:23,847 - UQpy: Scientific Machine Learning: Epoch 647 / 1,000 Train Loss 0.04714149177858704 Test Loss 0.060399945825338364\n",
      "[INFO] - 2024-05-15 17:59:27,371 - UQpy: Scientific Machine Learning: Epoch 648 / 1,000 Train Loss 0.048329535087472515 Test Loss 0.06509167701005936\n",
      "[INFO] - 2024-05-15 17:59:30,693 - UQpy: Scientific Machine Learning: Epoch 649 / 1,000 Train Loss 0.04911456511993157 Test Loss 0.06349927932024002\n",
      "[INFO] - 2024-05-15 17:59:33,985 - UQpy: Scientific Machine Learning: Epoch 650 / 1,000 Train Loss 0.04943045620855532 Test Loss 0.06227356940507889\n",
      "[INFO] - 2024-05-15 17:59:37,269 - UQpy: Scientific Machine Learning: Epoch 651 / 1,000 Train Loss 0.04868387999503236 Test Loss 0.06393944472074509\n",
      "[INFO] - 2024-05-15 17:59:40,568 - UQpy: Scientific Machine Learning: Epoch 652 / 1,000 Train Loss 0.05078350478097012 Test Loss 0.06825251877307892\n",
      "[INFO] - 2024-05-15 17:59:43,863 - UQpy: Scientific Machine Learning: Epoch 653 / 1,000 Train Loss 0.04950357522619398 Test Loss 0.06137461960315704\n",
      "[INFO] - 2024-05-15 17:59:47,175 - UQpy: Scientific Machine Learning: Epoch 654 / 1,000 Train Loss 0.04830142875251017 Test Loss 0.06443418562412262\n",
      "[INFO] - 2024-05-15 17:59:50,465 - UQpy: Scientific Machine Learning: Epoch 655 / 1,000 Train Loss 0.049743653520157464 Test Loss 0.06115753948688507\n",
      "[INFO] - 2024-05-15 17:59:53,754 - UQpy: Scientific Machine Learning: Epoch 656 / 1,000 Train Loss 0.0457073215787348 Test Loss 0.059785954654216766\n",
      "[INFO] - 2024-05-15 17:59:57,049 - UQpy: Scientific Machine Learning: Epoch 657 / 1,000 Train Loss 0.04667088114901593 Test Loss 0.05909404903650284\n",
      "[INFO] - 2024-05-15 18:00:00,349 - UQpy: Scientific Machine Learning: Epoch 658 / 1,000 Train Loss 0.0445718314302595 Test Loss 0.05848592892289162\n",
      "[INFO] - 2024-05-15 18:00:03,626 - UQpy: Scientific Machine Learning: Epoch 659 / 1,000 Train Loss 0.04413480233204992 Test Loss 0.05909638851881027\n",
      "[INFO] - 2024-05-15 18:00:06,940 - UQpy: Scientific Machine Learning: Epoch 660 / 1,000 Train Loss 0.04422104162605185 Test Loss 0.05843906104564667\n",
      "[INFO] - 2024-05-15 18:00:10,251 - UQpy: Scientific Machine Learning: Epoch 661 / 1,000 Train Loss 0.04418385166086649 Test Loss 0.0576336532831192\n",
      "[INFO] - 2024-05-15 18:00:13,552 - UQpy: Scientific Machine Learning: Epoch 662 / 1,000 Train Loss 0.04437982192949245 Test Loss 0.057859912514686584\n",
      "[INFO] - 2024-05-15 18:00:16,847 - UQpy: Scientific Machine Learning: Epoch 663 / 1,000 Train Loss 0.044770213726319764 Test Loss 0.056972943246364594\n",
      "[INFO] - 2024-05-15 18:00:20,162 - UQpy: Scientific Machine Learning: Epoch 664 / 1,000 Train Loss 0.04317831179421199 Test Loss 0.055903755128383636\n",
      "[INFO] - 2024-05-15 18:00:23,451 - UQpy: Scientific Machine Learning: Epoch 665 / 1,000 Train Loss 0.04400187024944707 Test Loss 0.05671074241399765\n",
      "[INFO] - 2024-05-15 18:00:26,747 - UQpy: Scientific Machine Learning: Epoch 666 / 1,000 Train Loss 0.043244188161272755 Test Loss 0.056420549750328064\n",
      "[INFO] - 2024-05-15 18:00:30,032 - UQpy: Scientific Machine Learning: Epoch 667 / 1,000 Train Loss 0.04637983127644187 Test Loss 0.058850109577178955\n",
      "[INFO] - 2024-05-15 18:00:33,252 - UQpy: Scientific Machine Learning: Epoch 668 / 1,000 Train Loss 0.04468499555399543 Test Loss 0.059522151947021484\n",
      "[INFO] - 2024-05-15 18:00:36,485 - UQpy: Scientific Machine Learning: Epoch 669 / 1,000 Train Loss 0.044323587299961796 Test Loss 0.05723334476351738\n",
      "[INFO] - 2024-05-15 18:00:39,706 - UQpy: Scientific Machine Learning: Epoch 670 / 1,000 Train Loss 0.042862659222201296 Test Loss 0.05576801300048828\n",
      "[INFO] - 2024-05-15 18:00:42,936 - UQpy: Scientific Machine Learning: Epoch 671 / 1,000 Train Loss 0.04319725205239497 Test Loss 0.06042471155524254\n",
      "[INFO] - 2024-05-15 18:00:46,162 - UQpy: Scientific Machine Learning: Epoch 672 / 1,000 Train Loss 0.044321074297553616 Test Loss 0.06221982464194298\n",
      "[INFO] - 2024-05-15 18:00:49,574 - UQpy: Scientific Machine Learning: Epoch 673 / 1,000 Train Loss 0.04632803640867535 Test Loss 0.057695161551237106\n",
      "[INFO] - 2024-05-15 18:00:53,368 - UQpy: Scientific Machine Learning: Epoch 674 / 1,000 Train Loss 0.05154750319687944 Test Loss 0.06891562044620514\n",
      "[INFO] - 2024-05-15 18:00:57,296 - UQpy: Scientific Machine Learning: Epoch 675 / 1,000 Train Loss 0.08492705343585265 Test Loss 0.1483805924654007\n",
      "[INFO] - 2024-05-15 18:01:01,923 - UQpy: Scientific Machine Learning: Epoch 676 / 1,000 Train Loss 0.09834473305626919 Test Loss 0.08864092826843262\n",
      "[INFO] - 2024-05-15 18:01:06,229 - UQpy: Scientific Machine Learning: Epoch 677 / 1,000 Train Loss 0.06764234445596996 Test Loss 0.06308457255363464\n",
      "[INFO] - 2024-05-15 18:01:09,539 - UQpy: Scientific Machine Learning: Epoch 678 / 1,000 Train Loss 0.05006304285243938 Test Loss 0.06390371918678284\n",
      "[INFO] - 2024-05-15 18:01:13,212 - UQpy: Scientific Machine Learning: Epoch 679 / 1,000 Train Loss 0.045744932795825755 Test Loss 0.05848054587841034\n",
      "[INFO] - 2024-05-15 18:01:16,485 - UQpy: Scientific Machine Learning: Epoch 680 / 1,000 Train Loss 0.04207282395739304 Test Loss 0.0538632906973362\n",
      "[INFO] - 2024-05-15 18:01:19,754 - UQpy: Scientific Machine Learning: Epoch 681 / 1,000 Train Loss 0.04105918305484872 Test Loss 0.05365748703479767\n",
      "[INFO] - 2024-05-15 18:01:23,295 - UQpy: Scientific Machine Learning: Epoch 682 / 1,000 Train Loss 0.0405217162088344 Test Loss 0.05361439287662506\n",
      "[INFO] - 2024-05-15 18:01:26,549 - UQpy: Scientific Machine Learning: Epoch 683 / 1,000 Train Loss 0.04029901335506063 Test Loss 0.053464777767658234\n",
      "[INFO] - 2024-05-15 18:01:29,791 - UQpy: Scientific Machine Learning: Epoch 684 / 1,000 Train Loss 0.03999818508562289 Test Loss 0.053072307258844376\n",
      "[INFO] - 2024-05-15 18:01:33,099 - UQpy: Scientific Machine Learning: Epoch 685 / 1,000 Train Loss 0.03969305871348632 Test Loss 0.052657950669527054\n",
      "[INFO] - 2024-05-15 18:01:36,405 - UQpy: Scientific Machine Learning: Epoch 686 / 1,000 Train Loss 0.03961606657034472 Test Loss 0.05234424024820328\n",
      "[INFO] - 2024-05-15 18:01:39,852 - UQpy: Scientific Machine Learning: Epoch 687 / 1,000 Train Loss 0.039464443137771206 Test Loss 0.0520905926823616\n",
      "[INFO] - 2024-05-15 18:01:43,515 - UQpy: Scientific Machine Learning: Epoch 688 / 1,000 Train Loss 0.03930483268279778 Test Loss 0.051982562988996506\n",
      "[INFO] - 2024-05-15 18:01:46,830 - UQpy: Scientific Machine Learning: Epoch 689 / 1,000 Train Loss 0.03921429881531941 Test Loss 0.05240379273891449\n",
      "[INFO] - 2024-05-15 18:01:50,125 - UQpy: Scientific Machine Learning: Epoch 690 / 1,000 Train Loss 0.039120931178331375 Test Loss 0.05199163407087326\n",
      "[INFO] - 2024-05-15 18:01:53,440 - UQpy: Scientific Machine Learning: Epoch 691 / 1,000 Train Loss 0.03891008386486455 Test Loss 0.051644906401634216\n",
      "[INFO] - 2024-05-15 18:01:56,750 - UQpy: Scientific Machine Learning: Epoch 692 / 1,000 Train Loss 0.038763519768652166 Test Loss 0.051788363605737686\n",
      "[INFO] - 2024-05-15 18:02:00,055 - UQpy: Scientific Machine Learning: Epoch 693 / 1,000 Train Loss 0.038908569338290316 Test Loss 0.05163492634892464\n",
      "[INFO] - 2024-05-15 18:02:03,340 - UQpy: Scientific Machine Learning: Epoch 694 / 1,000 Train Loss 0.03886924458569602 Test Loss 0.052181363105773926\n",
      "[INFO] - 2024-05-15 18:02:06,664 - UQpy: Scientific Machine Learning: Epoch 695 / 1,000 Train Loss 0.03869272846924631 Test Loss 0.051670752465724945\n",
      "[INFO] - 2024-05-15 18:02:10,002 - UQpy: Scientific Machine Learning: Epoch 696 / 1,000 Train Loss 0.03885690348320886 Test Loss 0.051672082394361496\n",
      "[INFO] - 2024-05-15 18:02:13,315 - UQpy: Scientific Machine Learning: Epoch 697 / 1,000 Train Loss 0.0386213166149039 Test Loss 0.051062628626823425\n",
      "[INFO] - 2024-05-15 18:02:16,598 - UQpy: Scientific Machine Learning: Epoch 698 / 1,000 Train Loss 0.03830881073678795 Test Loss 0.05074175074696541\n",
      "[INFO] - 2024-05-15 18:02:19,875 - UQpy: Scientific Machine Learning: Epoch 699 / 1,000 Train Loss 0.03822138336928267 Test Loss 0.05080549046397209\n",
      "[INFO] - 2024-05-15 18:02:23,149 - UQpy: Scientific Machine Learning: Epoch 700 / 1,000 Train Loss 0.03798969501727506 Test Loss 0.05058223009109497\n",
      "[INFO] - 2024-05-15 18:02:26,440 - UQpy: Scientific Machine Learning: Epoch 701 / 1,000 Train Loss 0.03799302650517539 Test Loss 0.0506632961332798\n",
      "[INFO] - 2024-05-15 18:02:29,735 - UQpy: Scientific Machine Learning: Epoch 702 / 1,000 Train Loss 0.03885420911798352 Test Loss 0.05196428671479225\n",
      "[INFO] - 2024-05-15 18:02:33,018 - UQpy: Scientific Machine Learning: Epoch 703 / 1,000 Train Loss 0.03946444813750292 Test Loss 0.051783159375190735\n",
      "[INFO] - 2024-05-15 18:02:36,324 - UQpy: Scientific Machine Learning: Epoch 704 / 1,000 Train Loss 0.0405532100090855 Test Loss 0.051087260246276855\n",
      "[INFO] - 2024-05-15 18:02:39,637 - UQpy: Scientific Machine Learning: Epoch 705 / 1,000 Train Loss 0.038312177222810294 Test Loss 0.051067981868982315\n",
      "[INFO] - 2024-05-15 18:02:42,932 - UQpy: Scientific Machine Learning: Epoch 706 / 1,000 Train Loss 0.03860955391275255 Test Loss 0.050983279943466187\n",
      "[INFO] - 2024-05-15 18:02:46,218 - UQpy: Scientific Machine Learning: Epoch 707 / 1,000 Train Loss 0.03907397193344016 Test Loss 0.05049433559179306\n",
      "[INFO] - 2024-05-15 18:02:49,489 - UQpy: Scientific Machine Learning: Epoch 708 / 1,000 Train Loss 0.037802869846162046 Test Loss 0.050112467259168625\n",
      "[INFO] - 2024-05-15 18:02:52,781 - UQpy: Scientific Machine Learning: Epoch 709 / 1,000 Train Loss 0.03759031899665531 Test Loss 0.05126447230577469\n",
      "[INFO] - 2024-05-15 18:02:56,069 - UQpy: Scientific Machine Learning: Epoch 710 / 1,000 Train Loss 0.038299297227671274 Test Loss 0.0495200976729393\n",
      "[INFO] - 2024-05-15 18:02:59,363 - UQpy: Scientific Machine Learning: Epoch 711 / 1,000 Train Loss 0.03738658326236825 Test Loss 0.04967198520898819\n",
      "[INFO] - 2024-05-15 18:03:02,762 - UQpy: Scientific Machine Learning: Epoch 712 / 1,000 Train Loss 0.03713446433999037 Test Loss 0.051813699305057526\n",
      "[INFO] - 2024-05-15 18:03:06,062 - UQpy: Scientific Machine Learning: Epoch 713 / 1,000 Train Loss 0.03774431200796052 Test Loss 0.050158508121967316\n",
      "[INFO] - 2024-05-15 18:03:09,387 - UQpy: Scientific Machine Learning: Epoch 714 / 1,000 Train Loss 0.03828516455465242 Test Loss 0.051331326365470886\n",
      "[INFO] - 2024-05-15 18:03:12,672 - UQpy: Scientific Machine Learning: Epoch 715 / 1,000 Train Loss 0.03740771016792247 Test Loss 0.050657738000154495\n",
      "[INFO] - 2024-05-15 18:03:16,365 - UQpy: Scientific Machine Learning: Epoch 716 / 1,000 Train Loss 0.037827249616384506 Test Loss 0.050815701484680176\n",
      "[INFO] - 2024-05-15 18:03:19,683 - UQpy: Scientific Machine Learning: Epoch 717 / 1,000 Train Loss 0.03919079899787903 Test Loss 0.05246096849441528\n",
      "[INFO] - 2024-05-15 18:03:22,980 - UQpy: Scientific Machine Learning: Epoch 718 / 1,000 Train Loss 0.038793827946248804 Test Loss 0.04907228797674179\n",
      "[INFO] - 2024-05-15 18:03:26,271 - UQpy: Scientific Machine Learning: Epoch 719 / 1,000 Train Loss 0.04019188959347574 Test Loss 0.058106571435928345\n",
      "[INFO] - 2024-05-15 18:03:29,565 - UQpy: Scientific Machine Learning: Epoch 720 / 1,000 Train Loss 0.04325772674852296 Test Loss 0.05468673259019852\n",
      "[INFO] - 2024-05-15 18:03:32,881 - UQpy: Scientific Machine Learning: Epoch 721 / 1,000 Train Loss 0.04189096077492362 Test Loss 0.0517379492521286\n",
      "[INFO] - 2024-05-15 18:03:36,504 - UQpy: Scientific Machine Learning: Epoch 722 / 1,000 Train Loss 0.039420591373192634 Test Loss 0.048910923302173615\n",
      "[INFO] - 2024-05-15 18:03:39,788 - UQpy: Scientific Machine Learning: Epoch 723 / 1,000 Train Loss 0.03724848959398897 Test Loss 0.05002375692129135\n",
      "[INFO] - 2024-05-15 18:03:43,113 - UQpy: Scientific Machine Learning: Epoch 724 / 1,000 Train Loss 0.036441773764396966 Test Loss 0.04809904843568802\n",
      "[INFO] - 2024-05-15 18:03:46,501 - UQpy: Scientific Machine Learning: Epoch 725 / 1,000 Train Loss 0.03573907921580892 Test Loss 0.047936197370290756\n",
      "[INFO] - 2024-05-15 18:03:49,818 - UQpy: Scientific Machine Learning: Epoch 726 / 1,000 Train Loss 0.035656448355630824 Test Loss 0.048386916518211365\n",
      "[INFO] - 2024-05-15 18:03:53,120 - UQpy: Scientific Machine Learning: Epoch 727 / 1,000 Train Loss 0.035740769125129045 Test Loss 0.047111134976148605\n",
      "[INFO] - 2024-05-15 18:03:56,406 - UQpy: Scientific Machine Learning: Epoch 728 / 1,000 Train Loss 0.035575285651966146 Test Loss 0.04842078685760498\n",
      "[INFO] - 2024-05-15 18:03:59,697 - UQpy: Scientific Machine Learning: Epoch 729 / 1,000 Train Loss 0.036674877628684044 Test Loss 0.04736590385437012\n",
      "[INFO] - 2024-05-15 18:04:03,006 - UQpy: Scientific Machine Learning: Epoch 730 / 1,000 Train Loss 0.03862766548991203 Test Loss 0.0523218959569931\n",
      "[INFO] - 2024-05-15 18:04:06,274 - UQpy: Scientific Machine Learning: Epoch 731 / 1,000 Train Loss 0.038037615877233054 Test Loss 0.04998914897441864\n",
      "[INFO] - 2024-05-15 18:04:09,560 - UQpy: Scientific Machine Learning: Epoch 732 / 1,000 Train Loss 0.03674016010604406 Test Loss 0.05093320459127426\n",
      "[INFO] - 2024-05-15 18:04:12,872 - UQpy: Scientific Machine Learning: Epoch 733 / 1,000 Train Loss 0.039128318428993225 Test Loss 0.060084544122219086\n",
      "[INFO] - 2024-05-15 18:04:16,352 - UQpy: Scientific Machine Learning: Epoch 734 / 1,000 Train Loss 0.03921749521242945 Test Loss 0.05118187144398689\n",
      "[INFO] - 2024-05-15 18:04:19,663 - UQpy: Scientific Machine Learning: Epoch 735 / 1,000 Train Loss 0.04192975369331084 Test Loss 0.05551024526357651\n",
      "[INFO] - 2024-05-15 18:04:22,951 - UQpy: Scientific Machine Learning: Epoch 736 / 1,000 Train Loss 0.053667553161319936 Test Loss 0.06630093604326248\n",
      "[INFO] - 2024-05-15 18:04:26,244 - UQpy: Scientific Machine Learning: Epoch 737 / 1,000 Train Loss 0.044991641354404 Test Loss 0.05708098039031029\n",
      "[INFO] - 2024-05-15 18:04:29,545 - UQpy: Scientific Machine Learning: Epoch 738 / 1,000 Train Loss 0.04372408770416912 Test Loss 0.05073925852775574\n",
      "[INFO] - 2024-05-15 18:04:32,944 - UQpy: Scientific Machine Learning: Epoch 739 / 1,000 Train Loss 0.03871723303669378 Test Loss 0.04887564480304718\n",
      "[INFO] - 2024-05-15 18:04:36,307 - UQpy: Scientific Machine Learning: Epoch 740 / 1,000 Train Loss 0.03571958928123901 Test Loss 0.04719998314976692\n",
      "[INFO] - 2024-05-15 18:04:39,586 - UQpy: Scientific Machine Learning: Epoch 741 / 1,000 Train Loss 0.03620692940526887 Test Loss 0.04693515598773956\n",
      "[INFO] - 2024-05-15 18:04:42,889 - UQpy: Scientific Machine Learning: Epoch 742 / 1,000 Train Loss 0.03537721598618909 Test Loss 0.046784911304712296\n",
      "[INFO] - 2024-05-15 18:04:46,201 - UQpy: Scientific Machine Learning: Epoch 743 / 1,000 Train Loss 0.03485859528576073 Test Loss 0.04900134727358818\n",
      "[INFO] - 2024-05-15 18:04:49,500 - UQpy: Scientific Machine Learning: Epoch 744 / 1,000 Train Loss 0.03587745982957514 Test Loss 0.04725760594010353\n",
      "[INFO] - 2024-05-15 18:04:52,685 - UQpy: Scientific Machine Learning: Epoch 745 / 1,000 Train Loss 0.03510663560346553 Test Loss 0.04673280194401741\n",
      "[INFO] - 2024-05-15 18:04:55,902 - UQpy: Scientific Machine Learning: Epoch 746 / 1,000 Train Loss 0.034401233553102144 Test Loss 0.04911159723997116\n",
      "[INFO] - 2024-05-15 18:04:59,534 - UQpy: Scientific Machine Learning: Epoch 747 / 1,000 Train Loss 0.03448622350237871 Test Loss 0.04551202058792114\n",
      "[INFO] - 2024-05-15 18:05:02,841 - UQpy: Scientific Machine Learning: Epoch 748 / 1,000 Train Loss 0.033644564351753184 Test Loss 0.04469946026802063\n",
      "[INFO] - 2024-05-15 18:05:06,148 - UQpy: Scientific Machine Learning: Epoch 749 / 1,000 Train Loss 0.033513781271482766 Test Loss 0.04593157768249512\n",
      "[INFO] - 2024-05-15 18:05:09,445 - UQpy: Scientific Machine Learning: Epoch 750 / 1,000 Train Loss 0.03355656564235687 Test Loss 0.045032940804958344\n",
      "[INFO] - 2024-05-15 18:05:12,776 - UQpy: Scientific Machine Learning: Epoch 751 / 1,000 Train Loss 0.0333983713859006 Test Loss 0.04483696073293686\n",
      "[INFO] - 2024-05-15 18:05:16,084 - UQpy: Scientific Machine Learning: Epoch 752 / 1,000 Train Loss 0.03335699135143506 Test Loss 0.044960856437683105\n",
      "[INFO] - 2024-05-15 18:05:19,493 - UQpy: Scientific Machine Learning: Epoch 753 / 1,000 Train Loss 0.03322082630505687 Test Loss 0.044065095484256744\n",
      "[INFO] - 2024-05-15 18:05:22,812 - UQpy: Scientific Machine Learning: Epoch 754 / 1,000 Train Loss 0.0341261850768014 Test Loss 0.048100296407938004\n",
      "[INFO] - 2024-05-15 18:05:26,087 - UQpy: Scientific Machine Learning: Epoch 755 / 1,000 Train Loss 0.034646580485921154 Test Loss 0.04571988433599472\n",
      "[INFO] - 2024-05-15 18:05:29,444 - UQpy: Scientific Machine Learning: Epoch 756 / 1,000 Train Loss 0.03394516449617712 Test Loss 0.04499319940805435\n",
      "[INFO] - 2024-05-15 18:05:32,734 - UQpy: Scientific Machine Learning: Epoch 757 / 1,000 Train Loss 0.0346019668014426 Test Loss 0.04645787551999092\n",
      "[INFO] - 2024-05-15 18:05:36,056 - UQpy: Scientific Machine Learning: Epoch 758 / 1,000 Train Loss 0.03390516095647687 Test Loss 0.04467307776212692\n",
      "[INFO] - 2024-05-15 18:05:39,333 - UQpy: Scientific Machine Learning: Epoch 759 / 1,000 Train Loss 0.03382760698073789 Test Loss 0.045331597328186035\n",
      "[INFO] - 2024-05-15 18:05:42,662 - UQpy: Scientific Machine Learning: Epoch 760 / 1,000 Train Loss 0.0343199654629356 Test Loss 0.046801697462797165\n",
      "[INFO] - 2024-05-15 18:05:45,955 - UQpy: Scientific Machine Learning: Epoch 761 / 1,000 Train Loss 0.037277738514699434 Test Loss 0.050005074590444565\n",
      "[INFO] - 2024-05-15 18:05:49,715 - UQpy: Scientific Machine Learning: Epoch 762 / 1,000 Train Loss 0.0352435277676896 Test Loss 0.04647721350193024\n",
      "[INFO] - 2024-05-15 18:05:53,008 - UQpy: Scientific Machine Learning: Epoch 763 / 1,000 Train Loss 0.033858952455614745 Test Loss 0.045849088579416275\n",
      "[INFO] - 2024-05-15 18:05:56,341 - UQpy: Scientific Machine Learning: Epoch 764 / 1,000 Train Loss 0.03355187677631253 Test Loss 0.04668651521205902\n",
      "[INFO] - 2024-05-15 18:05:59,642 - UQpy: Scientific Machine Learning: Epoch 765 / 1,000 Train Loss 0.03552055358886719 Test Loss 0.04619850963354111\n",
      "[INFO] - 2024-05-15 18:06:02,996 - UQpy: Scientific Machine Learning: Epoch 766 / 1,000 Train Loss 0.03350149418570494 Test Loss 0.045490384101867676\n",
      "[INFO] - 2024-05-15 18:06:06,305 - UQpy: Scientific Machine Learning: Epoch 767 / 1,000 Train Loss 0.032770688204388866 Test Loss 0.04352378472685814\n",
      "[INFO] - 2024-05-15 18:06:09,598 - UQpy: Scientific Machine Learning: Epoch 768 / 1,000 Train Loss 0.03278418610754766 Test Loss 0.0434582494199276\n",
      "[INFO] - 2024-05-15 18:06:12,900 - UQpy: Scientific Machine Learning: Epoch 769 / 1,000 Train Loss 0.03225527370446607 Test Loss 0.04324960708618164\n",
      "[INFO] - 2024-05-15 18:06:16,180 - UQpy: Scientific Machine Learning: Epoch 770 / 1,000 Train Loss 0.03227325058297107 Test Loss 0.043834660202264786\n",
      "[INFO] - 2024-05-15 18:06:19,837 - UQpy: Scientific Machine Learning: Epoch 771 / 1,000 Train Loss 0.0322414206242875 Test Loss 0.04267197847366333\n",
      "[INFO] - 2024-05-15 18:06:23,169 - UQpy: Scientific Machine Learning: Epoch 772 / 1,000 Train Loss 0.031252061458010426 Test Loss 0.04222509264945984\n",
      "[INFO] - 2024-05-15 18:06:26,482 - UQpy: Scientific Machine Learning: Epoch 773 / 1,000 Train Loss 0.03127473200622358 Test Loss 0.043373286724090576\n",
      "[INFO] - 2024-05-15 18:06:29,795 - UQpy: Scientific Machine Learning: Epoch 774 / 1,000 Train Loss 0.03209169316840799 Test Loss 0.043450117111206055\n",
      "[INFO] - 2024-05-15 18:06:33,097 - UQpy: Scientific Machine Learning: Epoch 775 / 1,000 Train Loss 0.03156393727189616 Test Loss 0.043531082570552826\n",
      "[INFO] - 2024-05-15 18:06:36,404 - UQpy: Scientific Machine Learning: Epoch 776 / 1,000 Train Loss 0.03216187146149183 Test Loss 0.04272618517279625\n",
      "[INFO] - 2024-05-15 18:06:39,708 - UQpy: Scientific Machine Learning: Epoch 777 / 1,000 Train Loss 0.03228172503019634 Test Loss 0.04480309784412384\n",
      "[INFO] - 2024-05-15 18:06:43,013 - UQpy: Scientific Machine Learning: Epoch 778 / 1,000 Train Loss 0.03360634699071709 Test Loss 0.04573105648159981\n",
      "[INFO] - 2024-05-15 18:06:46,343 - UQpy: Scientific Machine Learning: Epoch 779 / 1,000 Train Loss 0.03294871727886953 Test Loss 0.04603801295161247\n",
      "[INFO] - 2024-05-15 18:06:49,638 - UQpy: Scientific Machine Learning: Epoch 780 / 1,000 Train Loss 0.032647023957810904 Test Loss 0.04330296069383621\n",
      "[INFO] - 2024-05-15 18:06:52,953 - UQpy: Scientific Machine Learning: Epoch 781 / 1,000 Train Loss 0.03203500199474787 Test Loss 0.04324766993522644\n",
      "[INFO] - 2024-05-15 18:06:56,274 - UQpy: Scientific Machine Learning: Epoch 782 / 1,000 Train Loss 0.03464919260065807 Test Loss 0.05475565046072006\n",
      "[INFO] - 2024-05-15 18:06:59,597 - UQpy: Scientific Machine Learning: Epoch 783 / 1,000 Train Loss 0.03662598289941486 Test Loss 0.04375876858830452\n",
      "[INFO] - 2024-05-15 18:07:02,894 - UQpy: Scientific Machine Learning: Epoch 784 / 1,000 Train Loss 0.03752084752838863 Test Loss 0.04761820286512375\n",
      "[INFO] - 2024-05-15 18:07:06,183 - UQpy: Scientific Machine Learning: Epoch 785 / 1,000 Train Loss 0.03413624600752404 Test Loss 0.043002765625715256\n",
      "[INFO] - 2024-05-15 18:07:09,815 - UQpy: Scientific Machine Learning: Epoch 786 / 1,000 Train Loss 0.0318985886283611 Test Loss 0.042223405092954636\n",
      "[INFO] - 2024-05-15 18:07:13,456 - UQpy: Scientific Machine Learning: Epoch 787 / 1,000 Train Loss 0.031120505968206806 Test Loss 0.041833870112895966\n",
      "[INFO] - 2024-05-15 18:07:16,870 - UQpy: Scientific Machine Learning: Epoch 788 / 1,000 Train Loss 0.030718897126222913 Test Loss 0.04108128696680069\n",
      "[INFO] - 2024-05-15 18:07:20,140 - UQpy: Scientific Machine Learning: Epoch 789 / 1,000 Train Loss 0.03068721627718524 Test Loss 0.04089406877756119\n",
      "[INFO] - 2024-05-15 18:07:23,393 - UQpy: Scientific Machine Learning: Epoch 790 / 1,000 Train Loss 0.030749843308800144 Test Loss 0.042411334812641144\n",
      "[INFO] - 2024-05-15 18:07:26,569 - UQpy: Scientific Machine Learning: Epoch 791 / 1,000 Train Loss 0.030752418366701978 Test Loss 0.040386613458395004\n",
      "[INFO] - 2024-05-15 18:07:29,819 - UQpy: Scientific Machine Learning: Epoch 792 / 1,000 Train Loss 0.03195738439497195 Test Loss 0.04250648245215416\n",
      "[INFO] - 2024-05-15 18:07:33,184 - UQpy: Scientific Machine Learning: Epoch 793 / 1,000 Train Loss 0.03289975745505408 Test Loss 0.047717511653900146\n",
      "[INFO] - 2024-05-15 18:07:36,463 - UQpy: Scientific Machine Learning: Epoch 794 / 1,000 Train Loss 0.035973178320809415 Test Loss 0.04470107704401016\n",
      "[INFO] - 2024-05-15 18:07:39,749 - UQpy: Scientific Machine Learning: Epoch 795 / 1,000 Train Loss 0.03224799566363033 Test Loss 0.0434579998254776\n",
      "[INFO] - 2024-05-15 18:07:43,344 - UQpy: Scientific Machine Learning: Epoch 796 / 1,000 Train Loss 0.03243133513943145 Test Loss 0.041439976543188095\n",
      "[INFO] - 2024-05-15 18:07:46,661 - UQpy: Scientific Machine Learning: Epoch 797 / 1,000 Train Loss 0.04032911262229869 Test Loss 0.06353141367435455\n",
      "[INFO] - 2024-05-15 18:07:50,140 - UQpy: Scientific Machine Learning: Epoch 798 / 1,000 Train Loss 0.046011367812752724 Test Loss 0.04553250968456268\n",
      "[INFO] - 2024-05-15 18:07:53,537 - UQpy: Scientific Machine Learning: Epoch 799 / 1,000 Train Loss 0.034791111848072 Test Loss 0.049574993550777435\n",
      "[INFO] - 2024-05-15 18:07:56,957 - UQpy: Scientific Machine Learning: Epoch 800 / 1,000 Train Loss 0.03292053653613517 Test Loss 0.04461750388145447\n",
      "[INFO] - 2024-05-15 18:08:00,264 - UQpy: Scientific Machine Learning: Epoch 801 / 1,000 Train Loss 0.03898843769964419 Test Loss 0.048379093408584595\n",
      "[INFO] - 2024-05-15 18:08:03,601 - UQpy: Scientific Machine Learning: Epoch 802 / 1,000 Train Loss 0.03888182675367907 Test Loss 0.05280963331460953\n",
      "[INFO] - 2024-05-15 18:08:06,995 - UQpy: Scientific Machine Learning: Epoch 803 / 1,000 Train Loss 0.03487976248327054 Test Loss 0.044092386960983276\n",
      "[INFO] - 2024-05-15 18:08:10,442 - UQpy: Scientific Machine Learning: Epoch 804 / 1,000 Train Loss 0.03212625043172585 Test Loss 0.04364040121436119\n",
      "[INFO] - 2024-05-15 18:08:13,795 - UQpy: Scientific Machine Learning: Epoch 805 / 1,000 Train Loss 0.03162802226449314 Test Loss 0.04093374311923981\n",
      "[INFO] - 2024-05-15 18:08:17,108 - UQpy: Scientific Machine Learning: Epoch 806 / 1,000 Train Loss 0.03094295010362801 Test Loss 0.04419216513633728\n",
      "[INFO] - 2024-05-15 18:08:20,431 - UQpy: Scientific Machine Learning: Epoch 807 / 1,000 Train Loss 0.030748038111548675 Test Loss 0.042087990790605545\n",
      "[INFO] - 2024-05-15 18:08:23,788 - UQpy: Scientific Machine Learning: Epoch 808 / 1,000 Train Loss 0.03131474309453839 Test Loss 0.04244224354624748\n",
      "[INFO] - 2024-05-15 18:08:27,071 - UQpy: Scientific Machine Learning: Epoch 809 / 1,000 Train Loss 0.03096774151842845 Test Loss 0.04135638102889061\n",
      "[INFO] - 2024-05-15 18:08:30,368 - UQpy: Scientific Machine Learning: Epoch 810 / 1,000 Train Loss 0.030092746313465268 Test Loss 0.04029014706611633\n",
      "[INFO] - 2024-05-15 18:08:33,671 - UQpy: Scientific Machine Learning: Epoch 811 / 1,000 Train Loss 0.029718665406107903 Test Loss 0.04027062654495239\n",
      "[INFO] - 2024-05-15 18:08:36,962 - UQpy: Scientific Machine Learning: Epoch 812 / 1,000 Train Loss 0.029505988092798936 Test Loss 0.038605697453022\n",
      "[INFO] - 2024-05-15 18:08:40,267 - UQpy: Scientific Machine Learning: Epoch 813 / 1,000 Train Loss 0.0293051116168499 Test Loss 0.04006396234035492\n",
      "[INFO] - 2024-05-15 18:08:43,584 - UQpy: Scientific Machine Learning: Epoch 814 / 1,000 Train Loss 0.029425046847839104 Test Loss 0.04013293981552124\n",
      "[INFO] - 2024-05-15 18:08:46,879 - UQpy: Scientific Machine Learning: Epoch 815 / 1,000 Train Loss 0.02962759166563812 Test Loss 0.03990604355931282\n",
      "[INFO] - 2024-05-15 18:08:50,173 - UQpy: Scientific Machine Learning: Epoch 816 / 1,000 Train Loss 0.029093766879094273 Test Loss 0.03926286846399307\n",
      "[INFO] - 2024-05-15 18:08:53,489 - UQpy: Scientific Machine Learning: Epoch 817 / 1,000 Train Loss 0.0286991214869838 Test Loss 0.03861851990222931\n",
      "[INFO] - 2024-05-15 18:08:56,774 - UQpy: Scientific Machine Learning: Epoch 818 / 1,000 Train Loss 0.028819480322693523 Test Loss 0.039812732487916946\n",
      "[INFO] - 2024-05-15 18:09:00,065 - UQpy: Scientific Machine Learning: Epoch 819 / 1,000 Train Loss 0.028883298466864386 Test Loss 0.03899413347244263\n",
      "[INFO] - 2024-05-15 18:09:03,377 - UQpy: Scientific Machine Learning: Epoch 820 / 1,000 Train Loss 0.030211019182675762 Test Loss 0.04073592275381088\n",
      "[INFO] - 2024-05-15 18:09:06,664 - UQpy: Scientific Machine Learning: Epoch 821 / 1,000 Train Loss 0.029604380754263776 Test Loss 0.04078737273812294\n",
      "[INFO] - 2024-05-15 18:09:10,007 - UQpy: Scientific Machine Learning: Epoch 822 / 1,000 Train Loss 0.03193603671695057 Test Loss 0.04133612662553787\n",
      "[INFO] - 2024-05-15 18:09:13,402 - UQpy: Scientific Machine Learning: Epoch 823 / 1,000 Train Loss 0.030834246034684935 Test Loss 0.03995021432638168\n",
      "[INFO] - 2024-05-15 18:09:16,707 - UQpy: Scientific Machine Learning: Epoch 824 / 1,000 Train Loss 0.031437784531398824 Test Loss 0.04298090934753418\n",
      "[INFO] - 2024-05-15 18:09:20,117 - UQpy: Scientific Machine Learning: Epoch 825 / 1,000 Train Loss 0.03278530722385958 Test Loss 0.04211786389350891\n",
      "[INFO] - 2024-05-15 18:09:23,775 - UQpy: Scientific Machine Learning: Epoch 826 / 1,000 Train Loss 0.031338756315802276 Test Loss 0.04024533927440643\n",
      "[INFO] - 2024-05-15 18:09:27,080 - UQpy: Scientific Machine Learning: Epoch 827 / 1,000 Train Loss 0.030252800568153982 Test Loss 0.03914254158735275\n",
      "[INFO] - 2024-05-15 18:09:30,415 - UQpy: Scientific Machine Learning: Epoch 828 / 1,000 Train Loss 0.030464212067033116 Test Loss 0.04306970536708832\n",
      "[INFO] - 2024-05-15 18:09:33,713 - UQpy: Scientific Machine Learning: Epoch 829 / 1,000 Train Loss 0.032934502061260376 Test Loss 0.04617055132985115\n",
      "[INFO] - 2024-05-15 18:09:37,002 - UQpy: Scientific Machine Learning: Epoch 830 / 1,000 Train Loss 0.03986112998896524 Test Loss 0.04853293299674988\n",
      "[INFO] - 2024-05-15 18:09:40,326 - UQpy: Scientific Machine Learning: Epoch 831 / 1,000 Train Loss 0.03413254012794871 Test Loss 0.04776877909898758\n",
      "[INFO] - 2024-05-15 18:09:43,735 - UQpy: Scientific Machine Learning: Epoch 832 / 1,000 Train Loss 0.035630721794931514 Test Loss 0.04519825428724289\n",
      "[INFO] - 2024-05-15 18:09:47,161 - UQpy: Scientific Machine Learning: Epoch 833 / 1,000 Train Loss 0.032191395857616475 Test Loss 0.04362410306930542\n",
      "[INFO] - 2024-05-15 18:09:50,549 - UQpy: Scientific Machine Learning: Epoch 834 / 1,000 Train Loss 0.03149567701314625 Test Loss 0.04208558425307274\n",
      "[INFO] - 2024-05-15 18:09:54,179 - UQpy: Scientific Machine Learning: Epoch 835 / 1,000 Train Loss 0.030881188025600033 Test Loss 0.040638986974954605\n",
      "[INFO] - 2024-05-15 18:09:57,545 - UQpy: Scientific Machine Learning: Epoch 836 / 1,000 Train Loss 0.030928957325063254 Test Loss 0.04286912456154823\n",
      "[INFO] - 2024-05-15 18:10:00,881 - UQpy: Scientific Machine Learning: Epoch 837 / 1,000 Train Loss 0.03066900950905524 Test Loss 0.04133369401097298\n",
      "[INFO] - 2024-05-15 18:10:04,141 - UQpy: Scientific Machine Learning: Epoch 838 / 1,000 Train Loss 0.030603530081479174 Test Loss 0.038838762789964676\n",
      "[INFO] - 2024-05-15 18:10:07,473 - UQpy: Scientific Machine Learning: Epoch 839 / 1,000 Train Loss 0.0289219403149266 Test Loss 0.03877446800470352\n",
      "[INFO] - 2024-05-15 18:10:10,855 - UQpy: Scientific Machine Learning: Epoch 840 / 1,000 Train Loss 0.02817897302539725 Test Loss 0.03764205053448677\n",
      "[INFO] - 2024-05-15 18:10:14,197 - UQpy: Scientific Machine Learning: Epoch 841 / 1,000 Train Loss 0.02780761638362157 Test Loss 0.038728516548871994\n",
      "[INFO] - 2024-05-15 18:10:17,458 - UQpy: Scientific Machine Learning: Epoch 842 / 1,000 Train Loss 0.02782125545567588 Test Loss 0.03667239099740982\n",
      "[INFO] - 2024-05-15 18:10:20,847 - UQpy: Scientific Machine Learning: Epoch 843 / 1,000 Train Loss 0.027982536801382116 Test Loss 0.0393734946846962\n",
      "[INFO] - 2024-05-15 18:10:24,216 - UQpy: Scientific Machine Learning: Epoch 844 / 1,000 Train Loss 0.02839153759965771 Test Loss 0.037651438266038895\n",
      "[INFO] - 2024-05-15 18:10:27,518 - UQpy: Scientific Machine Learning: Epoch 845 / 1,000 Train Loss 0.028553784873924758 Test Loss 0.03820549696683884\n",
      "[INFO] - 2024-05-15 18:10:30,750 - UQpy: Scientific Machine Learning: Epoch 846 / 1,000 Train Loss 0.02807904662270295 Test Loss 0.03870945796370506\n",
      "[INFO] - 2024-05-15 18:10:34,012 - UQpy: Scientific Machine Learning: Epoch 847 / 1,000 Train Loss 0.027972958785922902 Test Loss 0.03691557049751282\n",
      "[INFO] - 2024-05-15 18:10:37,386 - UQpy: Scientific Machine Learning: Epoch 848 / 1,000 Train Loss 0.028571962722037967 Test Loss 0.037417907267808914\n",
      "[INFO] - 2024-05-15 18:10:40,818 - UQpy: Scientific Machine Learning: Epoch 849 / 1,000 Train Loss 0.02801303310613883 Test Loss 0.037067778408527374\n",
      "[INFO] - 2024-05-15 18:10:44,174 - UQpy: Scientific Machine Learning: Epoch 850 / 1,000 Train Loss 0.02745323785041508 Test Loss 0.03690369427204132\n",
      "[INFO] - 2024-05-15 18:10:47,429 - UQpy: Scientific Machine Learning: Epoch 851 / 1,000 Train Loss 0.027401388475769443 Test Loss 0.03703250735998154\n",
      "[INFO] - 2024-05-15 18:10:50,651 - UQpy: Scientific Machine Learning: Epoch 852 / 1,000 Train Loss 0.02830482450755019 Test Loss 0.039504364132881165\n",
      "[INFO] - 2024-05-15 18:10:53,877 - UQpy: Scientific Machine Learning: Epoch 853 / 1,000 Train Loss 0.02879632068307776 Test Loss 0.0373174250125885\n",
      "[INFO] - 2024-05-15 18:10:57,116 - UQpy: Scientific Machine Learning: Epoch 854 / 1,000 Train Loss 0.02908770230255629 Test Loss 0.03830328956246376\n",
      "[INFO] - 2024-05-15 18:11:00,477 - UQpy: Scientific Machine Learning: Epoch 855 / 1,000 Train Loss 0.02865636436954925 Test Loss 0.0375169962644577\n",
      "[INFO] - 2024-05-15 18:11:03,770 - UQpy: Scientific Machine Learning: Epoch 856 / 1,000 Train Loss 0.02882593576061098 Test Loss 0.04026586562395096\n",
      "[INFO] - 2024-05-15 18:11:07,071 - UQpy: Scientific Machine Learning: Epoch 857 / 1,000 Train Loss 0.0303993952509604 Test Loss 0.039683014154434204\n",
      "[INFO] - 2024-05-15 18:11:10,369 - UQpy: Scientific Machine Learning: Epoch 858 / 1,000 Train Loss 0.028866558874908247 Test Loss 0.03953104838728905\n",
      "[INFO] - 2024-05-15 18:11:13,655 - UQpy: Scientific Machine Learning: Epoch 859 / 1,000 Train Loss 0.028574118194611448 Test Loss 0.03695327788591385\n",
      "[INFO] - 2024-05-15 18:11:16,989 - UQpy: Scientific Machine Learning: Epoch 860 / 1,000 Train Loss 0.027039812485638418 Test Loss 0.03626126050949097\n",
      "[INFO] - 2024-05-15 18:11:20,321 - UQpy: Scientific Machine Learning: Epoch 861 / 1,000 Train Loss 0.026976824885136204 Test Loss 0.03820211812853813\n",
      "[INFO] - 2024-05-15 18:11:23,638 - UQpy: Scientific Machine Learning: Epoch 862 / 1,000 Train Loss 0.027928161091710393 Test Loss 0.03730866312980652\n",
      "[INFO] - 2024-05-15 18:11:26,940 - UQpy: Scientific Machine Learning: Epoch 863 / 1,000 Train Loss 0.029764670584546894 Test Loss 0.04446204751729965\n",
      "[INFO] - 2024-05-15 18:11:30,226 - UQpy: Scientific Machine Learning: Epoch 864 / 1,000 Train Loss 0.03130786797325862 Test Loss 0.041949331760406494\n",
      "[INFO] - 2024-05-15 18:11:33,542 - UQpy: Scientific Machine Learning: Epoch 865 / 1,000 Train Loss 0.031164292344137243 Test Loss 0.0414394810795784\n",
      "[INFO] - 2024-05-15 18:11:36,829 - UQpy: Scientific Machine Learning: Epoch 866 / 1,000 Train Loss 0.02898772737305415 Test Loss 0.03615792840719223\n",
      "[INFO] - 2024-05-15 18:11:40,112 - UQpy: Scientific Machine Learning: Epoch 867 / 1,000 Train Loss 0.02798080601190266 Test Loss 0.037088558077812195\n",
      "[INFO] - 2024-05-15 18:11:43,400 - UQpy: Scientific Machine Learning: Epoch 868 / 1,000 Train Loss 0.027220472791477254 Test Loss 0.03797420859336853\n",
      "[INFO] - 2024-05-15 18:11:46,697 - UQpy: Scientific Machine Learning: Epoch 869 / 1,000 Train Loss 0.027501876514993216 Test Loss 0.0363365039229393\n",
      "[INFO] - 2024-05-15 18:11:49,979 - UQpy: Scientific Machine Learning: Epoch 870 / 1,000 Train Loss 0.02692421947262789 Test Loss 0.03654639422893524\n",
      "[INFO] - 2024-05-15 18:11:53,239 - UQpy: Scientific Machine Learning: Epoch 871 / 1,000 Train Loss 0.02676058286114743 Test Loss 0.03568851575255394\n",
      "[INFO] - 2024-05-15 18:11:56,514 - UQpy: Scientific Machine Learning: Epoch 872 / 1,000 Train Loss 0.028565151812998874 Test Loss 0.04292645677924156\n",
      "[INFO] - 2024-05-15 18:11:59,814 - UQpy: Scientific Machine Learning: Epoch 873 / 1,000 Train Loss 0.0304595434940175 Test Loss 0.03807344660162926\n",
      "[INFO] - 2024-05-15 18:12:03,118 - UQpy: Scientific Machine Learning: Epoch 874 / 1,000 Train Loss 0.028498899681787742 Test Loss 0.03636159375309944\n",
      "[INFO] - 2024-05-15 18:12:06,405 - UQpy: Scientific Machine Learning: Epoch 875 / 1,000 Train Loss 0.02791420703655795 Test Loss 0.036261770874261856\n",
      "[INFO] - 2024-05-15 18:12:09,683 - UQpy: Scientific Machine Learning: Epoch 876 / 1,000 Train Loss 0.026544038598474703 Test Loss 0.03605080023407936\n",
      "[INFO] - 2024-05-15 18:12:12,970 - UQpy: Scientific Machine Learning: Epoch 877 / 1,000 Train Loss 0.026025250161948957 Test Loss 0.03644733130931854\n",
      "[INFO] - 2024-05-15 18:12:16,251 - UQpy: Scientific Machine Learning: Epoch 878 / 1,000 Train Loss 0.026598356075977023 Test Loss 0.03492090478539467\n",
      "[INFO] - 2024-05-15 18:12:19,541 - UQpy: Scientific Machine Learning: Epoch 879 / 1,000 Train Loss 0.027221727528070148 Test Loss 0.036509010940790176\n",
      "[INFO] - 2024-05-15 18:12:22,833 - UQpy: Scientific Machine Learning: Epoch 880 / 1,000 Train Loss 0.02934146015659759 Test Loss 0.03916972875595093\n",
      "[INFO] - 2024-05-15 18:12:26,128 - UQpy: Scientific Machine Learning: Epoch 881 / 1,000 Train Loss 0.02774885708564206 Test Loss 0.037536121904850006\n",
      "[INFO] - 2024-05-15 18:12:29,419 - UQpy: Scientific Machine Learning: Epoch 882 / 1,000 Train Loss 0.029136390866417634 Test Loss 0.04112149402499199\n",
      "[INFO] - 2024-05-15 18:12:32,718 - UQpy: Scientific Machine Learning: Epoch 883 / 1,000 Train Loss 0.0289018217866358 Test Loss 0.03793240338563919\n",
      "[INFO] - 2024-05-15 18:12:36,014 - UQpy: Scientific Machine Learning: Epoch 884 / 1,000 Train Loss 0.028984080411885913 Test Loss 0.03530344367027283\n",
      "[INFO] - 2024-05-15 18:12:39,302 - UQpy: Scientific Machine Learning: Epoch 885 / 1,000 Train Loss 0.027371611152040332 Test Loss 0.04043005406856537\n",
      "[INFO] - 2024-05-15 18:12:42,576 - UQpy: Scientific Machine Learning: Epoch 886 / 1,000 Train Loss 0.027578931595934063 Test Loss 0.034394994378089905\n",
      "[INFO] - 2024-05-15 18:12:45,853 - UQpy: Scientific Machine Learning: Epoch 887 / 1,000 Train Loss 0.026961715499821463 Test Loss 0.036723989993333817\n",
      "[INFO] - 2024-05-15 18:12:49,178 - UQpy: Scientific Machine Learning: Epoch 888 / 1,000 Train Loss 0.027136231037346942 Test Loss 0.03586526960134506\n",
      "[INFO] - 2024-05-15 18:12:52,519 - UQpy: Scientific Machine Learning: Epoch 889 / 1,000 Train Loss 0.026837410993481938 Test Loss 0.0347791388630867\n",
      "[INFO] - 2024-05-15 18:12:55,824 - UQpy: Scientific Machine Learning: Epoch 890 / 1,000 Train Loss 0.02701008672776975 Test Loss 0.03619982302188873\n",
      "[INFO] - 2024-05-15 18:12:59,133 - UQpy: Scientific Machine Learning: Epoch 891 / 1,000 Train Loss 0.027748954923529374 Test Loss 0.04047832265496254\n",
      "[INFO] - 2024-05-15 18:13:02,423 - UQpy: Scientific Machine Learning: Epoch 892 / 1,000 Train Loss 0.029417104038753007 Test Loss 0.04096700996160507\n",
      "[INFO] - 2024-05-15 18:13:05,705 - UQpy: Scientific Machine Learning: Epoch 893 / 1,000 Train Loss 0.03416326867514535 Test Loss 0.04349035769701004\n",
      "[INFO] - 2024-05-15 18:13:09,020 - UQpy: Scientific Machine Learning: Epoch 894 / 1,000 Train Loss 0.03549145005251232 Test Loss 0.06451873481273651\n",
      "[INFO] - 2024-05-15 18:13:12,277 - UQpy: Scientific Machine Learning: Epoch 895 / 1,000 Train Loss 0.05589805602243072 Test Loss 0.06680928170681\n",
      "[INFO] - 2024-05-15 18:13:15,543 - UQpy: Scientific Machine Learning: Epoch 896 / 1,000 Train Loss 0.04671407687036615 Test Loss 0.055389873683452606\n",
      "[INFO] - 2024-05-15 18:13:18,741 - UQpy: Scientific Machine Learning: Epoch 897 / 1,000 Train Loss 0.03561965454565851 Test Loss 0.03951284661889076\n",
      "[INFO] - 2024-05-15 18:13:21,974 - UQpy: Scientific Machine Learning: Epoch 898 / 1,000 Train Loss 0.029982831133039373 Test Loss 0.03820735961198807\n",
      "[INFO] - 2024-05-15 18:13:25,272 - UQpy: Scientific Machine Learning: Epoch 899 / 1,000 Train Loss 0.030039517306967786 Test Loss 0.03768127039074898\n",
      "[INFO] - 2024-05-15 18:13:28,524 - UQpy: Scientific Machine Learning: Epoch 900 / 1,000 Train Loss 0.029560566830791925 Test Loss 0.04162308946251869\n",
      "[INFO] - 2024-05-15 18:13:31,793 - UQpy: Scientific Machine Learning: Epoch 901 / 1,000 Train Loss 0.028016583503861176 Test Loss 0.03988691791892052\n",
      "[INFO] - 2024-05-15 18:13:35,030 - UQpy: Scientific Machine Learning: Epoch 902 / 1,000 Train Loss 0.028094257865297168 Test Loss 0.03508719056844711\n",
      "[INFO] - 2024-05-15 18:13:38,328 - UQpy: Scientific Machine Learning: Epoch 903 / 1,000 Train Loss 0.026990737962095362 Test Loss 0.035924032330513\n",
      "[INFO] - 2024-05-15 18:13:41,610 - UQpy: Scientific Machine Learning: Epoch 904 / 1,000 Train Loss 0.025970667796699626 Test Loss 0.03452266752719879\n",
      "[INFO] - 2024-05-15 18:13:44,851 - UQpy: Scientific Machine Learning: Epoch 905 / 1,000 Train Loss 0.02516893787603629 Test Loss 0.03365829214453697\n",
      "[INFO] - 2024-05-15 18:13:48,093 - UQpy: Scientific Machine Learning: Epoch 906 / 1,000 Train Loss 0.024924865580703084 Test Loss 0.03347872570157051\n",
      "[INFO] - 2024-05-15 18:13:51,298 - UQpy: Scientific Machine Learning: Epoch 907 / 1,000 Train Loss 0.024863757291122487 Test Loss 0.03331727907061577\n",
      "[INFO] - 2024-05-15 18:13:54,501 - UQpy: Scientific Machine Learning: Epoch 908 / 1,000 Train Loss 0.024520974508241603 Test Loss 0.03291269391775131\n",
      "[INFO] - 2024-05-15 18:13:57,712 - UQpy: Scientific Machine Learning: Epoch 909 / 1,000 Train Loss 0.02438801870142159 Test Loss 0.03298262506723404\n",
      "[INFO] - 2024-05-15 18:14:00,952 - UQpy: Scientific Machine Learning: Epoch 910 / 1,000 Train Loss 0.024460722349191968 Test Loss 0.03342636674642563\n",
      "[INFO] - 2024-05-15 18:14:04,178 - UQpy: Scientific Machine Learning: Epoch 911 / 1,000 Train Loss 0.024519667127414754 Test Loss 0.03302444890141487\n",
      "[INFO] - 2024-05-15 18:14:07,513 - UQpy: Scientific Machine Learning: Epoch 912 / 1,000 Train Loss 0.024623386561870575 Test Loss 0.03328995406627655\n",
      "[INFO] - 2024-05-15 18:14:10,818 - UQpy: Scientific Machine Learning: Epoch 913 / 1,000 Train Loss 0.024296523022808526 Test Loss 0.0330052375793457\n",
      "[INFO] - 2024-05-15 18:14:14,104 - UQpy: Scientific Machine Learning: Epoch 914 / 1,000 Train Loss 0.024315550923347473 Test Loss 0.033467039465904236\n",
      "[INFO] - 2024-05-15 18:14:17,400 - UQpy: Scientific Machine Learning: Epoch 915 / 1,000 Train Loss 0.02442842231769311 Test Loss 0.0329909548163414\n",
      "[INFO] - 2024-05-15 18:14:20,663 - UQpy: Scientific Machine Learning: Epoch 916 / 1,000 Train Loss 0.024243753795561037 Test Loss 0.03271424025297165\n",
      "[INFO] - 2024-05-15 18:14:23,877 - UQpy: Scientific Machine Learning: Epoch 917 / 1,000 Train Loss 0.024195835974655654 Test Loss 0.03304939344525337\n",
      "[INFO] - 2024-05-15 18:14:27,090 - UQpy: Scientific Machine Learning: Epoch 918 / 1,000 Train Loss 0.024141936464921424 Test Loss 0.03262393921613693\n",
      "[INFO] - 2024-05-15 18:14:30,276 - UQpy: Scientific Machine Learning: Epoch 919 / 1,000 Train Loss 0.024286224555812384 Test Loss 0.03253468498587608\n",
      "[INFO] - 2024-05-15 18:14:33,474 - UQpy: Scientific Machine Learning: Epoch 920 / 1,000 Train Loss 0.024234077059908918 Test Loss 0.03278643265366554\n",
      "[INFO] - 2024-05-15 18:14:36,700 - UQpy: Scientific Machine Learning: Epoch 921 / 1,000 Train Loss 0.024188169033119555 Test Loss 0.032600849866867065\n",
      "[INFO] - 2024-05-15 18:14:39,916 - UQpy: Scientific Machine Learning: Epoch 922 / 1,000 Train Loss 0.024603730949916337 Test Loss 0.03322678059339523\n",
      "[INFO] - 2024-05-15 18:14:43,139 - UQpy: Scientific Machine Learning: Epoch 923 / 1,000 Train Loss 0.024685388921122802 Test Loss 0.03330283612012863\n",
      "[INFO] - 2024-05-15 18:14:46,355 - UQpy: Scientific Machine Learning: Epoch 924 / 1,000 Train Loss 0.025506321830969108 Test Loss 0.03619037941098213\n",
      "[INFO] - 2024-05-15 18:14:49,551 - UQpy: Scientific Machine Learning: Epoch 925 / 1,000 Train Loss 0.027158277991570924 Test Loss 0.03554968535900116\n",
      "[INFO] - 2024-05-15 18:14:52,731 - UQpy: Scientific Machine Learning: Epoch 926 / 1,000 Train Loss 0.03073622649045367 Test Loss 0.03793402761220932\n",
      "[INFO] - 2024-05-15 18:14:55,934 - UQpy: Scientific Machine Learning: Epoch 927 / 1,000 Train Loss 0.02754846107410757 Test Loss 0.03645423799753189\n",
      "[INFO] - 2024-05-15 18:14:59,184 - UQpy: Scientific Machine Learning: Epoch 928 / 1,000 Train Loss 0.02795931755712158 Test Loss 0.037343405187129974\n",
      "[INFO] - 2024-05-15 18:15:02,380 - UQpy: Scientific Machine Learning: Epoch 929 / 1,000 Train Loss 0.027153084348691136 Test Loss 0.034925200045108795\n",
      "[INFO] - 2024-05-15 18:15:05,612 - UQpy: Scientific Machine Learning: Epoch 930 / 1,000 Train Loss 0.025211806752179797 Test Loss 0.03433404862880707\n",
      "[INFO] - 2024-05-15 18:15:08,836 - UQpy: Scientific Machine Learning: Epoch 931 / 1,000 Train Loss 0.024897189026600437 Test Loss 0.03292771056294441\n",
      "[INFO] - 2024-05-15 18:15:12,058 - UQpy: Scientific Machine Learning: Epoch 932 / 1,000 Train Loss 0.02488933876156807 Test Loss 0.03257334232330322\n",
      "[INFO] - 2024-05-15 18:15:15,295 - UQpy: Scientific Machine Learning: Epoch 933 / 1,000 Train Loss 0.024116009963970435 Test Loss 0.032280635088682175\n",
      "[INFO] - 2024-05-15 18:15:18,548 - UQpy: Scientific Machine Learning: Epoch 934 / 1,000 Train Loss 0.023668081254551287 Test Loss 0.03206709772348404\n",
      "[INFO] - 2024-05-15 18:15:21,756 - UQpy: Scientific Machine Learning: Epoch 935 / 1,000 Train Loss 0.023811966082767436 Test Loss 0.03193463385105133\n",
      "[INFO] - 2024-05-15 18:15:25,002 - UQpy: Scientific Machine Learning: Epoch 936 / 1,000 Train Loss 0.02382039072874345 Test Loss 0.032048482447862625\n",
      "[INFO] - 2024-05-15 18:15:28,252 - UQpy: Scientific Machine Learning: Epoch 937 / 1,000 Train Loss 0.023990530912813387 Test Loss 0.033968109637498856\n",
      "[INFO] - 2024-05-15 18:15:31,469 - UQpy: Scientific Machine Learning: Epoch 938 / 1,000 Train Loss 0.027972128046186345 Test Loss 0.03632575646042824\n",
      "[INFO] - 2024-05-15 18:15:34,723 - UQpy: Scientific Machine Learning: Epoch 939 / 1,000 Train Loss 0.032440080944644775 Test Loss 0.04422309994697571\n",
      "[INFO] - 2024-05-15 18:15:37,970 - UQpy: Scientific Machine Learning: Epoch 940 / 1,000 Train Loss 0.03178154667349238 Test Loss 0.04001408815383911\n",
      "[INFO] - 2024-05-15 18:15:41,209 - UQpy: Scientific Machine Learning: Epoch 941 / 1,000 Train Loss 0.029811482582437366 Test Loss 0.03550490364432335\n",
      "[INFO] - 2024-05-15 18:15:44,446 - UQpy: Scientific Machine Learning: Epoch 942 / 1,000 Train Loss 0.02566391464911009 Test Loss 0.03289756923913956\n",
      "[INFO] - 2024-05-15 18:15:47,696 - UQpy: Scientific Machine Learning: Epoch 943 / 1,000 Train Loss 0.024327423717630536 Test Loss 0.032006558030843735\n",
      "[INFO] - 2024-05-15 18:15:50,905 - UQpy: Scientific Machine Learning: Epoch 944 / 1,000 Train Loss 0.023734211529556074 Test Loss 0.03176674246788025\n",
      "[INFO] - 2024-05-15 18:15:54,106 - UQpy: Scientific Machine Learning: Epoch 945 / 1,000 Train Loss 0.02347221696063092 Test Loss 0.031367652118206024\n",
      "[INFO] - 2024-05-15 18:15:57,317 - UQpy: Scientific Machine Learning: Epoch 946 / 1,000 Train Loss 0.023779069514651047 Test Loss 0.032149866223335266\n",
      "[INFO] - 2024-05-15 18:16:00,601 - UQpy: Scientific Machine Learning: Epoch 947 / 1,000 Train Loss 0.023429932464894495 Test Loss 0.03260444104671478\n",
      "[INFO] - 2024-05-15 18:16:03,854 - UQpy: Scientific Machine Learning: Epoch 948 / 1,000 Train Loss 0.0239576109145817 Test Loss 0.03206798806786537\n",
      "[INFO] - 2024-05-15 18:16:07,059 - UQpy: Scientific Machine Learning: Epoch 949 / 1,000 Train Loss 0.02435853508742232 Test Loss 0.032430220395326614\n",
      "[INFO] - 2024-05-15 18:16:10,246 - UQpy: Scientific Machine Learning: Epoch 950 / 1,000 Train Loss 0.023936137949165544 Test Loss 0.031874880194664\n",
      "[INFO] - 2024-05-15 18:16:13,409 - UQpy: Scientific Machine Learning: Epoch 951 / 1,000 Train Loss 0.02367138215585759 Test Loss 0.03110470436513424\n",
      "[INFO] - 2024-05-15 18:16:16,651 - UQpy: Scientific Machine Learning: Epoch 952 / 1,000 Train Loss 0.023879157281235644 Test Loss 0.034176819026470184\n",
      "[INFO] - 2024-05-15 18:16:20,004 - UQpy: Scientific Machine Learning: Epoch 953 / 1,000 Train Loss 0.025683039603264707 Test Loss 0.03303605690598488\n",
      "[INFO] - 2024-05-15 18:16:23,305 - UQpy: Scientific Machine Learning: Epoch 954 / 1,000 Train Loss 0.025702232494950294 Test Loss 0.0357828289270401\n",
      "[INFO] - 2024-05-15 18:16:27,071 - UQpy: Scientific Machine Learning: Epoch 955 / 1,000 Train Loss 0.025174350918907868 Test Loss 0.03244940936565399\n",
      "[INFO] - 2024-05-15 18:16:30,335 - UQpy: Scientific Machine Learning: Epoch 956 / 1,000 Train Loss 0.024840046014440686 Test Loss 0.036664579063653946\n",
      "[INFO] - 2024-05-15 18:16:34,096 - UQpy: Scientific Machine Learning: Epoch 957 / 1,000 Train Loss 0.02572060150927619 Test Loss 0.034588463604450226\n",
      "[INFO] - 2024-05-15 18:16:37,333 - UQpy: Scientific Machine Learning: Epoch 958 / 1,000 Train Loss 0.026423666136045205 Test Loss 0.03329596295952797\n",
      "[INFO] - 2024-05-15 18:16:40,532 - UQpy: Scientific Machine Learning: Epoch 959 / 1,000 Train Loss 0.024152034109360294 Test Loss 0.033102475106716156\n",
      "[INFO] - 2024-05-15 18:16:43,779 - UQpy: Scientific Machine Learning: Epoch 960 / 1,000 Train Loss 0.026479108553183705 Test Loss 0.037785306572914124\n",
      "[INFO] - 2024-05-15 18:16:47,013 - UQpy: Scientific Machine Learning: Epoch 961 / 1,000 Train Loss 0.029014345062406438 Test Loss 0.03620176389813423\n",
      "[INFO] - 2024-05-15 18:16:50,294 - UQpy: Scientific Machine Learning: Epoch 962 / 1,000 Train Loss 0.027657137889611095 Test Loss 0.03332221880555153\n",
      "[INFO] - 2024-05-15 18:16:53,575 - UQpy: Scientific Machine Learning: Epoch 963 / 1,000 Train Loss 0.026270203194335887 Test Loss 0.032480042427778244\n",
      "[INFO] - 2024-05-15 18:16:56,796 - UQpy: Scientific Machine Learning: Epoch 964 / 1,000 Train Loss 0.024231212801839177 Test Loss 0.03179938346147537\n",
      "[INFO] - 2024-05-15 18:17:00,015 - UQpy: Scientific Machine Learning: Epoch 965 / 1,000 Train Loss 0.023745284856934296 Test Loss 0.03230113536119461\n",
      "[INFO] - 2024-05-15 18:17:03,262 - UQpy: Scientific Machine Learning: Epoch 966 / 1,000 Train Loss 0.024056263366027883 Test Loss 0.03292626142501831\n",
      "[INFO] - 2024-05-15 18:17:06,651 - UQpy: Scientific Machine Learning: Epoch 967 / 1,000 Train Loss 0.023190213661444813 Test Loss 0.030565321445465088\n",
      "[INFO] - 2024-05-15 18:17:10,920 - UQpy: Scientific Machine Learning: Epoch 968 / 1,000 Train Loss 0.02265779017225692 Test Loss 0.03035404160618782\n",
      "[INFO] - 2024-05-15 18:17:14,223 - UQpy: Scientific Machine Learning: Epoch 969 / 1,000 Train Loss 0.022927609125250263 Test Loss 0.0309542715549469\n",
      "[INFO] - 2024-05-15 18:17:17,551 - UQpy: Scientific Machine Learning: Epoch 970 / 1,000 Train Loss 0.022598030143662504 Test Loss 0.03205147758126259\n",
      "[INFO] - 2024-05-15 18:17:20,854 - UQpy: Scientific Machine Learning: Epoch 971 / 1,000 Train Loss 0.02274152332622754 Test Loss 0.030280310660600662\n",
      "[INFO] - 2024-05-15 18:17:24,163 - UQpy: Scientific Machine Learning: Epoch 972 / 1,000 Train Loss 0.02255607474791376 Test Loss 0.03133015334606171\n",
      "[INFO] - 2024-05-15 18:17:27,487 - UQpy: Scientific Machine Learning: Epoch 973 / 1,000 Train Loss 0.023901230508559627 Test Loss 0.03283111751079559\n",
      "[INFO] - 2024-05-15 18:17:30,797 - UQpy: Scientific Machine Learning: Epoch 974 / 1,000 Train Loss 0.025063453634318552 Test Loss 0.03586934134364128\n",
      "[INFO] - 2024-05-15 18:17:34,240 - UQpy: Scientific Machine Learning: Epoch 975 / 1,000 Train Loss 0.026609677135160093 Test Loss 0.03521163389086723\n",
      "[INFO] - 2024-05-15 18:17:37,562 - UQpy: Scientific Machine Learning: Epoch 976 / 1,000 Train Loss 0.0251736438980228 Test Loss 0.03236360847949982\n",
      "[INFO] - 2024-05-15 18:17:40,898 - UQpy: Scientific Machine Learning: Epoch 977 / 1,000 Train Loss 0.024562513455748558 Test Loss 0.031246311962604523\n",
      "[INFO] - 2024-05-15 18:17:44,240 - UQpy: Scientific Machine Learning: Epoch 978 / 1,000 Train Loss 0.02340170221501275 Test Loss 0.031264446675777435\n",
      "[INFO] - 2024-05-15 18:17:48,636 - UQpy: Scientific Machine Learning: Epoch 979 / 1,000 Train Loss 0.024684846303180644 Test Loss 0.03184540569782257\n",
      "[INFO] - 2024-05-15 18:17:51,925 - UQpy: Scientific Machine Learning: Epoch 980 / 1,000 Train Loss 0.02310861127549096 Test Loss 0.03108879365026951\n",
      "[INFO] - 2024-05-15 18:17:55,224 - UQpy: Scientific Machine Learning: Epoch 981 / 1,000 Train Loss 0.0229443594519245 Test Loss 0.029887039214372635\n",
      "[INFO] - 2024-05-15 18:17:58,522 - UQpy: Scientific Machine Learning: Epoch 982 / 1,000 Train Loss 0.02257483050619301 Test Loss 0.030232613906264305\n",
      "[INFO] - 2024-05-15 18:18:01,835 - UQpy: Scientific Machine Learning: Epoch 983 / 1,000 Train Loss 0.023523629152853238 Test Loss 0.03124196082353592\n",
      "[INFO] - 2024-05-15 18:18:05,160 - UQpy: Scientific Machine Learning: Epoch 984 / 1,000 Train Loss 0.023190952445331373 Test Loss 0.031218640506267548\n",
      "[INFO] - 2024-05-15 18:18:09,006 - UQpy: Scientific Machine Learning: Epoch 985 / 1,000 Train Loss 0.023858282617048213 Test Loss 0.032457463443279266\n",
      "[INFO] - 2024-05-15 18:18:13,156 - UQpy: Scientific Machine Learning: Epoch 986 / 1,000 Train Loss 0.028078807615920118 Test Loss 0.03890395909547806\n",
      "[INFO] - 2024-05-15 18:18:16,479 - UQpy: Scientific Machine Learning: Epoch 987 / 1,000 Train Loss 0.02854227019768012 Test Loss 0.03517497330904007\n",
      "[INFO] - 2024-05-15 18:18:19,798 - UQpy: Scientific Machine Learning: Epoch 988 / 1,000 Train Loss 0.0300991674394984 Test Loss 0.04059937596321106\n",
      "[INFO] - 2024-05-15 18:18:23,095 - UQpy: Scientific Machine Learning: Epoch 989 / 1,000 Train Loss 0.029945344026935727 Test Loss 0.03676456958055496\n",
      "[INFO] - 2024-05-15 18:18:26,427 - UQpy: Scientific Machine Learning: Epoch 990 / 1,000 Train Loss 0.0275431025381151 Test Loss 0.041370440274477005\n",
      "[INFO] - 2024-05-15 18:18:29,734 - UQpy: Scientific Machine Learning: Epoch 991 / 1,000 Train Loss 0.02784217119609055 Test Loss 0.03548063710331917\n",
      "[INFO] - 2024-05-15 18:18:33,034 - UQpy: Scientific Machine Learning: Epoch 992 / 1,000 Train Loss 0.0248251469119599 Test Loss 0.030499085783958435\n",
      "[INFO] - 2024-05-15 18:18:36,344 - UQpy: Scientific Machine Learning: Epoch 993 / 1,000 Train Loss 0.022867910269843906 Test Loss 0.03039976954460144\n",
      "[INFO] - 2024-05-15 18:18:39,649 - UQpy: Scientific Machine Learning: Epoch 994 / 1,000 Train Loss 0.022129004036909657 Test Loss 0.02967892214655876\n",
      "[INFO] - 2024-05-15 18:18:42,845 - UQpy: Scientific Machine Learning: Epoch 995 / 1,000 Train Loss 0.021827750790276025 Test Loss 0.030133016407489777\n",
      "[INFO] - 2024-05-15 18:18:46,055 - UQpy: Scientific Machine Learning: Epoch 996 / 1,000 Train Loss 0.02196029480546713 Test Loss 0.031123368069529533\n",
      "[INFO] - 2024-05-15 18:18:49,446 - UQpy: Scientific Machine Learning: Epoch 997 / 1,000 Train Loss 0.021799499365059954 Test Loss 0.029113218188285828\n",
      "[INFO] - 2024-05-15 18:18:52,754 - UQpy: Scientific Machine Learning: Epoch 998 / 1,000 Train Loss 0.02155446388611668 Test Loss 0.029601050540804863\n",
      "[INFO] - 2024-05-15 18:18:56,050 - UQpy: Scientific Machine Learning: Epoch 999 / 1,000 Train Loss 0.02155668121811591 Test Loss 0.02933347597718239\n",
      "[INFO] - 2024-05-15 18:18:59,428 - UQpy: Scientific Machine Learning: Epoch 1,000 / 1,000 Train Loss 0.021572412432808625 Test Loss 0.029442530125379562\n",
      "[INFO] - 2024-05-15 18:18:59,428 - UQpy: Scientific Machine Learning: Completed training and testing DeepOperatorNetwork\n"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "\n",
    "trainer.run(train_data=train_data, test_data=test_data, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0ba93e3-e0da-42b7-8670-660d1d1e4d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate test data and save results\n",
    "def eval_model(test_data,model):\n",
    "    model.eval()\n",
    "    ux_pred_list = []\n",
    "    uy_pred_list = []\n",
    "    ux_test_list = []\n",
    "    uy_test_list = []\n",
    "    x_list = []\n",
    "    for batch_number, (*x, y) in enumerate(test_data):\n",
    "        ux_pred, uy_pred = model(*x)\n",
    "        ux_test , uy_test = y\n",
    "        ux_pred_list.append(ux_pred)\n",
    "        uy_pred_list.append(uy_pred)\n",
    "        ux_test_list.append(ux_test)\n",
    "        uy_test_list.append(uy_test)\n",
    "        x_list.append(x[1][:,0,:])\n",
    "    return torch.cat(ux_pred_list), torch.cat(uy_pred_list), torch.cat(ux_test_list), torch.cat(uy_test_list), torch.cat(x_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f82c3dad-2381-4102-bad9-221889debb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "ux_pred, uy_pred, ux_test, uy_test, x_test = eval_model(test_data,model)\n",
    "ux_pred = rescale(ux_pred.detach(), np.squeeze(ux_train_mean, axis=2), np.squeeze(ux_train_std, axis=2))\n",
    "uy_pred = rescale(uy_pred.detach(), np.squeeze(uy_train_mean, axis=2), np.squeeze(uy_train_std, axis=2))\n",
    "ux_test = rescale(ux_test.detach(), np.squeeze(ux_train_mean, axis=2), np.squeeze(ux_train_std, axis=2))\n",
    "uy_test = rescale(uy_test.detach(), np.squeeze(uy_train_mean, axis=2), np.squeeze(uy_train_std, axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66f0309b-2f91-4abd-87f8-2f47a8a760d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "savemat('Elastic_plate.mat',{'x_test': x_test.detach().numpy(), 'ux_test': ux_test.detach().numpy(), 'uy_test': uy_test.detach().numpy(), \n",
    "                              'ux_pred': ux_pred.detach().numpy(), 'uy_pred': uy_pred.detach().numpy()} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c467ba-99b0-4e10-917e-31792cd1ba92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
