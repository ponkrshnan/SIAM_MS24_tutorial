{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37a4b674-0473-46a4-9aa9-a2cda04d1b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from scipy.io import savemat\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from UQpy.scientific_machine_learning.neural_networks import DeepOperatorNetwork\n",
    "from UQpy.scientific_machine_learning.trainers import BBBTrainer\n",
    "from UQpy.scientific_machine_learning.layers.BayesianLayer import BayesianLayer\n",
    "from UQpy.scientific_machine_learning.layers.BayesianConvLayer import BayesianConvLayer\n",
    "from dataset import load_data, rescale\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"UQpy\")\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17d6fc72-0ff3-4945-ab2d-3f581a502593",
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = {\n",
    "    \"prior_mu\": 0,\n",
    "    \"prior_sigma\": 0.01,\n",
    "    \"posterior_mu_initial\": (0, 0.1),\n",
    "    \"posterior_rho_initial\": (-5, 0.1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4bc664d-fb76-4527-b177-ae2916482300",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BranchNet(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.fnn = nn.Sequential(BayesianLayer(101, 100, priors=priors), nn.Tanh())\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            BayesianConvLayer(1, 16, (5, 5), padding=\"same\", priors=priors),\n",
    "            nn.AvgPool2d(2, 1, padding=0),\n",
    "            BayesianConvLayer(16, 16, (5, 5), padding=\"same\", priors=priors),\n",
    "            nn.AvgPool2d(2, 1, padding=0),\n",
    "            BayesianConvLayer(16, 16, (5, 5), padding=\"same\", priors=priors),\n",
    "            nn.AvgPool2d(2, 1, padding=0),\n",
    "            BayesianConvLayer(16, 64, (5, 5), padding=\"same\", priors=priors),\n",
    "            nn.AvgPool2d(2, 1, padding=0),\n",
    "        )\n",
    "        self.dnn = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            BayesianLayer(64 * 6 * 6, 512, priors=priors),\n",
    "            nn.Tanh(),\n",
    "            BayesianLayer(512, 512, priors=priors),\n",
    "            nn.Tanh(),\n",
    "            BayesianLayer(512, 200, priors=priors),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fnn(x)\n",
    "        x = x.view(-1, 1, 10, 10)\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.dnn(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74450db9-f474-4145-81ee-f52751107cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrunkNet(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.fnn = nn.Sequential(\n",
    "            BayesianLayer(2, 128, priors=priors),\n",
    "            nn.Tanh(),\n",
    "            BayesianLayer(128, 128, priors=priors),\n",
    "            nn.Tanh(),\n",
    "            BayesianLayer(128, 128, priors=priors),\n",
    "            nn.Tanh(),\n",
    "            BayesianLayer(128, 200, priors=priors),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.Xmin = np.array([0.0, 0.0]).reshape((-1, 2))\n",
    "        self.Xmax = np.array([1.0, 1.0]).reshape((-1, 2))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = 2.0 * (x - self.Xmin) / (self.Xmax - self.Xmin) - 1.0\n",
    "        x = x.float()\n",
    "        x = self.fnn(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3ae6c79-5430-4f85-b89e-acbb288ca0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "branch_network = BranchNet()\n",
    "trunk_network = TrunkNet()\n",
    "model = DeepOperatorNetwork(branch_network, trunk_network, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edefb1a3-0736-4777-ac64-3a06082dcecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets and create data loaders\n",
    "class ElasticityDataSet(Dataset):\n",
    "    \"\"\"Load the Elasticity dataset\"\"\"\n",
    "\n",
    "    def __init__(self, x, f_x, u_x, u_y):\n",
    "        self.x = x\n",
    "        self.f_x = f_x\n",
    "        self.u_x = u_x\n",
    "        self.u_y = u_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.f_x.shape[0])\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.x, self.f_x[i, :], (self.u_x[i, :, 0], self.u_y[i, :, 0])\n",
    "\n",
    "(F_train,Ux_train, Uy_train, F_test, Ux_test, Uy_test,\n",
    "    X, ux_train_mean, ux_train_std, uy_train_mean, uy_train_std,) = load_data()\n",
    "train_data = DataLoader(\n",
    "    ElasticityDataSet(\n",
    "        np.float32(X), np.float32(F_train), np.float32(Ux_train), np.float32(Uy_train)\n",
    "    ),\n",
    "    batch_size=100,\n",
    "    shuffle=True,\n",
    ")\n",
    "test_data = DataLoader(\n",
    "    ElasticityDataSet(\n",
    "        np.float32(X), np.float32(F_test), np.float32(Ux_test), np.float32(Uy_test)\n",
    "    ),\n",
    "    batch_size=100,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44b73243-76e6-4873-b705-6076ad23ae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, reduction: str = \"mean\", *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, prediction, label):\n",
    "        return F.mse_loss(\n",
    "            prediction[0], label[0], reduction=self.reduction\n",
    "        ) + F.mse_loss(prediction[1], label[1], reduction=self.reduction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46193a23-2b8b-415a-acf9-0205b66df0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "trainer = BBBTrainer(model, optimizer, LossFunction())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ac34a06-0e7a-4ec5-8f66-d68b4a70bab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] - 2024-05-16 10:58:07,132 - UQpy: Scientific Machine Learning: Beginning training and testing DeepOperatorNetwork\n",
      "[INFO] - 2024-05-16 10:58:11,128 - UQpy: Scientific Machine Learning: Epoch 1 / 1,000 Train Loss 44.90986723648874 Train NLL 44.07970503756874 Train KL 83016216.0 Test NLL 9.500065803527832\n",
      "[INFO] - 2024-05-16 10:58:15,075 - UQpy: Scientific Machine Learning: Epoch 2 / 1,000 Train Loss 6.856808285964163 Train NLL 6.025748554028962 Train KL 83105968.0 Test NLL 2.323637008666992\n",
      "[INFO] - 2024-05-16 10:58:19,024 - UQpy: Scientific Machine Learning: Epoch 3 / 1,000 Train Loss 3.8570807984000757 Train NLL 3.025841656484102 Train KL 83123920.0 Test NLL 2.029174566268921\n",
      "[INFO] - 2024-05-16 10:58:22,955 - UQpy: Scientific Machine Learning: Epoch 4 / 1,000 Train Loss 3.18944209500363 Train NLL 2.358229687339381 Train KL 83121232.0 Test NLL 2.0012998580932617\n",
      "[INFO] - 2024-05-16 10:58:26,905 - UQpy: Scientific Machine Learning: Epoch 5 / 1,000 Train Loss 2.802559174989399 Train NLL 1.9714141393962659 Train KL 83114512.0 Test NLL 2.388293743133545\n",
      "[INFO] - 2024-05-16 10:58:30,858 - UQpy: Scientific Machine Learning: Epoch 6 / 1,000 Train Loss 2.9266050112874886 Train NLL 2.09554217991076 Train KL 83106280.0 Test NLL 2.618558406829834\n",
      "[INFO] - 2024-05-16 10:58:34,807 - UQpy: Scientific Machine Learning: Epoch 7 / 1,000 Train Loss 2.70058810710907 Train NLL 1.8696132584622032 Train KL 83097472.0 Test NLL 1.8278443813323975\n",
      "[INFO] - 2024-05-16 10:58:38,756 - UQpy: Scientific Machine Learning: Epoch 8 / 1,000 Train Loss 2.2943658577768424 Train NLL 1.4634875341465599 Train KL 83087832.0 Test NLL 1.3428065776824951\n",
      "[INFO] - 2024-05-16 10:58:42,684 - UQpy: Scientific Machine Learning: Epoch 9 / 1,000 Train Loss 2.058147330033152 Train NLL 1.2273719342131364 Train KL 83077536.0 Test NLL 1.519848346710205\n",
      "[INFO] - 2024-05-16 10:58:46,649 - UQpy: Scientific Machine Learning: Epoch 10 / 1,000 Train Loss 2.1930106752797176 Train NLL 1.3623422039182562 Train KL 83066856.0 Test NLL 1.146481990814209\n",
      "[INFO] - 2024-05-16 10:58:50,612 - UQpy: Scientific Machine Learning: Epoch 11 / 1,000 Train Loss 2.0870134391282735 Train NLL 1.2564582824707031 Train KL 83055512.0 Test NLL 2.0009188652038574\n",
      "[INFO] - 2024-05-16 10:58:54,603 - UQpy: Scientific Machine Learning: Epoch 12 / 1,000 Train Loss 2.2184306947808516 Train NLL 1.3879950862181814 Train KL 83043560.0 Test NLL 1.4468579292297363\n",
      "[INFO] - 2024-05-16 10:58:58,582 - UQpy: Scientific Machine Learning: Epoch 13 / 1,000 Train Loss 2.047235972002933 Train NLL 1.2169188857078552 Train KL 83031720.0 Test NLL 1.0841493606567383\n",
      "[INFO] - 2024-05-16 10:59:02,555 - UQpy: Scientific Machine Learning: Epoch 14 / 1,000 Train Loss 2.0195916138197245 Train NLL 1.189398633806329 Train KL 83019304.0 Test NLL 1.2001945972442627\n",
      "[INFO] - 2024-05-16 10:59:06,573 - UQpy: Scientific Machine Learning: Epoch 15 / 1,000 Train Loss 2.215240064420198 Train NLL 1.385176608436986 Train KL 83006352.0 Test NLL 1.2182835340499878\n",
      "[INFO] - 2024-05-16 10:59:10,584 - UQpy: Scientific Machine Learning: Epoch 16 / 1,000 Train Loss 2.06321152260429 Train NLL 1.2332814806386043 Train KL 82993008.0 Test NLL 1.3499925136566162\n",
      "[INFO] - 2024-05-16 10:59:14,542 - UQpy: Scientific Machine Learning: Epoch 17 / 1,000 Train Loss 2.085826365571273 Train NLL 1.2560305752252277 Train KL 82979592.0 Test NLL 1.1644878387451172\n",
      "[INFO] - 2024-05-16 10:59:18,554 - UQpy: Scientific Machine Learning: Epoch 18 / 1,000 Train Loss 1.9234722789965177 Train NLL 1.0938183382937783 Train KL 82965400.0 Test NLL 0.9496784210205078\n",
      "[INFO] - 2024-05-16 10:59:22,556 - UQpy: Scientific Machine Learning: Epoch 19 / 1,000 Train Loss 1.858881021800794 Train NLL 1.029374266925611 Train KL 82950680.0 Test NLL 1.4290146827697754\n",
      "[INFO] - 2024-05-16 10:59:26,550 - UQpy: Scientific Machine Learning: Epoch 20 / 1,000 Train Loss 1.8993567040092068 Train NLL 1.0699976180729114 Train KL 82935912.0 Test NLL 1.2165251970291138\n",
      "[INFO] - 2024-05-16 10:59:30,543 - UQpy: Scientific Machine Learning: Epoch 21 / 1,000 Train Loss 1.8703139957628752 Train NLL 1.0411104810865301 Train KL 82920344.0 Test NLL 0.97305828332901\n",
      "[INFO] - 2024-05-16 10:59:34,540 - UQpy: Scientific Machine Learning: Epoch 22 / 1,000 Train Loss 2.224523186683655 Train NLL 1.3954786030869735 Train KL 82904456.0 Test NLL 1.0405542850494385\n",
      "[INFO] - 2024-05-16 10:59:38,581 - UQpy: Scientific Machine Learning: Epoch 23 / 1,000 Train Loss 2.1834595642591776 Train NLL 1.3545717471524288 Train KL 82888784.0 Test NLL 1.04396653175354\n",
      "[INFO] - 2024-05-16 10:59:42,607 - UQpy: Scientific Machine Learning: Epoch 24 / 1,000 Train Loss 2.002632586579574 Train NLL 1.1738999862419932 Train KL 82873264.0 Test NLL 1.2138488292694092\n",
      "[INFO] - 2024-05-16 10:59:46,614 - UQpy: Scientific Machine Learning: Epoch 25 / 1,000 Train Loss 1.7927324207205522 Train NLL 0.9641644484118411 Train KL 82856792.0 Test NLL 1.1254830360412598\n",
      "[INFO] - 2024-05-16 10:59:50,644 - UQpy: Scientific Machine Learning: Epoch 26 / 1,000 Train Loss 1.9443262501766807 Train NLL 1.1159325242042542 Train KL 82839376.0 Test NLL 1.2370424270629883\n",
      "[INFO] - 2024-05-16 10:59:54,659 - UQpy: Scientific Machine Learning: Epoch 27 / 1,000 Train Loss 1.8258156964653416 Train NLL 0.9975972081485548 Train KL 82821856.0 Test NLL 1.3997929096221924\n",
      "[INFO] - 2024-05-16 10:59:58,653 - UQpy: Scientific Machine Learning: Epoch 28 / 1,000 Train Loss 1.960226855779949 Train NLL 1.132194437478718 Train KL 82803232.0 Test NLL 0.9833470582962036\n",
      "[INFO] - 2024-05-16 11:00:02,692 - UQpy: Scientific Machine Learning: Epoch 29 / 1,000 Train Loss 1.8213789525784945 Train NLL 0.9935337869744552 Train KL 82784504.0 Test NLL 0.9792987108230591\n",
      "[INFO] - 2024-05-16 11:00:06,732 - UQpy: Scientific Machine Learning: Epoch 30 / 1,000 Train Loss 1.7233809671903912 Train NLL 0.8957290210221943 Train KL 82765200.0 Test NLL 1.2659626007080078\n",
      "[INFO] - 2024-05-16 11:00:12,911 - UQpy: Scientific Machine Learning: Epoch 31 / 1,000 Train Loss 1.7944210328553851 Train NLL 0.9669665066819442 Train KL 82745448.0 Test NLL 0.9447519183158875\n",
      "[INFO] - 2024-05-16 11:00:16,916 - UQpy: Scientific Machine Learning: Epoch 32 / 1,000 Train Loss 1.8008311045797247 Train NLL 0.9735737035149022 Train KL 82725744.0 Test NLL 0.8750472068786621\n",
      "[INFO] - 2024-05-16 11:00:20,926 - UQpy: Scientific Machine Learning: Epoch 33 / 1,000 Train Loss 1.83394539983649 Train NLL 1.0068893150279397 Train KL 82705608.0 Test NLL 1.1857848167419434\n",
      "[INFO] - 2024-05-16 11:00:24,970 - UQpy: Scientific Machine Learning: Epoch 34 / 1,000 Train Loss 1.7628676389393054 Train NLL 0.9360185767474928 Train KL 82684904.0 Test NLL 1.392443299293518\n",
      "[INFO] - 2024-05-16 11:00:28,993 - UQpy: Scientific Machine Learning: Epoch 35 / 1,000 Train Loss 1.7549903706500405 Train NLL 0.9283547307315626 Train KL 82663560.0 Test NLL 1.4651210308074951\n",
      "[INFO] - 2024-05-16 11:00:33,017 - UQpy: Scientific Machine Learning: Epoch 36 / 1,000 Train Loss 1.7697299467889887 Train NLL 0.9433096772746036 Train KL 82642024.0 Test NLL 1.5523804426193237\n",
      "[INFO] - 2024-05-16 11:00:37,144 - UQpy: Scientific Machine Learning: Epoch 37 / 1,000 Train Loss 1.6806057315123708 Train NLL 0.8544006253543653 Train KL 82620496.0 Test NLL 0.7418212294578552\n",
      "[INFO] - 2024-05-16 11:00:41,305 - UQpy: Scientific Machine Learning: Epoch 38 / 1,000 Train Loss 1.6575249747226113 Train NLL 0.8315398191150866 Train KL 82598520.0 Test NLL 0.8848773241043091\n",
      "[INFO] - 2024-05-16 11:00:45,543 - UQpy: Scientific Machine Learning: Epoch 39 / 1,000 Train Loss 1.590378435034501 Train NLL 0.7646217471674869 Train KL 82575664.0 Test NLL 0.8845487833023071\n",
      "[INFO] - 2024-05-16 11:00:50,077 - UQpy: Scientific Machine Learning: Epoch 40 / 1,000 Train Loss 1.7016446527681852 Train NLL 0.8761168749708879 Train KL 82552768.0 Test NLL 1.0119197368621826\n",
      "[INFO] - 2024-05-16 11:00:54,389 - UQpy: Scientific Machine Learning: Epoch 41 / 1,000 Train Loss 1.9386999418860988 Train NLL 1.1133982413693477 Train KL 82530160.0 Test NLL 0.8440076112747192\n",
      "[INFO] - 2024-05-16 11:00:59,100 - UQpy: Scientific Machine Learning: Epoch 42 / 1,000 Train Loss 1.720111188135649 Train NLL 0.8950301942072416 Train KL 82508096.0 Test NLL 0.7797176241874695\n",
      "[INFO] - 2024-05-16 11:01:03,577 - UQpy: Scientific Machine Learning: Epoch 43 / 1,000 Train Loss 1.7630075404518528 Train NLL 0.9381646388455441 Train KL 82484288.0 Test NLL 1.436753749847412\n",
      "[INFO] - 2024-05-16 11:01:07,616 - UQpy: Scientific Machine Learning: Epoch 44 / 1,000 Train Loss 1.8345900272068225 Train NLL 1.009987849938242 Train KL 82460216.0 Test NLL 0.8666874170303345\n",
      "[INFO] - 2024-05-16 11:01:11,630 - UQpy: Scientific Machine Learning: Epoch 45 / 1,000 Train Loss 1.6483219924725985 Train NLL 0.8239624312049464 Train KL 82435944.0 Test NLL 1.0358314514160156\n",
      "[INFO] - 2024-05-16 11:01:15,796 - UQpy: Scientific Machine Learning: Epoch 46 / 1,000 Train Loss 1.7003975228259438 Train NLL 0.8762906915263126 Train KL 82410672.0 Test NLL 1.2083394527435303\n",
      "[INFO] - 2024-05-16 11:01:19,816 - UQpy: Scientific Machine Learning: Epoch 47 / 1,000 Train Loss 1.6565727560143721 Train NLL 0.8327184852800871 Train KL 82385440.0 Test NLL 1.0074323415756226\n",
      "[INFO] - 2024-05-16 11:01:23,849 - UQpy: Scientific Machine Learning: Epoch 48 / 1,000 Train Loss 1.6366561400262933 Train NLL 0.8130609675457603 Train KL 82359528.0 Test NLL 0.8982675075531006\n",
      "[INFO] - 2024-05-16 11:01:27,992 - UQpy: Scientific Machine Learning: Epoch 49 / 1,000 Train Loss 1.7825940219979537 Train NLL 0.9592625467400802 Train KL 82333152.0 Test NLL 1.0751254558563232\n",
      "[INFO] - 2024-05-16 11:01:32,225 - UQpy: Scientific Machine Learning: Epoch 50 / 1,000 Train Loss 1.7731765194943077 Train NLL 0.9501081830576846 Train KL 82306840.0 Test NLL 1.1388359069824219\n",
      "[INFO] - 2024-05-16 11:01:36,464 - UQpy: Scientific Machine Learning: Epoch 51 / 1,000 Train Loss 1.6611758282310085 Train NLL 0.8383707592361852 Train KL 82280504.0 Test NLL 1.0090303421020508\n",
      "[INFO] - 2024-05-16 11:01:40,643 - UQpy: Scientific Machine Learning: Epoch 52 / 1,000 Train Loss 1.924272098039326 Train NLL 1.1017431208961888 Train KL 82252904.0 Test NLL 1.321117877960205\n",
      "[INFO] - 2024-05-16 11:01:44,829 - UQpy: Scientific Machine Learning: Epoch 53 / 1,000 Train Loss 1.9877598411158512 Train NLL 1.1655058829407943 Train KL 82225392.0 Test NLL 1.2607684135437012\n",
      "[INFO] - 2024-05-16 11:01:49,143 - UQpy: Scientific Machine Learning: Epoch 54 / 1,000 Train Loss 1.731336323838485 Train NLL 0.9093533817090487 Train KL 82198296.0 Test NLL 1.1589176654815674\n",
      "[INFO] - 2024-05-16 11:01:53,332 - UQpy: Scientific Machine Learning: Epoch 55 / 1,000 Train Loss 1.715125937210886 Train NLL 0.8934240372557389 Train KL 82170192.0 Test NLL 0.7787271738052368\n",
      "[INFO] - 2024-05-16 11:01:57,553 - UQpy: Scientific Machine Learning: Epoch 56 / 1,000 Train Loss 1.557564333865517 Train NLL 0.7361479527071902 Train KL 82141640.0 Test NLL 0.9788110256195068\n",
      "[INFO] - 2024-05-16 11:02:01,817 - UQpy: Scientific Machine Learning: Epoch 57 / 1,000 Train Loss 1.6858916282653809 Train NLL 0.8647709143789191 Train KL 82112072.0 Test NLL 1.0031285285949707\n",
      "[INFO] - 2024-05-16 11:02:05,988 - UQpy: Scientific Machine Learning: Epoch 58 / 1,000 Train Loss 1.76528715459924 Train NLL 0.944462145629682 Train KL 82082496.0 Test NLL 1.106619954109192\n",
      "[INFO] - 2024-05-16 11:02:10,138 - UQpy: Scientific Machine Learning: Epoch 59 / 1,000 Train Loss 1.6609837444205033 Train NLL 0.8404573609954432 Train KL 82052632.0 Test NLL 0.832768440246582\n",
      "[INFO] - 2024-05-16 11:02:14,390 - UQpy: Scientific Machine Learning: Epoch 60 / 1,000 Train Loss 1.595542550086975 Train NLL 0.7753192280468187 Train KL 82022336.0 Test NLL 0.8282393217086792\n",
      "[INFO] - 2024-05-16 11:02:18,607 - UQpy: Scientific Machine Learning: Epoch 61 / 1,000 Train Loss 1.6333591436084949 Train NLL 0.813445972768884 Train KL 81991320.0 Test NLL 1.563211441040039\n",
      "[INFO] - 2024-05-16 11:02:22,855 - UQpy: Scientific Machine Learning: Epoch 62 / 1,000 Train Loss 1.7114517939718146 Train NLL 0.8918503083680805 Train KL 81960160.0 Test NLL 1.0979093313217163\n",
      "[INFO] - 2024-05-16 11:02:27,050 - UQpy: Scientific Machine Learning: Epoch 63 / 1,000 Train Loss 1.6820452966188129 Train NLL 0.8627529081545378 Train KL 81929248.0 Test NLL 0.8184548616409302\n",
      "[INFO] - 2024-05-16 11:02:31,208 - UQpy: Scientific Machine Learning: Epoch 64 / 1,000 Train Loss 1.924989982655174 Train NLL 1.1060093766764592 Train KL 81898064.0 Test NLL 1.3466682434082031\n",
      "[INFO] - 2024-05-16 11:02:35,329 - UQpy: Scientific Machine Learning: Epoch 65 / 1,000 Train Loss 1.9495172124159963 Train NLL 1.1308499355065196 Train KL 81866728.0 Test NLL 1.5796880722045898\n",
      "[INFO] - 2024-05-16 11:02:39,502 - UQpy: Scientific Machine Learning: Epoch 66 / 1,000 Train Loss 1.7244210117741634 Train NLL 0.9060725789321097 Train KL 81834840.0 Test NLL 1.2625560760498047\n",
      "[INFO] - 2024-05-16 11:02:43,656 - UQpy: Scientific Machine Learning: Epoch 67 / 1,000 Train Loss 1.6384061951386302 Train NLL 0.8203860458574797 Train KL 81802008.0 Test NLL 1.085744857788086\n",
      "[INFO] - 2024-05-16 11:02:47,875 - UQpy: Scientific Machine Learning: Epoch 68 / 1,000 Train Loss 1.727169532524912 Train NLL 0.9094813811151605 Train KL 81768824.0 Test NLL 0.865526556968689\n",
      "[INFO] - 2024-05-16 11:02:52,053 - UQpy: Scientific Machine Learning: Epoch 69 / 1,000 Train Loss 1.6239772219406932 Train NLL 0.8066229851622331 Train KL 81735424.0 Test NLL 1.0653016567230225\n",
      "[INFO] - 2024-05-16 11:02:56,232 - UQpy: Scientific Machine Learning: Epoch 70 / 1,000 Train Loss 1.7470177663000006 Train NLL 0.9300021748793753 Train KL 81701568.0 Test NLL 1.1046435832977295\n",
      "[INFO] - 2024-05-16 11:03:00,468 - UQpy: Scientific Machine Learning: Epoch 71 / 1,000 Train Loss 1.5740240686818172 Train NLL 0.7573519756919459 Train KL 81667208.0 Test NLL 0.8582695722579956\n",
      "[INFO] - 2024-05-16 11:03:04,613 - UQpy: Scientific Machine Learning: Epoch 72 / 1,000 Train Loss 1.5631547978049831 Train NLL 0.7468295411059731 Train KL 81632528.0 Test NLL 0.8801267147064209\n",
      "[INFO] - 2024-05-16 11:03:08,828 - UQpy: Scientific Machine Learning: Epoch 73 / 1,000 Train Loss 1.5755236337059422 Train NLL 0.7595480586353102 Train KL 81597552.0 Test NLL 1.038353681564331\n",
      "[INFO] - 2024-05-16 11:03:12,983 - UQpy: Scientific Machine Learning: Epoch 74 / 1,000 Train Loss 1.5928415561977185 Train NLL 0.7772175622613806 Train KL 81562392.0 Test NLL 0.8600726127624512\n",
      "[INFO] - 2024-05-16 11:03:17,118 - UQpy: Scientific Machine Learning: Epoch 75 / 1,000 Train Loss 1.637683535877027 Train NLL 0.8224150271792161 Train KL 81526848.0 Test NLL 0.7487884163856506\n",
      "[INFO] - 2024-05-16 11:03:21,271 - UQpy: Scientific Machine Learning: Epoch 76 / 1,000 Train Loss 1.5582797966505353 Train NLL 0.7433807034241525 Train KL 81489904.0 Test NLL 0.7616791725158691\n",
      "[INFO] - 2024-05-16 11:03:25,421 - UQpy: Scientific Machine Learning: Epoch 77 / 1,000 Train Loss 1.474050615963183 Train NLL 0.6595239137348375 Train KL 81452680.0 Test NLL 0.7818243503570557\n",
      "[INFO] - 2024-05-16 11:03:29,585 - UQpy: Scientific Machine Learning: Epoch 78 / 1,000 Train Loss 1.5423880940989445 Train NLL 0.7282388241667497 Train KL 81414936.0 Test NLL 0.9183304309844971\n",
      "[INFO] - 2024-05-16 11:03:33,769 - UQpy: Scientific Machine Learning: Epoch 79 / 1,000 Train Loss 1.5969900833932977 Train NLL 0.7832210910947699 Train KL 81376896.0 Test NLL 1.1041826009750366\n",
      "[INFO] - 2024-05-16 11:03:37,946 - UQpy: Scientific Machine Learning: Epoch 80 / 1,000 Train Loss 1.5076354365599782 Train NLL 0.6942470356037742 Train KL 81338840.0 Test NLL 0.9011929631233215\n",
      "[INFO] - 2024-05-16 11:03:42,127 - UQpy: Scientific Machine Learning: Epoch 81 / 1,000 Train Loss 1.5361810671655756 Train NLL 0.7231850310375816 Train KL 81299608.0 Test NLL 0.7675504684448242\n",
      "[INFO] - 2024-05-16 11:03:46,290 - UQpy: Scientific Machine Learning: Epoch 82 / 1,000 Train Loss 1.610325813293457 Train NLL 0.79771909274553 Train KL 81260664.0 Test NLL 1.100281000137329\n",
      "[INFO] - 2024-05-16 11:03:50,433 - UQpy: Scientific Machine Learning: Epoch 83 / 1,000 Train Loss 1.526131598572982 Train NLL 0.7139139928315815 Train KL 81221768.0 Test NLL 0.727053701877594\n",
      "[INFO] - 2024-05-16 11:03:54,577 - UQpy: Scientific Machine Learning: Epoch 84 / 1,000 Train Loss 1.5393990027277094 Train NLL 0.7275785897907457 Train KL 81182048.0 Test NLL 0.8356133699417114\n",
      "[INFO] - 2024-05-16 11:03:58,773 - UQpy: Scientific Machine Learning: Epoch 85 / 1,000 Train Loss 1.6279634613739817 Train NLL 0.8165444072924162 Train KL 81141912.0 Test NLL 1.149738073348999\n",
      "[INFO] - 2024-05-16 11:04:02,942 - UQpy: Scientific Machine Learning: Epoch 86 / 1,000 Train Loss 1.5783492013027793 Train NLL 0.7673318291965284 Train KL 81101736.0 Test NLL 0.8149845600128174\n",
      "[INFO] - 2024-05-16 11:04:07,113 - UQpy: Scientific Machine Learning: Epoch 87 / 1,000 Train Loss 1.6210253175936247 Train NLL 0.8104206386365389 Train KL 81060472.0 Test NLL 0.801791250705719\n",
      "[INFO] - 2024-05-16 11:04:11,160 - UQpy: Scientific Machine Learning: Epoch 88 / 1,000 Train Loss 1.7547940454984967 Train NLL 0.944604051740546 Train KL 81019000.0 Test NLL 0.9839853048324585\n",
      "[INFO] - 2024-05-16 11:04:15,171 - UQpy: Scientific Machine Learning: Epoch 89 / 1,000 Train Loss 1.5877984950416966 Train NLL 0.7780217716568395 Train KL 80977664.0 Test NLL 1.1003865003585815\n",
      "[INFO] - 2024-05-16 11:04:19,219 - UQpy: Scientific Machine Learning: Epoch 90 / 1,000 Train Loss 1.501815218674509 Train NLL 0.692461189470793 Train KL 80935408.0 Test NLL 0.8069483637809753\n",
      "[INFO] - 2024-05-16 11:04:23,231 - UQpy: Scientific Machine Learning: Epoch 91 / 1,000 Train Loss 1.6102324535972194 Train NLL 0.8013077425329309 Train KL 80892464.0 Test NLL 1.6434836387634277\n",
      "[INFO] - 2024-05-16 11:04:27,304 - UQpy: Scientific Machine Learning: Epoch 92 / 1,000 Train Loss 1.6698014296983417 Train NLL 0.8613017389648839 Train KL 80849976.0 Test NLL 0.917775571346283\n",
      "[INFO] - 2024-05-16 11:04:31,353 - UQpy: Scientific Machine Learning: Epoch 93 / 1,000 Train Loss 1.6050717705174495 Train NLL 0.7969965589673895 Train KL 80807520.0 Test NLL 0.8613531589508057\n",
      "[INFO] - 2024-05-16 11:04:35,371 - UQpy: Scientific Machine Learning: Epoch 94 / 1,000 Train Loss 1.6120444473467375 Train NLL 0.8044021772710901 Train KL 80764224.0 Test NLL 1.0042070150375366\n",
      "[INFO] - 2024-05-16 11:04:39,434 - UQpy: Scientific Machine Learning: Epoch 95 / 1,000 Train Loss 1.6069667339324951 Train NLL 0.7997631116917259 Train KL 80720360.0 Test NLL 1.0884504318237305\n",
      "[INFO] - 2024-05-16 11:04:43,442 - UQpy: Scientific Machine Learning: Epoch 96 / 1,000 Train Loss 1.5735450543855365 Train NLL 0.7667894630055678 Train KL 80675560.0 Test NLL 0.8361150026321411\n",
      "[INFO] - 2024-05-16 11:04:47,472 - UQpy: Scientific Machine Learning: Epoch 97 / 1,000 Train Loss 1.5028736277630454 Train NLL 0.6965738158476981 Train KL 80629992.0 Test NLL 0.7632626295089722\n",
      "[INFO] - 2024-05-16 11:04:51,492 - UQpy: Scientific Machine Learning: Epoch 98 / 1,000 Train Loss 1.5209442941766036 Train NLL 0.7151023494569879 Train KL 80584200.0 Test NLL 0.9478785991668701\n",
      "[INFO] - 2024-05-16 11:04:55,467 - UQpy: Scientific Machine Learning: Epoch 99 / 1,000 Train Loss 1.4991870867578607 Train NLL 0.6938065164967587 Train KL 80538056.0 Test NLL 0.8914041519165039\n",
      "[INFO] - 2024-05-16 11:04:59,511 - UQpy: Scientific Machine Learning: Epoch 100 / 1,000 Train Loss 1.5624890703904002 Train NLL 0.7575732187220925 Train KL 80491584.0 Test NLL 0.8875336050987244\n",
      "[INFO] - 2024-05-16 11:05:03,564 - UQpy: Scientific Machine Learning: Epoch 101 / 1,000 Train Loss 1.5609214243135954 Train NLL 0.7564718629184523 Train KL 80444968.0 Test NLL 1.2237231731414795\n",
      "[INFO] - 2024-05-16 11:05:07,650 - UQpy: Scientific Machine Learning: Epoch 102 / 1,000 Train Loss 1.6438424399024563 Train NLL 0.8398678318450326 Train KL 80397464.0 Test NLL 0.8123254776000977\n",
      "[INFO] - 2024-05-16 11:05:11,745 - UQpy: Scientific Machine Learning: Epoch 103 / 1,000 Train Loss 1.6221448371284886 Train NLL 0.8186508401444084 Train KL 80349408.0 Test NLL 0.9614943861961365\n",
      "[INFO] - 2024-05-16 11:05:15,831 - UQpy: Scientific Machine Learning: Epoch 104 / 1,000 Train Loss 1.589483474430285 Train NLL 0.7864728554299003 Train KL 80301064.0 Test NLL 1.0115046501159668\n",
      "[INFO] - 2024-05-16 11:05:19,932 - UQpy: Scientific Machine Learning: Epoch 105 / 1,000 Train Loss 1.5799534634539956 Train NLL 0.7774225598887393 Train KL 80253088.0 Test NLL 1.4582667350769043\n",
      "[INFO] - 2024-05-16 11:05:23,980 - UQpy: Scientific Machine Learning: Epoch 106 / 1,000 Train Loss 1.8126528012125116 Train NLL 1.0106068786821867 Train KL 80204592.0 Test NLL 1.0394883155822754\n",
      "[INFO] - 2024-05-16 11:05:28,023 - UQpy: Scientific Machine Learning: Epoch 107 / 1,000 Train Loss 1.755440730797617 Train NLL 0.9538815084256624 Train KL 80155928.0 Test NLL 1.9798924922943115\n",
      "[INFO] - 2024-05-16 11:05:32,083 - UQpy: Scientific Machine Learning: Epoch 108 / 1,000 Train Loss 2.039274309810839 Train NLL 1.238205323093816 Train KL 80106904.0 Test NLL 1.4103975296020508\n",
      "[INFO] - 2024-05-16 11:05:36,116 - UQpy: Scientific Machine Learning: Epoch 109 / 1,000 Train Loss 1.9403439195532548 Train NLL 1.139766884477515 Train KL 80057704.0 Test NLL 0.9622186422348022\n",
      "[INFO] - 2024-05-16 11:05:40,275 - UQpy: Scientific Machine Learning: Epoch 110 / 1,000 Train Loss 1.8055115059802407 Train NLL 1.0054348268006976 Train KL 80007656.0 Test NLL 0.9878906011581421\n",
      "[INFO] - 2024-05-16 11:05:44,451 - UQpy: Scientific Machine Learning: Epoch 111 / 1,000 Train Loss 1.788104728648537 Train NLL 0.9885336192030656 Train KL 79957112.0 Test NLL 0.943217396736145\n",
      "[INFO] - 2024-05-16 11:05:48,552 - UQpy: Scientific Machine Learning: Epoch 112 / 1,000 Train Loss 1.5922670427121615 Train NLL 0.793214057621203 Train KL 79905304.0 Test NLL 0.981797456741333\n",
      "[INFO] - 2024-05-16 11:05:52,704 - UQpy: Scientific Machine Learning: Epoch 113 / 1,000 Train Loss 1.5198889845295955 Train NLL 0.7213587384474905 Train KL 79853032.0 Test NLL 0.8944518566131592\n",
      "[INFO] - 2024-05-16 11:05:56,908 - UQpy: Scientific Machine Learning: Epoch 114 / 1,000 Train Loss 1.5390298492030094 Train NLL 0.7410338928824977 Train KL 79799608.0 Test NLL 0.9106496572494507\n",
      "[INFO] - 2024-05-16 11:06:01,186 - UQpy: Scientific Machine Learning: Epoch 115 / 1,000 Train Loss 1.678534143849423 Train NLL 0.8810721165255496 Train KL 79746208.0 Test NLL 0.9628373384475708\n",
      "[INFO] - 2024-05-16 11:06:05,284 - UQpy: Scientific Machine Learning: Epoch 116 / 1,000 Train Loss 1.503600471898129 Train NLL 0.7066677965615925 Train KL 79693272.0 Test NLL 0.8117271661758423\n",
      "[INFO] - 2024-05-16 11:06:09,424 - UQpy: Scientific Machine Learning: Epoch 117 / 1,000 Train Loss 1.5263889526066028 Train NLL 0.730001405665749 Train KL 79638744.0 Test NLL 0.713018000125885\n",
      "[INFO] - 2024-05-16 11:06:13,556 - UQpy: Scientific Machine Learning: Epoch 118 / 1,000 Train Loss 1.5234703264738385 Train NLL 0.7276352047920227 Train KL 79583512.0 Test NLL 0.891999363899231\n",
      "[INFO] - 2024-05-16 11:06:17,809 - UQpy: Scientific Machine Learning: Epoch 119 / 1,000 Train Loss 1.5380594040218152 Train NLL 0.7427744990900943 Train KL 79528488.0 Test NLL 0.815445065498352\n",
      "[INFO] - 2024-05-16 11:06:22,123 - UQpy: Scientific Machine Learning: Epoch 120 / 1,000 Train Loss 1.5559586537511725 Train NLL 0.7612246588656777 Train KL 79473392.0 Test NLL 1.0667922496795654\n",
      "[INFO] - 2024-05-16 11:06:26,217 - UQpy: Scientific Machine Learning: Epoch 121 / 1,000 Train Loss 1.5388522587324445 Train NLL 0.7446758496133905 Train KL 79417640.0 Test NLL 0.9925680756568909\n",
      "[INFO] - 2024-05-16 11:06:30,660 - UQpy: Scientific Machine Learning: Epoch 122 / 1,000 Train Loss 1.4924073972200091 Train NLL 0.6987943617921126 Train KL 79361304.0 Test NLL 0.9115936756134033\n",
      "[INFO] - 2024-05-16 11:06:34,930 - UQpy: Scientific Machine Learning: Epoch 123 / 1,000 Train Loss 1.5230343530052586 Train NLL 0.7299933464903581 Train KL 79304112.0 Test NLL 0.7719658613204956\n",
      "[INFO] - 2024-05-16 11:06:39,137 - UQpy: Scientific Machine Learning: Epoch 124 / 1,000 Train Loss 1.48505741671512 Train NLL 0.6925871325166602 Train KL 79247032.0 Test NLL 0.7885062098503113\n",
      "[INFO] - 2024-05-16 11:06:43,190 - UQpy: Scientific Machine Learning: Epoch 125 / 1,000 Train Loss 1.5418080907118947 Train NLL 0.7499178318600905 Train KL 79189024.0 Test NLL 0.6918218731880188\n",
      "[INFO] - 2024-05-16 11:06:47,241 - UQpy: Scientific Machine Learning: Epoch 126 / 1,000 Train Loss 1.6149694292168868 Train NLL 0.8236671309722098 Train KL 79130224.0 Test NLL 0.8278343677520752\n",
      "[INFO] - 2024-05-16 11:06:51,327 - UQpy: Scientific Machine Learning: Epoch 127 / 1,000 Train Loss 1.6691656489121287 Train NLL 0.8784434701267042 Train KL 79072216.0 Test NLL 0.8065195679664612\n",
      "[INFO] - 2024-05-16 11:06:55,409 - UQpy: Scientific Machine Learning: Epoch 128 / 1,000 Train Loss 1.6136129529852616 Train NLL 0.823469318841633 Train KL 79014352.0 Test NLL 0.7311831116676331\n",
      "[INFO] - 2024-05-16 11:06:59,505 - UQpy: Scientific Machine Learning: Epoch 129 / 1,000 Train Loss 1.7706466850481535 Train NLL 0.9810967163035744 Train KL 78954992.0 Test NLL 1.052895188331604\n",
      "[INFO] - 2024-05-16 11:07:03,562 - UQpy: Scientific Machine Learning: Epoch 130 / 1,000 Train Loss 1.8618108285100836 Train NLL 1.072848075314572 Train KL 78896280.0 Test NLL 1.198097825050354\n",
      "[INFO] - 2024-05-16 11:07:07,667 - UQpy: Scientific Machine Learning: Epoch 131 / 1,000 Train Loss 1.634600978148611 Train NLL 0.8462381770736292 Train KL 78836280.0 Test NLL 1.064671277999878\n",
      "[INFO] - 2024-05-16 11:07:11,827 - UQpy: Scientific Machine Learning: Epoch 132 / 1,000 Train Loss 1.5982855119203265 Train NLL 0.8105272838943883 Train KL 78775832.0 Test NLL 0.8001885414123535\n",
      "[INFO] - 2024-05-16 11:07:15,956 - UQpy: Scientific Machine Learning: Epoch 133 / 1,000 Train Loss 1.8126138197748285 Train NLL 1.0254702505312467 Train KL 78714352.0 Test NLL 1.6064878702163696\n",
      "[INFO] - 2024-05-16 11:07:20,024 - UQpy: Scientific Machine Learning: Epoch 134 / 1,000 Train Loss 1.5647535512321873 Train NLL 0.7782206566710221 Train KL 78653280.0 Test NLL 1.218240737915039\n",
      "[INFO] - 2024-05-16 11:07:24,094 - UQpy: Scientific Machine Learning: Epoch 135 / 1,000 Train Loss 1.551007120232833 Train NLL 0.7651009120439228 Train KL 78590616.0 Test NLL 0.9104094505310059\n",
      "[INFO] - 2024-05-16 11:07:28,129 - UQpy: Scientific Machine Learning: Epoch 136 / 1,000 Train Loss 1.4881282731106407 Train NLL 0.7028616070747375 Train KL 78526664.0 Test NLL 0.7038201689720154\n",
      "[INFO] - 2024-05-16 11:07:32,251 - UQpy: Scientific Machine Learning: Epoch 137 / 1,000 Train Loss 1.4813291148135537 Train NLL 0.6967027720652128 Train KL 78462632.0 Test NLL 1.1076207160949707\n",
      "[INFO] - 2024-05-16 11:07:36,262 - UQpy: Scientific Machine Learning: Epoch 138 / 1,000 Train Loss 1.538321664458827 Train NLL 0.7543400792699111 Train KL 78398168.0 Test NLL 1.5043652057647705\n",
      "[INFO] - 2024-05-16 11:07:40,489 - UQpy: Scientific Machine Learning: Epoch 139 / 1,000 Train Loss 1.537501190838061 Train NLL 0.7541693213738894 Train KL 78333184.0 Test NLL 1.0882080793380737\n",
      "[INFO] - 2024-05-16 11:07:44,547 - UQpy: Scientific Machine Learning: Epoch 140 / 1,000 Train Loss 1.7970491961428994 Train NLL 1.014366909077293 Train KL 78268232.0 Test NLL 1.1142144203186035\n",
      "[INFO] - 2024-05-16 11:07:48,659 - UQpy: Scientific Machine Learning: Epoch 141 / 1,000 Train Loss 1.6642721201244153 Train NLL 0.8822374249759474 Train KL 78203472.0 Test NLL 1.158705234527588\n",
      "[INFO] - 2024-05-16 11:07:52,740 - UQpy: Scientific Machine Learning: Epoch 142 / 1,000 Train Loss 1.5426212047275745 Train NLL 0.761236802527779 Train KL 78138432.0 Test NLL 1.196495532989502\n",
      "[INFO] - 2024-05-16 11:07:56,790 - UQpy: Scientific Machine Learning: Epoch 143 / 1,000 Train Loss 1.5372827492262189 Train NLL 0.7565655347548033 Train KL 78071720.0 Test NLL 0.7317423820495605\n",
      "[INFO] - 2024-05-16 11:08:00,919 - UQpy: Scientific Machine Learning: Epoch 144 / 1,000 Train Loss 1.4445665008143376 Train NLL 0.664523865047254 Train KL 78004272.0 Test NLL 1.4226640462875366\n",
      "[INFO] - 2024-05-16 11:08:05,059 - UQpy: Scientific Machine Learning: Epoch 145 / 1,000 Train Loss 1.5408892255080373 Train NLL 0.7615328603669217 Train KL 77935640.0 Test NLL 0.9357285499572754\n",
      "[INFO] - 2024-05-16 11:08:09,138 - UQpy: Scientific Machine Learning: Epoch 146 / 1,000 Train Loss 1.447186871578819 Train NLL 0.6685154312535336 Train KL 77867144.0 Test NLL 0.7075939178466797\n",
      "[INFO] - 2024-05-16 11:08:13,327 - UQpy: Scientific Machine Learning: Epoch 147 / 1,000 Train Loss 1.5042186348061812 Train NLL 0.7262371496150368 Train KL 77798144.0 Test NLL 1.1030642986297607\n",
      "[INFO] - 2024-05-16 11:08:17,536 - UQpy: Scientific Machine Learning: Epoch 148 / 1,000 Train Loss 1.417145151841013 Train NLL 0.6398602692704451 Train KL 77728488.0 Test NLL 0.7783070802688599\n",
      "[INFO] - 2024-05-16 11:08:21,745 - UQpy: Scientific Machine Learning: Epoch 149 / 1,000 Train Loss 1.5435512379596108 Train NLL 0.766969404722515 Train KL 77658192.0 Test NLL 0.7344261407852173\n",
      "[INFO] - 2024-05-16 11:08:25,871 - UQpy: Scientific Machine Learning: Epoch 150 / 1,000 Train Loss 1.5295352935791016 Train NLL 0.7536606192588806 Train KL 77587472.0 Test NLL 0.8845235109329224\n",
      "[INFO] - 2024-05-16 11:08:30,065 - UQpy: Scientific Machine Learning: Epoch 151 / 1,000 Train Loss 1.563607604880082 Train NLL 0.7884410917758942 Train KL 77516656.0 Test NLL 0.9866222739219666\n",
      "[INFO] - 2024-05-16 11:08:34,195 - UQpy: Scientific Machine Learning: Epoch 152 / 1,000 Train Loss 1.3886851762470447 Train NLL 0.614234552571648 Train KL 77445056.0 Test NLL 0.6517059803009033\n",
      "[INFO] - 2024-05-16 11:08:38,278 - UQpy: Scientific Machine Learning: Epoch 153 / 1,000 Train Loss 1.531896333945425 Train NLL 0.7581727551786523 Train KL 77372352.0 Test NLL 1.104057788848877\n",
      "[INFO] - 2024-05-16 11:08:42,365 - UQpy: Scientific Machine Learning: Epoch 154 / 1,000 Train Loss 1.5504211438329596 Train NLL 0.7774331506929899 Train KL 77298800.0 Test NLL 0.6961707472801208\n",
      "[INFO] - 2024-05-16 11:08:46,638 - UQpy: Scientific Machine Learning: Epoch 155 / 1,000 Train Loss 1.358331655201159 Train NLL 0.5860705156075326 Train KL 77226120.0 Test NLL 0.6954226493835449\n",
      "[INFO] - 2024-05-16 11:08:50,757 - UQpy: Scientific Machine Learning: Epoch 156 / 1,000 Train Loss 1.4515275766975002 Train NLL 0.6800047049396917 Train KL 77152288.0 Test NLL 0.8360804319381714\n",
      "[INFO] - 2024-05-16 11:08:54,897 - UQpy: Scientific Machine Learning: Epoch 157 / 1,000 Train Loss 1.4936514214465493 Train NLL 0.7228784749382421 Train KL 77077288.0 Test NLL 0.6500438451766968\n",
      "[INFO] - 2024-05-16 11:08:59,037 - UQpy: Scientific Machine Learning: Epoch 158 / 1,000 Train Loss 1.4576152312128168 Train NLL 0.6875918225238198 Train KL 77002344.0 Test NLL 0.8557820916175842\n",
      "[INFO] - 2024-05-16 11:09:03,145 - UQpy: Scientific Machine Learning: Epoch 159 / 1,000 Train Loss 1.4375282400532772 Train NLL 0.6682624252218949 Train KL 76926568.0 Test NLL 0.6334227323532104\n",
      "[INFO] - 2024-05-16 11:09:07,351 - UQpy: Scientific Machine Learning: Epoch 160 / 1,000 Train Loss 1.3906270642029612 Train NLL 0.6221197451415815 Train KL 76850720.0 Test NLL 0.7521642446517944\n",
      "[INFO] - 2024-05-16 11:09:11,397 - UQpy: Scientific Machine Learning: Epoch 161 / 1,000 Train Loss 1.3448950303228278 Train NLL 0.5771611066241014 Train KL 76773400.0 Test NLL 0.676856517791748\n",
      "[INFO] - 2024-05-16 11:09:15,529 - UQpy: Scientific Machine Learning: Epoch 162 / 1,000 Train Loss 1.4070587158203125 Train NLL 0.6401035519022691 Train KL 76695504.0 Test NLL 0.9033231139183044\n",
      "[INFO] - 2024-05-16 11:09:19,602 - UQpy: Scientific Machine Learning: Epoch 163 / 1,000 Train Loss 1.503518022988972 Train NLL 0.7373439324529547 Train KL 76617408.0 Test NLL 1.473060131072998\n",
      "[INFO] - 2024-05-16 11:09:23,680 - UQpy: Scientific Machine Learning: Epoch 164 / 1,000 Train Loss 1.3904434128811485 Train NLL 0.6250529571583396 Train KL 76539056.0 Test NLL 0.6861895322799683\n",
      "[INFO] - 2024-05-16 11:09:27,750 - UQpy: Scientific Machine Learning: Epoch 165 / 1,000 Train Loss 1.3632190729442395 Train NLL 0.598621644471821 Train KL 76459736.0 Test NLL 0.7985295057296753\n",
      "[INFO] - 2024-05-16 11:09:31,877 - UQpy: Scientific Machine Learning: Epoch 166 / 1,000 Train Loss 1.4296859502792358 Train NLL 0.6658929225645567 Train KL 76379312.0 Test NLL 0.6634534001350403\n",
      "[INFO] - 2024-05-16 11:09:35,981 - UQpy: Scientific Machine Learning: Epoch 167 / 1,000 Train Loss 1.535122890221445 Train NLL 0.7721335480087682 Train KL 76298944.0 Test NLL 0.7196975946426392\n",
      "[INFO] - 2024-05-16 11:09:40,073 - UQpy: Scientific Machine Learning: Epoch 168 / 1,000 Train Loss 1.3703244171644513 Train NLL 0.6081416999038897 Train KL 76218272.0 Test NLL 0.5915220379829407\n",
      "[INFO] - 2024-05-16 11:09:44,188 - UQpy: Scientific Machine Learning: Epoch 169 / 1,000 Train Loss 1.3770802585702193 Train NLL 0.6157141218059942 Train KL 76136616.0 Test NLL 0.7970540523529053\n",
      "[INFO] - 2024-05-16 11:09:48,233 - UQpy: Scientific Machine Learning: Epoch 170 / 1,000 Train Loss 1.4932622156645123 Train NLL 0.7327183343862232 Train KL 76054384.0 Test NLL 0.6583024263381958\n",
      "[INFO] - 2024-05-16 11:09:52,297 - UQpy: Scientific Machine Learning: Epoch 171 / 1,000 Train Loss 1.4405331423408108 Train NLL 0.6808090633467624 Train KL 75972408.0 Test NLL 1.2840006351470947\n",
      "[INFO] - 2024-05-16 11:09:56,403 - UQpy: Scientific Machine Learning: Epoch 172 / 1,000 Train Loss 1.4538989506269757 Train NLL 0.6950063109397888 Train KL 75889264.0 Test NLL 0.8196048140525818\n",
      "[INFO] - 2024-05-16 11:10:00,482 - UQpy: Scientific Machine Learning: Epoch 173 / 1,000 Train Loss 1.3759295312981856 Train NLL 0.6178738459160453 Train KL 75805568.0 Test NLL 0.652183473110199\n",
      "[INFO] - 2024-05-16 11:10:04,590 - UQpy: Scientific Machine Learning: Epoch 174 / 1,000 Train Loss 1.3719642476031655 Train NLL 0.6147655358439997 Train KL 75719872.0 Test NLL 0.6834543943405151\n",
      "[INFO] - 2024-05-16 11:10:08,705 - UQpy: Scientific Machine Learning: Epoch 175 / 1,000 Train Loss 1.4630632212287502 Train NLL 0.7067234437716635 Train KL 75633976.0 Test NLL 0.6712400913238525\n",
      "[INFO] - 2024-05-16 11:10:12,767 - UQpy: Scientific Machine Learning: Epoch 176 / 1,000 Train Loss 1.4373954471788908 Train NLL 0.6819125539378116 Train KL 75548296.0 Test NLL 0.8448312282562256\n",
      "[INFO] - 2024-05-16 11:10:16,909 - UQpy: Scientific Machine Learning: Epoch 177 / 1,000 Train Loss 1.4483864621112221 Train NLL 0.6937653234130458 Train KL 75462120.0 Test NLL 0.7647796869277954\n",
      "[INFO] - 2024-05-16 11:10:20,997 - UQpy: Scientific Machine Learning: Epoch 178 / 1,000 Train Loss 1.3708339615872032 Train NLL 0.6170779780337685 Train KL 75375592.0 Test NLL 0.7487345933914185\n",
      "[INFO] - 2024-05-16 11:10:25,121 - UQpy: Scientific Machine Learning: Epoch 179 / 1,000 Train Loss 1.3886331508034153 Train NLL 0.6357515815057253 Train KL 75288152.0 Test NLL 1.2377891540527344\n",
      "[INFO] - 2024-05-16 11:10:29,342 - UQpy: Scientific Machine Learning: Epoch 180 / 1,000 Train Loss 1.3787790474138761 Train NLL 0.6267772677697634 Train KL 75200184.0 Test NLL 0.6231884360313416\n",
      "[INFO] - 2024-05-16 11:10:33,620 - UQpy: Scientific Machine Learning: Epoch 181 / 1,000 Train Loss 1.361561944610194 Train NLL 0.6104417615815213 Train KL 75112000.0 Test NLL 0.936591625213623\n",
      "[INFO] - 2024-05-16 11:10:37,738 - UQpy: Scientific Machine Learning: Epoch 182 / 1,000 Train Loss 1.3226990260575946 Train NLL 0.5724742020431318 Train KL 75022488.0 Test NLL 0.7754068374633789\n",
      "[INFO] - 2024-05-16 11:10:41,966 - UQpy: Scientific Machine Learning: Epoch 183 / 1,000 Train Loss 1.357321808212682 Train NLL 0.6079986017001303 Train KL 74932320.0 Test NLL 0.6886705160140991\n",
      "[INFO] - 2024-05-16 11:10:46,088 - UQpy: Scientific Machine Learning: Epoch 184 / 1,000 Train Loss 1.4124905686629445 Train NLL 0.6640748020849729 Train KL 74841576.0 Test NLL 0.9094234108924866\n",
      "[INFO] - 2024-05-16 11:10:50,305 - UQpy: Scientific Machine Learning: Epoch 185 / 1,000 Train Loss 1.3626553949556852 Train NLL 0.6151522570534756 Train KL 74750320.0 Test NLL 0.9002233743667603\n",
      "[INFO] - 2024-05-16 11:10:54,346 - UQpy: Scientific Machine Learning: Epoch 186 / 1,000 Train Loss 1.3859189936989231 Train NLL 0.6393294177557293 Train KL 74658944.0 Test NLL 0.8362640142440796\n",
      "[INFO] - 2024-05-16 11:10:58,461 - UQpy: Scientific Machine Learning: Epoch 187 / 1,000 Train Loss 1.4674632361060695 Train NLL 0.7217935794278195 Train KL 74566968.0 Test NLL 0.6408238410949707\n",
      "[INFO] - 2024-05-16 11:11:02,734 - UQpy: Scientific Machine Learning: Epoch 188 / 1,000 Train Loss 1.3388162851333618 Train NLL 0.5940709396412498 Train KL 74474544.0 Test NLL 0.7432947158813477\n",
      "[INFO] - 2024-05-16 11:11:06,914 - UQpy: Scientific Machine Learning: Epoch 189 / 1,000 Train Loss 1.3891639333022268 Train NLL 0.6453544277893869 Train KL 74380952.0 Test NLL 0.6788769364356995\n",
      "[INFO] - 2024-05-16 11:11:11,137 - UQpy: Scientific Machine Learning: Epoch 190 / 1,000 Train Loss 1.3740600159293728 Train NLL 0.6311955404909033 Train KL 74286448.0 Test NLL 0.6678804159164429\n",
      "[INFO] - 2024-05-16 11:11:15,316 - UQpy: Scientific Machine Learning: Epoch 191 / 1,000 Train Loss 1.332339838931435 Train NLL 0.5904256497558794 Train KL 74191424.0 Test NLL 0.6563388109207153\n",
      "[INFO] - 2024-05-16 11:11:19,485 - UQpy: Scientific Machine Learning: Epoch 192 / 1,000 Train Loss 1.417418398355183 Train NLL 0.676465282314702 Train KL 74095320.0 Test NLL 0.8919087648391724\n",
      "[INFO] - 2024-05-16 11:11:23,661 - UQpy: Scientific Machine Learning: Epoch 193 / 1,000 Train Loss 1.3902724667599327 Train NLL 0.650276160553882 Train KL 73999632.0 Test NLL 0.6235201954841614\n",
      "[INFO] - 2024-05-16 11:11:27,851 - UQpy: Scientific Machine Learning: Epoch 194 / 1,000 Train Loss 1.3076304322794865 Train NLL 0.5685876858861822 Train KL 73904280.0 Test NLL 0.6825865507125854\n",
      "[INFO] - 2024-05-16 11:11:32,084 - UQpy: Scientific Machine Learning: Epoch 195 / 1,000 Train Loss 1.33736084636889 Train NLL 0.5992960427936754 Train KL 73806480.0 Test NLL 0.6069176197052002\n",
      "[INFO] - 2024-05-16 11:11:36,324 - UQpy: Scientific Machine Learning: Epoch 196 / 1,000 Train Loss 1.319596384700976 Train NLL 0.5825178889851821 Train KL 73707848.0 Test NLL 0.8830462694168091\n",
      "[INFO] - 2024-05-16 11:11:40,601 - UQpy: Scientific Machine Learning: Epoch 197 / 1,000 Train Loss 1.5392458627098484 Train NLL 0.803155055171565 Train KL 73609088.0 Test NLL 0.8736773729324341\n",
      "[INFO] - 2024-05-16 11:11:44,825 - UQpy: Scientific Machine Learning: Epoch 198 / 1,000 Train Loss 1.4440985416111194 Train NLL 0.7089852549527821 Train KL 73511320.0 Test NLL 0.8187388181686401\n",
      "[INFO] - 2024-05-16 11:11:49,070 - UQpy: Scientific Machine Learning: Epoch 199 / 1,000 Train Loss 1.2991358104505037 Train NLL 0.5650116675778439 Train KL 73412416.0 Test NLL 0.7172987461090088\n",
      "[INFO] - 2024-05-16 11:11:53,327 - UQpy: Scientific Machine Learning: Epoch 200 / 1,000 Train Loss 1.3239852503726357 Train NLL 0.590868436976483 Train KL 73311696.0 Test NLL 0.7395210862159729\n",
      "[INFO] - 2024-05-16 11:11:57,563 - UQpy: Scientific Machine Learning: Epoch 201 / 1,000 Train Loss 1.3506135438617908 Train NLL 0.6185098478668615 Train KL 73210360.0 Test NLL 0.6296590566635132\n",
      "[INFO] - 2024-05-16 11:12:01,761 - UQpy: Scientific Machine Learning: Epoch 202 / 1,000 Train Loss 1.2994888769952875 Train NLL 0.5683897451350564 Train KL 73109912.0 Test NLL 0.6132581233978271\n",
      "[INFO] - 2024-05-16 11:12:06,010 - UQpy: Scientific Machine Learning: Epoch 203 / 1,000 Train Loss 1.4385762026435451 Train NLL 0.7084983900973671 Train KL 73007776.0 Test NLL 0.6280794143676758\n",
      "[INFO] - 2024-05-16 11:12:10,216 - UQpy: Scientific Machine Learning: Epoch 204 / 1,000 Train Loss 1.4188259149852551 Train NLL 0.689774193261799 Train KL 72905168.0 Test NLL 0.6776933670043945\n",
      "[INFO] - 2024-05-16 11:12:14,436 - UQpy: Scientific Machine Learning: Epoch 205 / 1,000 Train Loss 1.3444681418569464 Train NLL 0.616441587084218 Train KL 72802656.0 Test NLL 0.7436450719833374\n",
      "[INFO] - 2024-05-16 11:12:18,656 - UQpy: Scientific Machine Learning: Epoch 206 / 1,000 Train Loss 1.2691318863316585 Train NLL 0.5421531545488458 Train KL 72697872.0 Test NLL 0.7016535997390747\n",
      "[INFO] - 2024-05-16 11:12:22,884 - UQpy: Scientific Machine Learning: Epoch 207 / 1,000 Train Loss 1.2707120995772512 Train NLL 0.5447888421384912 Train KL 72592328.0 Test NLL 0.6184960603713989\n",
      "[INFO] - 2024-05-16 11:12:27,072 - UQpy: Scientific Machine Learning: Epoch 208 / 1,000 Train Loss 1.2314569385428178 Train NLL 0.506596094683597 Train KL 72486080.0 Test NLL 0.667878270149231\n",
      "[INFO] - 2024-05-16 11:12:31,258 - UQpy: Scientific Machine Learning: Epoch 209 / 1,000 Train Loss 1.3241526829568964 Train NLL 0.6003691271731728 Train KL 72378368.0 Test NLL 0.6916689872741699\n",
      "[INFO] - 2024-05-16 11:12:35,440 - UQpy: Scientific Machine Learning: Epoch 210 / 1,000 Train Loss 1.3474634948529696 Train NLL 0.6247539802601463 Train KL 72270952.0 Test NLL 0.6454265117645264\n",
      "[INFO] - 2024-05-16 11:12:39,678 - UQpy: Scientific Machine Learning: Epoch 211 / 1,000 Train Loss 1.2671343025408293 Train NLL 0.5454971100154676 Train KL 72163712.0 Test NLL 0.6186205148696899\n",
      "[INFO] - 2024-05-16 11:12:44,064 - UQpy: Scientific Machine Learning: Epoch 212 / 1,000 Train Loss 1.2806817481392307 Train NLL 0.5601261822800887 Train KL 72055560.0 Test NLL 0.6669687032699585\n",
      "[INFO] - 2024-05-16 11:12:48,293 - UQpy: Scientific Machine Learning: Epoch 213 / 1,000 Train Loss 1.2856145030573796 Train NLL 0.5661495531860151 Train KL 71946496.0 Test NLL 0.8090325593948364\n",
      "[INFO] - 2024-05-16 11:12:52,476 - UQpy: Scientific Machine Learning: Epoch 214 / 1,000 Train Loss 1.2794762661582546 Train NLL 0.5611075115831274 Train KL 71836872.0 Test NLL 0.6710840463638306\n",
      "[INFO] - 2024-05-16 11:12:56,626 - UQpy: Scientific Machine Learning: Epoch 215 / 1,000 Train Loss 1.2994078962426436 Train NLL 0.5821427546049419 Train KL 71726512.0 Test NLL 0.6827752590179443\n",
      "[INFO] - 2024-05-16 11:13:00,814 - UQpy: Scientific Machine Learning: Epoch 216 / 1,000 Train Loss 1.4091457379491705 Train NLL 0.6929901555964821 Train KL 71615560.0 Test NLL 0.6761495471000671\n",
      "[INFO] - 2024-05-16 11:13:05,116 - UQpy: Scientific Machine Learning: Epoch 217 / 1,000 Train Loss 1.3022097913842452 Train NLL 0.5871565247836866 Train KL 71505328.0 Test NLL 0.667314887046814\n",
      "[INFO] - 2024-05-16 11:13:09,360 - UQpy: Scientific Machine Learning: Epoch 218 / 1,000 Train Loss 1.2471056611914384 Train NLL 0.5331295791425203 Train KL 71397608.0 Test NLL 0.5354702472686768\n",
      "[INFO] - 2024-05-16 11:13:13,670 - UQpy: Scientific Machine Learning: Epoch 219 / 1,000 Train Loss 1.4450246723074662 Train NLL 0.7321514455895675 Train KL 71287320.0 Test NLL 0.7371895909309387\n",
      "[INFO] - 2024-05-16 11:13:17,917 - UQpy: Scientific Machine Learning: Epoch 220 / 1,000 Train Loss 1.3829150074406673 Train NLL 0.6711458093241641 Train KL 71176920.0 Test NLL 0.5682656764984131\n",
      "[INFO] - 2024-05-16 11:13:22,098 - UQpy: Scientific Machine Learning: Epoch 221 / 1,000 Train Loss 1.476684764811867 Train NLL 0.7660362281297383 Train KL 71064856.0 Test NLL 1.1783666610717773\n",
      "[INFO] - 2024-05-16 11:13:26,296 - UQpy: Scientific Machine Learning: Epoch 222 / 1,000 Train Loss 1.53334126974407 Train NLL 0.823823564930966 Train KL 70951768.0 Test NLL 0.8298267126083374\n",
      "[INFO] - 2024-05-16 11:13:30,581 - UQpy: Scientific Machine Learning: Epoch 223 / 1,000 Train Loss 1.3762724336824919 Train NLL 0.667875487553446 Train KL 70839696.0 Test NLL 0.8398288488388062\n",
      "[INFO] - 2024-05-16 11:13:34,825 - UQpy: Scientific Machine Learning: Epoch 224 / 1,000 Train Loss 1.3086332521940534 Train NLL 0.6013704757941397 Train KL 70726280.0 Test NLL 0.7958053350448608\n",
      "[INFO] - 2024-05-16 11:13:38,885 - UQpy: Scientific Machine Learning: Epoch 225 / 1,000 Train Loss 1.3310020158165379 Train NLL 0.6249053697837027 Train KL 70609664.0 Test NLL 0.964641809463501\n",
      "[INFO] - 2024-05-16 11:13:42,899 - UQpy: Scientific Machine Learning: Epoch 226 / 1,000 Train Loss 1.2750620465529592 Train NLL 0.5701345032767245 Train KL 70492752.0 Test NLL 0.62754225730896\n",
      "[INFO] - 2024-05-16 11:13:46,960 - UQpy: Scientific Machine Learning: Epoch 227 / 1,000 Train Loss 1.3288938810950832 Train NLL 0.625142514705658 Train KL 70375144.0 Test NLL 0.5632683038711548\n",
      "[INFO] - 2024-05-16 11:13:51,017 - UQpy: Scientific Machine Learning: Epoch 228 / 1,000 Train Loss 1.3256238573475887 Train NLL 0.6230489674367403 Train KL 70257488.0 Test NLL 0.7227314710617065\n",
      "[INFO] - 2024-05-16 11:13:55,107 - UQpy: Scientific Machine Learning: Epoch 229 / 1,000 Train Loss 1.334111301522506 Train NLL 0.6327158463628668 Train KL 70139544.0 Test NLL 0.5814145803451538\n",
      "[INFO] - 2024-05-16 11:13:59,138 - UQpy: Scientific Machine Learning: Epoch 230 / 1,000 Train Loss 1.2507336453387612 Train NLL 0.550533148803209 Train KL 70020056.0 Test NLL 0.7052667140960693\n",
      "[INFO] - 2024-05-16 11:14:03,227 - UQpy: Scientific Machine Learning: Epoch 231 / 1,000 Train Loss 1.2090292290637368 Train NLL 0.510041920762313 Train KL 69898728.0 Test NLL 0.5840867757797241\n",
      "[INFO] - 2024-05-16 11:14:07,307 - UQpy: Scientific Machine Learning: Epoch 232 / 1,000 Train Loss 1.1832045316696167 Train NLL 0.4854430681780765 Train KL 69776152.0 Test NLL 0.48080360889434814\n",
      "[INFO] - 2024-05-16 11:14:11,378 - UQpy: Scientific Machine Learning: Epoch 233 / 1,000 Train Loss 1.1744598432591087 Train NLL 0.4779312971391176 Train KL 69652848.0 Test NLL 0.6205723285675049\n",
      "[INFO] - 2024-05-16 11:14:15,453 - UQpy: Scientific Machine Learning: Epoch 234 / 1,000 Train Loss 1.332718522925126 Train NLL 0.6374266759345406 Train KL 69529192.0 Test NLL 0.5568027496337891\n",
      "[INFO] - 2024-05-16 11:14:19,521 - UQpy: Scientific Machine Learning: Epoch 235 / 1,000 Train Loss 1.2357660155547292 Train NLL 0.5417025026522184 Train KL 69406352.0 Test NLL 0.5169219970703125\n",
      "[INFO] - 2024-05-16 11:14:23,570 - UQpy: Scientific Machine Learning: Epoch 236 / 1,000 Train Loss 1.2211902267054509 Train NLL 0.5283663288543099 Train KL 69282392.0 Test NLL 0.6234017014503479\n",
      "[INFO] - 2024-05-16 11:14:27,604 - UQpy: Scientific Machine Learning: Epoch 237 / 1,000 Train Loss 1.1963717435535632 Train NLL 0.5048032183396188 Train KL 69156848.0 Test NLL 0.5422837734222412\n",
      "[INFO] - 2024-05-16 11:14:31,676 - UQpy: Scientific Machine Learning: Epoch 238 / 1,000 Train Loss 1.2783752554341365 Train NLL 0.5880707721961173 Train KL 69030448.0 Test NLL 0.661634087562561\n",
      "[INFO] - 2024-05-16 11:14:35,621 - UQpy: Scientific Machine Learning: Epoch 239 / 1,000 Train Loss 1.3295886955763165 Train NLL 0.6405357718467712 Train KL 68905280.0 Test NLL 0.6095322370529175\n",
      "[INFO] - 2024-05-16 11:14:39,556 - UQpy: Scientific Machine Learning: Epoch 240 / 1,000 Train Loss 1.2857566381755627 Train NLL 0.5979692935943604 Train KL 68778736.0 Test NLL 0.8091006875038147\n",
      "[INFO] - 2024-05-16 11:14:43,438 - UQpy: Scientific Machine Learning: Epoch 241 / 1,000 Train Loss 1.3799672879670795 Train NLL 0.6934447366940347 Train KL 68652248.0 Test NLL 0.5583765506744385\n",
      "[INFO] - 2024-05-16 11:14:47,532 - UQpy: Scientific Machine Learning: Epoch 242 / 1,000 Train Loss 1.2988020683589734 Train NLL 0.6135305479953164 Train KL 68527144.0 Test NLL 0.546340823173523\n",
      "[INFO] - 2024-05-16 11:14:51,561 - UQpy: Scientific Machine Learning: Epoch 243 / 1,000 Train Loss 1.2076280179776644 Train NLL 0.5236307602179678 Train KL 68399720.0 Test NLL 0.5544883012771606\n",
      "[INFO] - 2024-05-16 11:14:55,609 - UQpy: Scientific Machine Learning: Epoch 244 / 1,000 Train Loss 1.2279968387202214 Train NLL 0.5452958502267536 Train KL 68270096.0 Test NLL 0.5539315342903137\n",
      "[INFO] - 2024-05-16 11:14:59,609 - UQpy: Scientific Machine Learning: Epoch 245 / 1,000 Train Loss 1.2231536664460834 Train NLL 0.5417483919545224 Train KL 68140528.0 Test NLL 0.773090124130249\n",
      "[INFO] - 2024-05-16 11:15:03,518 - UQpy: Scientific Machine Learning: Epoch 246 / 1,000 Train Loss 1.1918135536344427 Train NLL 0.5117177131928896 Train KL 68009576.0 Test NLL 0.5165828466415405\n",
      "[INFO] - 2024-05-16 11:15:07,425 - UQpy: Scientific Machine Learning: Epoch 247 / 1,000 Train Loss 1.1735392871655916 Train NLL 0.49475763189165217 Train KL 67878168.0 Test NLL 0.7474267482757568\n",
      "[INFO] - 2024-05-16 11:15:11,458 - UQpy: Scientific Machine Learning: Epoch 248 / 1,000 Train Loss 1.249545040883516 Train NLL 0.5720953721749155 Train KL 67744960.0 Test NLL 0.7158430814743042\n",
      "[INFO] - 2024-05-16 11:15:15,484 - UQpy: Scientific Machine Learning: Epoch 249 / 1,000 Train Loss 1.2012833262744702 Train NLL 0.5251640175518236 Train KL 67611928.0 Test NLL 0.6893289685249329\n",
      "[INFO] - 2024-05-16 11:15:19,377 - UQpy: Scientific Machine Learning: Epoch 250 / 1,000 Train Loss 1.3311094547572888 Train NLL 0.6563300678604528 Train KL 67477944.0 Test NLL 0.6170992255210876\n",
      "[INFO] - 2024-05-16 11:15:23,264 - UQpy: Scientific Machine Learning: Epoch 251 / 1,000 Train Loss 1.334698664514642 Train NLL 0.6612473224338732 Train KL 67345128.0 Test NLL 0.7517476081848145\n",
      "[INFO] - 2024-05-16 11:15:27,133 - UQpy: Scientific Machine Learning: Epoch 252 / 1,000 Train Loss 1.2285786553433067 Train NLL 0.556464915212832 Train KL 67211368.0 Test NLL 0.6174081563949585\n",
      "[INFO] - 2024-05-16 11:15:31,001 - UQpy: Scientific Machine Learning: Epoch 253 / 1,000 Train Loss 1.1736349808542352 Train NLL 0.5028837825122633 Train KL 67075112.0 Test NLL 0.5301917195320129\n",
      "[INFO] - 2024-05-16 11:15:34,863 - UQpy: Scientific Machine Learning: Epoch 254 / 1,000 Train Loss 1.2568546345359402 Train NLL 0.5874837919285423 Train KL 66937076.0 Test NLL 0.9253376722335815\n",
      "[INFO] - 2024-05-16 11:15:38,769 - UQpy: Scientific Machine Learning: Epoch 255 / 1,000 Train Loss 1.4594495421961735 Train NLL 0.7914295776894218 Train KL 66802000.0 Test NLL 0.5147551894187927\n",
      "[INFO] - 2024-05-16 11:15:42,745 - UQpy: Scientific Machine Learning: Epoch 256 / 1,000 Train Loss 1.3795543124801235 Train NLL 0.7128780421457792 Train KL 66667628.0 Test NLL 0.6564695835113525\n",
      "[INFO] - 2024-05-16 11:15:46,619 - UQpy: Scientific Machine Learning: Epoch 257 / 1,000 Train Loss 1.277490691134804 Train NLL 0.6121561699791959 Train KL 66533456.0 Test NLL 0.6435821652412415\n",
      "[INFO] - 2024-05-16 11:15:50,585 - UQpy: Scientific Machine Learning: Epoch 258 / 1,000 Train Loss 1.2213264954717535 Train NLL 0.5573626050823614 Train KL 66396388.0 Test NLL 0.582036018371582\n",
      "[INFO] - 2024-05-16 11:15:54,503 - UQpy: Scientific Machine Learning: Epoch 259 / 1,000 Train Loss 1.1839886841021086 Train NLL 0.5214086363190099 Train KL 66257996.0 Test NLL 0.6525506973266602\n",
      "[INFO] - 2024-05-16 11:15:58,566 - UQpy: Scientific Machine Learning: Epoch 260 / 1,000 Train Loss 1.2328351171393144 Train NLL 0.571656595719488 Train KL 66117856.0 Test NLL 0.9795984625816345\n",
      "[INFO] - 2024-05-16 11:16:02,675 - UQpy: Scientific Machine Learning: Epoch 261 / 1,000 Train Loss 1.1563782723326432 Train NLL 0.49659570110471624 Train KL 65978252.0 Test NLL 0.6642497181892395\n",
      "[INFO] - 2024-05-16 11:16:06,753 - UQpy: Scientific Machine Learning: Epoch 262 / 1,000 Train Loss 1.169115490035007 Train NLL 0.5107463943330866 Train KL 65836916.0 Test NLL 0.5962175130844116\n",
      "[INFO] - 2024-05-16 11:16:10,742 - UQpy: Scientific Machine Learning: Epoch 263 / 1,000 Train Loss 1.1650670986426503 Train NLL 0.5081205822919544 Train KL 65694644.0 Test NLL 0.7183839082717896\n",
      "[INFO] - 2024-05-16 11:16:14,694 - UQpy: Scientific Machine Learning: Epoch 264 / 1,000 Train Loss 1.1111995515070463 Train NLL 0.45568281412124634 Train KL 65551668.0 Test NLL 0.4767613410949707\n",
      "[INFO] - 2024-05-16 11:16:18,850 - UQpy: Scientific Machine Learning: Epoch 265 / 1,000 Train Loss 1.125373733671088 Train NLL 0.47129965299054194 Train KL 65407408.0 Test NLL 0.5087140798568726\n",
      "[INFO] - 2024-05-16 11:16:23,039 - UQpy: Scientific Machine Learning: Epoch 266 / 1,000 Train Loss 1.2429253082526357 Train NLL 0.5903021975567466 Train KL 65262316.0 Test NLL 0.6532909870147705\n",
      "[INFO] - 2024-05-16 11:16:27,153 - UQpy: Scientific Machine Learning: Epoch 267 / 1,000 Train Loss 1.1584946826884621 Train NLL 0.5073001133768182 Train KL 65119460.0 Test NLL 0.6265271306037903\n",
      "[INFO] - 2024-05-16 11:16:31,280 - UQpy: Scientific Machine Learning: Epoch 268 / 1,000 Train Loss 1.1681253408130847 Train NLL 0.5183804552806052 Train KL 64974492.0 Test NLL 0.6570401191711426\n",
      "[INFO] - 2024-05-16 11:16:35,477 - UQpy: Scientific Machine Learning: Epoch 269 / 1,000 Train Loss 1.182057637917368 Train NLL 0.5337699808572468 Train KL 64828772.0 Test NLL 0.5325021743774414\n",
      "[INFO] - 2024-05-16 11:16:39,554 - UQpy: Scientific Machine Learning: Epoch 270 / 1,000 Train Loss 1.2753305999856246 Train NLL 0.6285076251155451 Train KL 64682300.0 Test NLL 0.6964553594589233\n",
      "[INFO] - 2024-05-16 11:16:43,671 - UQpy: Scientific Machine Learning: Epoch 271 / 1,000 Train Loss 1.2300147257353131 Train NLL 0.5846541523933411 Train KL 64536056.0 Test NLL 0.634692907333374\n",
      "[INFO] - 2024-05-16 11:16:47,770 - UQpy: Scientific Machine Learning: Epoch 272 / 1,000 Train Loss 1.156730968701212 Train NLL 0.512837185671455 Train KL 64389376.0 Test NLL 0.5058100819587708\n",
      "[INFO] - 2024-05-16 11:16:51,966 - UQpy: Scientific Machine Learning: Epoch 273 / 1,000 Train Loss 1.1756517385181628 Train NLL 0.5332393803094563 Train KL 64241232.0 Test NLL 0.7755879163742065\n",
      "[INFO] - 2024-05-16 11:16:56,175 - UQpy: Scientific Machine Learning: Epoch 274 / 1,000 Train Loss 1.212813841669183 Train NLL 0.5718860469366375 Train KL 64092788.0 Test NLL 0.5575904846191406\n",
      "[INFO] - 2024-05-16 11:17:00,337 - UQpy: Scientific Machine Learning: Epoch 275 / 1,000 Train Loss 1.1403845799596686 Train NLL 0.5009419761205974 Train KL 63944260.0 Test NLL 0.596217155456543\n",
      "[INFO] - 2024-05-16 11:17:04,463 - UQpy: Scientific Machine Learning: Epoch 276 / 1,000 Train Loss 1.136418486896314 Train NLL 0.4984855322461379 Train KL 63793288.0 Test NLL 0.5698150992393494\n",
      "[INFO] - 2024-05-16 11:17:08,593 - UQpy: Scientific Machine Learning: Epoch 277 / 1,000 Train Loss 1.152481546527461 Train NLL 0.5160634800007469 Train KL 63641808.0 Test NLL 0.6849539875984192\n",
      "[INFO] - 2024-05-16 11:17:12,703 - UQpy: Scientific Machine Learning: Epoch 278 / 1,000 Train Loss 1.2233550830891258 Train NLL 0.5884624280427632 Train KL 63489268.0 Test NLL 1.0385894775390625\n",
      "[INFO] - 2024-05-16 11:17:16,737 - UQpy: Scientific Machine Learning: Epoch 279 / 1,000 Train Loss 1.187754091463591 Train NLL 0.5543761974886844 Train KL 63337780.0 Test NLL 0.8326785564422607\n",
      "[INFO] - 2024-05-16 11:17:20,765 - UQpy: Scientific Machine Learning: Epoch 280 / 1,000 Train Loss 1.2188721016833657 Train NLL 0.587012463494351 Train KL 63185968.0 Test NLL 0.6635571122169495\n",
      "[INFO] - 2024-05-16 11:17:24,819 - UQpy: Scientific Machine Learning: Epoch 281 / 1,000 Train Loss 1.1491004699154903 Train NLL 0.5187643029187855 Train KL 63033612.0 Test NLL 0.5522556900978088\n",
      "[INFO] - 2024-05-16 11:17:28,893 - UQpy: Scientific Machine Learning: Epoch 282 / 1,000 Train Loss 1.1324665766013295 Train NLL 0.5036605753396687 Train KL 62880600.0 Test NLL 0.5722449421882629\n",
      "[INFO] - 2024-05-16 11:17:32,998 - UQpy: Scientific Machine Learning: Epoch 283 / 1,000 Train Loss 1.215495526790619 Train NLL 0.5882374565852316 Train KL 62725800.0 Test NLL 0.5825608372688293\n",
      "[INFO] - 2024-05-16 11:17:37,167 - UQpy: Scientific Machine Learning: Epoch 284 / 1,000 Train Loss 1.1360616213396977 Train NLL 0.5103521660754555 Train KL 62570948.0 Test NLL 0.6677775382995605\n",
      "[INFO] - 2024-05-16 11:17:41,278 - UQpy: Scientific Machine Learning: Epoch 285 / 1,000 Train Loss 1.0767329303841842 Train NLL 0.45258232323746933 Train KL 62415064.0 Test NLL 0.5309587717056274\n",
      "[INFO] - 2024-05-16 11:17:45,411 - UQpy: Scientific Machine Learning: Epoch 286 / 1,000 Train Loss 1.168960147782376 Train NLL 0.5463768039879046 Train KL 62258336.0 Test NLL 0.45926839113235474\n",
      "[INFO] - 2024-05-16 11:17:49,503 - UQpy: Scientific Machine Learning: Epoch 287 / 1,000 Train Loss 1.1037452158175016 Train NLL 0.48271810381036057 Train KL 62102716.0 Test NLL 0.5999814867973328\n",
      "[INFO] - 2024-05-16 11:17:53,733 - UQpy: Scientific Machine Learning: Epoch 288 / 1,000 Train Loss 1.0981141768003766 Train NLL 0.4786750423280816 Train KL 61943908.0 Test NLL 0.7048976421356201\n",
      "[INFO] - 2024-05-16 11:17:57,899 - UQpy: Scientific Machine Learning: Epoch 289 / 1,000 Train Loss 1.140529099263643 Train NLL 0.5226826118795496 Train KL 61784652.0 Test NLL 0.5996793508529663\n",
      "[INFO] - 2024-05-16 11:18:02,301 - UQpy: Scientific Machine Learning: Epoch 290 / 1,000 Train Loss 1.1870441248542385 Train NLL 0.5707921856328061 Train KL 61625196.0 Test NLL 0.6149691939353943\n",
      "[INFO] - 2024-05-16 11:18:06,547 - UQpy: Scientific Machine Learning: Epoch 291 / 1,000 Train Loss 1.1888375533254523 Train NLL 0.5741746049178275 Train KL 61466292.0 Test NLL 0.5019723176956177\n",
      "[INFO] - 2024-05-16 11:18:10,563 - UQpy: Scientific Machine Learning: Epoch 292 / 1,000 Train Loss 1.1749638319015503 Train NLL 0.5618889002423537 Train KL 61307500.0 Test NLL 0.751226544380188\n",
      "[INFO] - 2024-05-16 11:18:14,526 - UQpy: Scientific Machine Learning: Epoch 293 / 1,000 Train Loss 1.1084120775523938 Train NLL 0.49693032471757187 Train KL 61148172.0 Test NLL 0.594985842704773\n",
      "[INFO] - 2024-05-16 11:18:18,430 - UQpy: Scientific Machine Learning: Epoch 294 / 1,000 Train Loss 1.0814640867082697 Train NLL 0.47160940107546356 Train KL 60985472.0 Test NLL 0.5291493535041809\n",
      "[INFO] - 2024-05-16 11:18:22,375 - UQpy: Scientific Machine Learning: Epoch 295 / 1,000 Train Loss 1.1192258408195095 Train NLL 0.5109991635146894 Train KL 60822676.0 Test NLL 0.6102225184440613\n",
      "[INFO] - 2024-05-16 11:18:26,461 - UQpy: Scientific Machine Learning: Epoch 296 / 1,000 Train Loss 1.1018591272203546 Train NLL 0.49525721292746694 Train KL 60660192.0 Test NLL 0.5105909109115601\n",
      "[INFO] - 2024-05-16 11:18:30,522 - UQpy: Scientific Machine Learning: Epoch 297 / 1,000 Train Loss 1.0879667526797245 Train NLL 0.48299702845121684 Train KL 60496976.0 Test NLL 0.4884679913520813\n",
      "[INFO] - 2024-05-16 11:18:34,642 - UQpy: Scientific Machine Learning: Epoch 298 / 1,000 Train Loss 1.069622620155937 Train NLL 0.4663031556104359 Train KL 60331944.0 Test NLL 1.0579907894134521\n",
      "[INFO] - 2024-05-16 11:18:38,902 - UQpy: Scientific Machine Learning: Epoch 299 / 1,000 Train Loss 1.1513719809682745 Train NLL 0.5497016185208371 Train KL 60167032.0 Test NLL 0.5694704055786133\n",
      "[INFO] - 2024-05-16 11:18:42,907 - UQpy: Scientific Machine Learning: Epoch 300 / 1,000 Train Loss 1.1292185908869694 Train NLL 0.5291867256164551 Train KL 60003192.0 Test NLL 0.5771001577377319\n",
      "[INFO] - 2024-05-16 11:18:46,963 - UQpy: Scientific Machine Learning: Epoch 301 / 1,000 Train Loss 1.0859999311597723 Train NLL 0.48760943820602015 Train KL 59839036.0 Test NLL 0.6108449101448059\n",
      "[INFO] - 2024-05-16 11:18:51,011 - UQpy: Scientific Machine Learning: Epoch 302 / 1,000 Train Loss 1.0591487319845903 Train NLL 0.4624260867896833 Train KL 59672268.0 Test NLL 0.5400115251541138\n",
      "[INFO] - 2024-05-16 11:18:55,041 - UQpy: Scientific Machine Learning: Epoch 303 / 1,000 Train Loss 1.1020380321301912 Train NLL 0.5069857490690131 Train KL 59505228.0 Test NLL 0.6561967134475708\n",
      "[INFO] - 2024-05-16 11:18:59,101 - UQpy: Scientific Machine Learning: Epoch 304 / 1,000 Train Loss 1.045502035241378 Train NLL 0.45212569362238836 Train KL 59337632.0 Test NLL 0.878013014793396\n",
      "[INFO] - 2024-05-16 11:19:03,142 - UQpy: Scientific Machine Learning: Epoch 305 / 1,000 Train Loss 1.1320231462779797 Train NLL 0.5403359767637754 Train KL 59168720.0 Test NLL 0.8175537586212158\n",
      "[INFO] - 2024-05-16 11:19:07,237 - UQpy: Scientific Machine Learning: Epoch 306 / 1,000 Train Loss 1.2152443935996609 Train NLL 0.6252284410752749 Train KL 59001588.0 Test NLL 0.50670325756073\n",
      "[INFO] - 2024-05-16 11:19:11,318 - UQpy: Scientific Machine Learning: Epoch 307 / 1,000 Train Loss 1.1558831490968402 Train NLL 0.5675067776127866 Train KL 58837632.0 Test NLL 0.6204543113708496\n",
      "[INFO] - 2024-05-16 11:19:15,346 - UQpy: Scientific Machine Learning: Epoch 308 / 1,000 Train Loss 1.1105345330740277 Train NLL 0.5237357820335188 Train KL 58679876.0 Test NLL 0.5340628623962402\n",
      "[INFO] - 2024-05-16 11:19:19,375 - UQpy: Scientific Machine Learning: Epoch 309 / 1,000 Train Loss 1.09121215657184 Train NLL 0.5060212659208398 Train KL 58519092.0 Test NLL 0.531015157699585\n",
      "[INFO] - 2024-05-16 11:19:23,534 - UQpy: Scientific Machine Learning: Epoch 310 / 1,000 Train Loss 1.1929008082339638 Train NLL 0.6093553775235226 Train KL 58354540.0 Test NLL 0.4881896376609802\n",
      "[INFO] - 2024-05-16 11:19:27,634 - UQpy: Scientific Machine Learning: Epoch 311 / 1,000 Train Loss 1.107268584401984 Train NLL 0.5253834771482568 Train KL 58188512.0 Test NLL 0.5061777234077454\n",
      "[INFO] - 2024-05-16 11:19:31,720 - UQpy: Scientific Machine Learning: Epoch 312 / 1,000 Train Loss 1.0665173248240822 Train NLL 0.4863247479263105 Train KL 58019252.0 Test NLL 0.9091812968254089\n",
      "[INFO] - 2024-05-16 11:19:35,827 - UQpy: Scientific Machine Learning: Epoch 313 / 1,000 Train Loss 1.080856166387859 Train NLL 0.5023514816635534 Train KL 57850468.0 Test NLL 0.53336501121521\n",
      "[INFO] - 2024-05-16 11:19:39,895 - UQpy: Scientific Machine Learning: Epoch 314 / 1,000 Train Loss 1.0753871171098006 Train NLL 0.49858135455533076 Train KL 57680572.0 Test NLL 0.6244944334030151\n",
      "[INFO] - 2024-05-16 11:19:43,956 - UQpy: Scientific Machine Learning: Epoch 315 / 1,000 Train Loss 1.1428983807563782 Train NLL 0.5677915874280428 Train KL 57510684.0 Test NLL 0.5919485092163086\n",
      "[INFO] - 2024-05-16 11:19:48,026 - UQpy: Scientific Machine Learning: Epoch 316 / 1,000 Train Loss 1.0465215695531744 Train NLL 0.4731187381242451 Train KL 57340280.0 Test NLL 0.5785760879516602\n",
      "[INFO] - 2024-05-16 11:19:52,145 - UQpy: Scientific Machine Learning: Epoch 317 / 1,000 Train Loss 1.0515271111538536 Train NLL 0.4798593207409507 Train KL 57166780.0 Test NLL 0.6821492910385132\n",
      "[INFO] - 2024-05-16 11:19:56,214 - UQpy: Scientific Machine Learning: Epoch 318 / 1,000 Train Loss 1.064209862759239 Train NLL 0.49427861602682815 Train KL 56993124.0 Test NLL 0.5756528377532959\n",
      "[INFO] - 2024-05-16 11:20:00,252 - UQpy: Scientific Machine Learning: Epoch 319 / 1,000 Train Loss 1.1503708393950212 Train NLL 0.5821735670692042 Train KL 56819732.0 Test NLL 0.5476023554801941\n",
      "[INFO] - 2024-05-16 11:20:04,308 - UQpy: Scientific Machine Learning: Epoch 320 / 1,000 Train Loss 1.122730603343562 Train NLL 0.5562520136958674 Train KL 56647856.0 Test NLL 0.7143476009368896\n",
      "[INFO] - 2024-05-16 11:20:08,452 - UQpy: Scientific Machine Learning: Epoch 321 / 1,000 Train Loss 1.1071018921701532 Train NLL 0.5423589238995 Train KL 56474296.0 Test NLL 0.6373419761657715\n",
      "[INFO] - 2024-05-16 11:20:12,531 - UQpy: Scientific Machine Learning: Epoch 322 / 1,000 Train Loss 1.0644341707229614 Train NLL 0.5014343434258511 Train KL 56299984.0 Test NLL 0.5076977610588074\n",
      "[INFO] - 2024-05-16 11:20:16,731 - UQpy: Scientific Machine Learning: Epoch 323 / 1,000 Train Loss 1.0667761815221686 Train NLL 0.5055348637856936 Train KL 56124136.0 Test NLL 0.4626363515853882\n",
      "[INFO] - 2024-05-16 11:20:20,843 - UQpy: Scientific Machine Learning: Epoch 324 / 1,000 Train Loss 1.0256722663578235 Train NLL 0.466184244344109 Train KL 55948800.0 Test NLL 0.7551313042640686\n",
      "[INFO] - 2024-05-16 11:20:24,812 - UQpy: Scientific Machine Learning: Epoch 325 / 1,000 Train Loss 1.0481715986603184 Train NLL 0.4904507759370302 Train KL 55772080.0 Test NLL 0.6881903409957886\n",
      "[INFO] - 2024-05-16 11:20:28,823 - UQpy: Scientific Machine Learning: Epoch 326 / 1,000 Train Loss 1.0147225668555813 Train NLL 0.45877397060394287 Train KL 55594868.0 Test NLL 0.5032268762588501\n",
      "[INFO] - 2024-05-16 11:20:32,796 - UQpy: Scientific Machine Learning: Epoch 327 / 1,000 Train Loss 0.996679569545545 Train NLL 0.4425243139266968 Train KL 55415528.0 Test NLL 0.5343471169471741\n",
      "[INFO] - 2024-05-16 11:20:36,751 - UQpy: Scientific Machine Learning: Epoch 328 / 1,000 Train Loss 1.0280601570480747 Train NLL 0.47570894423284027 Train KL 55235120.0 Test NLL 0.6304936408996582\n",
      "[INFO] - 2024-05-16 11:20:40,829 - UQpy: Scientific Machine Learning: Epoch 329 / 1,000 Train Loss 1.0356754723348116 Train NLL 0.4851263636036923 Train KL 55054912.0 Test NLL 0.6519830226898193\n",
      "[INFO] - 2024-05-16 11:20:44,730 - UQpy: Scientific Machine Learning: Epoch 330 / 1,000 Train Loss 1.0303590485924168 Train NLL 0.481590189431843 Train KL 54876880.0 Test NLL 0.5323019027709961\n",
      "[INFO] - 2024-05-16 11:20:48,672 - UQpy: Scientific Machine Learning: Epoch 331 / 1,000 Train Loss 1.0750591033383419 Train NLL 0.5280819770536924 Train KL 54697712.0 Test NLL 0.5113520622253418\n",
      "[INFO] - 2024-05-16 11:20:52,593 - UQpy: Scientific Machine Learning: Epoch 332 / 1,000 Train Loss 1.0353061368590908 Train NLL 0.49009940028190613 Train KL 54520676.0 Test NLL 0.539042592048645\n",
      "[INFO] - 2024-05-16 11:20:56,563 - UQpy: Scientific Machine Learning: Epoch 333 / 1,000 Train Loss 0.9684870556781167 Train NLL 0.42508099424211604 Train KL 54340608.0 Test NLL 0.4390268325805664\n",
      "[INFO] - 2024-05-16 11:21:00,612 - UQpy: Scientific Machine Learning: Epoch 334 / 1,000 Train Loss 0.9817516301807604 Train NLL 0.4401731334234539 Train KL 54157856.0 Test NLL 0.6423819661140442\n",
      "[INFO] - 2024-05-16 11:21:04,572 - UQpy: Scientific Machine Learning: Epoch 335 / 1,000 Train Loss 1.0062954017990513 Train NLL 0.46654180947102997 Train KL 53975352.0 Test NLL 0.5063835382461548\n",
      "[INFO] - 2024-05-16 11:21:08,518 - UQpy: Scientific Machine Learning: Epoch 336 / 1,000 Train Loss 0.9688237811389723 Train NLL 0.4308871015122062 Train KL 53793664.0 Test NLL 0.5511956214904785\n",
      "[INFO] - 2024-05-16 11:21:12,487 - UQpy: Scientific Machine Learning: Epoch 337 / 1,000 Train Loss 1.021297131714068 Train NLL 0.4851831056569752 Train KL 53611408.0 Test NLL 0.6130227446556091\n",
      "[INFO] - 2024-05-16 11:21:16,438 - UQpy: Scientific Machine Learning: Epoch 338 / 1,000 Train Loss 1.0354001741660268 Train NLL 0.5010902897307747 Train KL 53430988.0 Test NLL 0.7426277995109558\n",
      "[INFO] - 2024-05-16 11:21:20,391 - UQpy: Scientific Machine Learning: Epoch 339 / 1,000 Train Loss 1.049128337910301 Train NLL 0.5166406019737846 Train KL 53248780.0 Test NLL 0.5764584541320801\n",
      "[INFO] - 2024-05-16 11:21:24,346 - UQpy: Scientific Machine Learning: Epoch 340 / 1,000 Train Loss 1.0415447636654502 Train NLL 0.5108697932017477 Train KL 53067500.0 Test NLL 0.5829212665557861\n",
      "[INFO] - 2024-05-16 11:21:28,352 - UQpy: Scientific Machine Learning: Epoch 341 / 1,000 Train Loss 1.053185798619923 Train NLL 0.5243299619147652 Train KL 52885580.0 Test NLL 0.6479886770248413\n",
      "[INFO] - 2024-05-16 11:21:32,299 - UQpy: Scientific Machine Learning: Epoch 342 / 1,000 Train Loss 1.0393463530038531 Train NLL 0.5123043342640525 Train KL 52704200.0 Test NLL 0.4798004627227783\n",
      "[INFO] - 2024-05-16 11:21:36,249 - UQpy: Scientific Machine Learning: Epoch 343 / 1,000 Train Loss 0.9482944105800829 Train NLL 0.4230775880186181 Train KL 52521680.0 Test NLL 0.661146879196167\n",
      "[INFO] - 2024-05-16 11:21:40,296 - UQpy: Scientific Machine Learning: Epoch 344 / 1,000 Train Loss 1.0191318643720526 Train NLL 0.4957601333919324 Train KL 52337172.0 Test NLL 0.6836501955986023\n",
      "[INFO] - 2024-05-16 11:21:44,207 - UQpy: Scientific Machine Learning: Epoch 345 / 1,000 Train Loss 1.0692572311351174 Train NLL 0.5477192433256852 Train KL 52153804.0 Test NLL 0.7088212370872498\n",
      "[INFO] - 2024-05-16 11:21:48,153 - UQpy: Scientific Machine Learning: Epoch 346 / 1,000 Train Loss 1.028211025815261 Train NLL 0.5084841063148097 Train KL 51972696.0 Test NLL 0.5940709710121155\n",
      "[INFO] - 2024-05-16 11:21:52,180 - UQpy: Scientific Machine Learning: Epoch 347 / 1,000 Train Loss 1.0107968198625665 Train NLL 0.49289261510497645 Train KL 51790420.0 Test NLL 0.4592188596725464\n",
      "[INFO] - 2024-05-16 11:21:56,234 - UQpy: Scientific Machine Learning: Epoch 348 / 1,000 Train Loss 1.0004974289944297 Train NLL 0.4844369041292291 Train KL 51606048.0 Test NLL 0.5402722358703613\n",
      "[INFO] - 2024-05-16 11:22:00,188 - UQpy: Scientific Machine Learning: Epoch 349 / 1,000 Train Loss 0.9491361536477742 Train NLL 0.4349082002514287 Train KL 51422800.0 Test NLL 0.6380742788314819\n",
      "[INFO] - 2024-05-16 11:22:04,140 - UQpy: Scientific Machine Learning: Epoch 350 / 1,000 Train Loss 0.9810160649450201 Train NLL 0.46864748001098633 Train KL 51236860.0 Test NLL 0.6907906532287598\n",
      "[INFO] - 2024-05-16 11:22:08,258 - UQpy: Scientific Machine Learning: Epoch 351 / 1,000 Train Loss 1.0314729903873645 Train NLL 0.5209683556305734 Train KL 51050468.0 Test NLL 0.6645761728286743\n",
      "[INFO] - 2024-05-16 11:22:12,164 - UQpy: Scientific Machine Learning: Epoch 352 / 1,000 Train Loss 0.9826667340178239 Train NLL 0.4740147669064371 Train KL 50865196.0 Test NLL 0.4608049988746643\n",
      "[INFO] - 2024-05-16 11:22:16,116 - UQpy: Scientific Machine Learning: Epoch 353 / 1,000 Train Loss 0.8989740547380949 Train NLL 0.3921942099144584 Train KL 50677984.0 Test NLL 0.5839491486549377\n",
      "[INFO] - 2024-05-16 11:22:20,044 - UQpy: Scientific Machine Learning: Epoch 354 / 1,000 Train Loss 0.9358993109903837 Train NLL 0.4310158138212405 Train KL 50488352.0 Test NLL 0.5306898355484009\n",
      "[INFO] - 2024-05-16 11:22:23,952 - UQpy: Scientific Machine Learning: Epoch 355 / 1,000 Train Loss 0.983512376484118 Train NLL 0.4805232945241426 Train KL 50298912.0 Test NLL 0.5439127683639526\n",
      "[INFO] - 2024-05-16 11:22:27,881 - UQpy: Scientific Machine Learning: Epoch 356 / 1,000 Train Loss 0.9048573280635633 Train NLL 0.40375692279715286 Train KL 50110036.0 Test NLL 0.5237659215927124\n",
      "[INFO] - 2024-05-16 11:22:31,819 - UQpy: Scientific Machine Learning: Epoch 357 / 1,000 Train Loss 0.9765556736996299 Train NLL 0.47735730754701716 Train KL 49919836.0 Test NLL 0.6783355474472046\n",
      "[INFO] - 2024-05-16 11:22:35,776 - UQpy: Scientific Machine Learning: Epoch 358 / 1,000 Train Loss 0.9393098542564794 Train NLL 0.4420065519056822 Train KL 49730332.0 Test NLL 0.5222863554954529\n",
      "[INFO] - 2024-05-16 11:22:39,835 - UQpy: Scientific Machine Learning: Epoch 359 / 1,000 Train Loss 0.9453721987573724 Train NLL 0.449967014162164 Train KL 49540520.0 Test NLL 0.5365017652511597\n",
      "[INFO] - 2024-05-16 11:22:43,924 - UQpy: Scientific Machine Learning: Epoch 360 / 1,000 Train Loss 0.9918455042337117 Train NLL 0.49834853097012166 Train KL 49349696.0 Test NLL 0.6356302499771118\n",
      "[INFO] - 2024-05-16 11:22:47,966 - UQpy: Scientific Machine Learning: Epoch 361 / 1,000 Train Loss 0.9981345220615989 Train NLL 0.5065159217307442 Train KL 49161864.0 Test NLL 0.49350428581237793\n",
      "[INFO] - 2024-05-16 11:22:52,036 - UQpy: Scientific Machine Learning: Epoch 362 / 1,000 Train Loss 1.0030978823962964 Train NLL 0.5133536661926069 Train KL 48974416.0 Test NLL 0.599686324596405\n",
      "[INFO] - 2024-05-16 11:22:56,112 - UQpy: Scientific Machine Learning: Epoch 363 / 1,000 Train Loss 0.9439450941587749 Train NLL 0.45606965924564163 Train KL 48787544.0 Test NLL 0.4989413917064667\n",
      "[INFO] - 2024-05-16 11:23:00,181 - UQpy: Scientific Machine Learning: Epoch 364 / 1,000 Train Loss 0.9442249818852073 Train NLL 0.45822117360014664 Train KL 48600380.0 Test NLL 0.5778713226318359\n",
      "[INFO] - 2024-05-16 11:23:04,441 - UQpy: Scientific Machine Learning: Epoch 365 / 1,000 Train Loss 0.9409135391837672 Train NLL 0.45677396498228373 Train KL 48413960.0 Test NLL 0.5756270289421082\n",
      "[INFO] - 2024-05-16 11:23:08,479 - UQpy: Scientific Machine Learning: Epoch 366 / 1,000 Train Loss 0.9435530367650484 Train NLL 0.46125863414061696 Train KL 48229444.0 Test NLL 0.4760845899581909\n",
      "[INFO] - 2024-05-16 11:23:12,362 - UQpy: Scientific Machine Learning: Epoch 367 / 1,000 Train Loss 0.9271851464321739 Train NLL 0.4467714287732777 Train KL 48041376.0 Test NLL 0.4911705255508423\n",
      "[INFO] - 2024-05-16 11:23:16,318 - UQpy: Scientific Machine Learning: Epoch 368 / 1,000 Train Loss 0.912729169193067 Train NLL 0.4342140445583745 Train KL 47851512.0 Test NLL 0.48425519466400146\n",
      "[INFO] - 2024-05-16 11:23:20,249 - UQpy: Scientific Machine Learning: Epoch 369 / 1,000 Train Loss 0.9139654730495653 Train NLL 0.437368559209924 Train KL 47659688.0 Test NLL 0.5393650531768799\n",
      "[INFO] - 2024-05-16 11:23:24,145 - UQpy: Scientific Machine Learning: Epoch 370 / 1,000 Train Loss 0.9977108459723624 Train NLL 0.5230070195699993 Train KL 47470384.0 Test NLL 0.6019244194030762\n",
      "[INFO] - 2024-05-16 11:23:28,078 - UQpy: Scientific Machine Learning: Epoch 371 / 1,000 Train Loss 0.9218340359236065 Train NLL 0.44898329753624766 Train KL 47285076.0 Test NLL 0.46989893913269043\n",
      "[INFO] - 2024-05-16 11:23:31,938 - UQpy: Scientific Machine Learning: Epoch 372 / 1,000 Train Loss 0.8944342701058638 Train NLL 0.4234610748918433 Train KL 47097316.0 Test NLL 0.5428215861320496\n",
      "[INFO] - 2024-05-16 11:23:35,876 - UQpy: Scientific Machine Learning: Epoch 373 / 1,000 Train Loss 0.9162806680327967 Train NLL 0.4472271313792781 Train KL 46905356.0 Test NLL 0.7450881004333496\n",
      "[INFO] - 2024-05-16 11:23:39,779 - UQpy: Scientific Machine Learning: Epoch 374 / 1,000 Train Loss 0.9627471189749869 Train NLL 0.4956016948348598 Train KL 46714544.0 Test NLL 0.6473631858825684\n",
      "[INFO] - 2024-05-16 11:23:43,645 - UQpy: Scientific Machine Learning: Epoch 375 / 1,000 Train Loss 0.9343584901408145 Train NLL 0.469101115276939 Train KL 46525744.0 Test NLL 1.0650594234466553\n",
      "[INFO] - 2024-05-16 11:23:47,503 - UQpy: Scientific Machine Learning: Epoch 376 / 1,000 Train Loss 0.9685675533194291 Train NLL 0.5051995311912737 Train KL 46336812.0 Test NLL 0.498095840215683\n",
      "[INFO] - 2024-05-16 11:23:51,434 - UQpy: Scientific Machine Learning: Epoch 377 / 1,000 Train Loss 0.9124158307125694 Train NLL 0.45091976774366277 Train KL 46149604.0 Test NLL 0.46127772331237793\n",
      "[INFO] - 2024-05-16 11:23:55,340 - UQpy: Scientific Machine Learning: Epoch 378 / 1,000 Train Loss 0.8861605744612845 Train NLL 0.426566818827077 Train KL 45959372.0 Test NLL 0.4356793463230133\n",
      "[INFO] - 2024-05-16 11:23:59,186 - UQpy: Scientific Machine Learning: Epoch 379 / 1,000 Train Loss 0.8930735556702865 Train NLL 0.43540691701989426 Train KL 45766660.0 Test NLL 0.5396784543991089\n",
      "[INFO] - 2024-05-16 11:24:03,127 - UQpy: Scientific Machine Learning: Epoch 380 / 1,000 Train Loss 0.9002591151940195 Train NLL 0.4445272401759499 Train KL 45573184.0 Test NLL 0.8849866390228271\n",
      "[INFO] - 2024-05-16 11:24:07,040 - UQpy: Scientific Machine Learning: Epoch 381 / 1,000 Train Loss 0.9364459012684069 Train NLL 0.48263945077594955 Train KL 45380644.0 Test NLL 0.4502612352371216\n",
      "[INFO] - 2024-05-16 11:24:10,984 - UQpy: Scientific Machine Learning: Epoch 382 / 1,000 Train Loss 0.9928708045106185 Train NLL 0.5409448507585024 Train KL 45192596.0 Test NLL 0.7429212331771851\n",
      "[INFO] - 2024-05-16 11:24:15,037 - UQpy: Scientific Machine Learning: Epoch 383 / 1,000 Train Loss 0.9282201685403523 Train NLL 0.47814227248492996 Train KL 45007792.0 Test NLL 0.5388179421424866\n",
      "[INFO] - 2024-05-16 11:24:19,061 - UQpy: Scientific Machine Learning: Epoch 384 / 1,000 Train Loss 0.8896580746299342 Train NLL 0.44144993236190394 Train KL 44820820.0 Test NLL 0.4649343490600586\n",
      "[INFO] - 2024-05-16 11:24:23,078 - UQpy: Scientific Machine Learning: Epoch 385 / 1,000 Train Loss 0.8811449220305995 Train NLL 0.43483177141139384 Train KL 44631320.0 Test NLL 0.5031818747520447\n",
      "[INFO] - 2024-05-16 11:24:27,134 - UQpy: Scientific Machine Learning: Epoch 386 / 1,000 Train Loss 0.9569963091298154 Train NLL 0.5125952268901625 Train KL 44440108.0 Test NLL 0.6825439929962158\n",
      "[INFO] - 2024-05-16 11:24:31,025 - UQpy: Scientific Machine Learning: Epoch 387 / 1,000 Train Loss 0.897205462581233 Train NLL 0.454697543068936 Train KL 44250792.0 Test NLL 0.47017383575439453\n",
      "[INFO] - 2024-05-16 11:24:34,947 - UQpy: Scientific Machine Learning: Epoch 388 / 1,000 Train Loss 0.8807551641213266 Train NLL 0.4401409641692513 Train KL 44061424.0 Test NLL 0.6944149136543274\n",
      "[INFO] - 2024-05-16 11:24:38,856 - UQpy: Scientific Machine Learning: Epoch 389 / 1,000 Train Loss 0.9170769233452646 Train NLL 0.4783485249469155 Train KL 43872840.0 Test NLL 0.5493659973144531\n",
      "[INFO] - 2024-05-16 11:24:42,755 - UQpy: Scientific Machine Learning: Epoch 390 / 1,000 Train Loss 0.9892384378533614 Train NLL 0.5523814762893476 Train KL 43685696.0 Test NLL 0.4894483685493469\n",
      "[INFO] - 2024-05-16 11:24:46,651 - UQpy: Scientific Machine Learning: Epoch 391 / 1,000 Train Loss 0.9364221472489206 Train NLL 0.501410186290741 Train KL 43501196.0 Test NLL 0.48024260997772217\n",
      "[INFO] - 2024-05-16 11:24:50,563 - UQpy: Scientific Machine Learning: Epoch 392 / 1,000 Train Loss 0.8461232624555889 Train NLL 0.41298043571020426 Train KL 43314280.0 Test NLL 0.4738430082798004\n",
      "[INFO] - 2024-05-16 11:24:54,513 - UQpy: Scientific Machine Learning: Epoch 393 / 1,000 Train Loss 0.8901773032389189 Train NLL 0.4589491972797795 Train KL 43122812.0 Test NLL 0.5060532689094543\n",
      "[INFO] - 2024-05-16 11:24:58,571 - UQpy: Scientific Machine Learning: Epoch 394 / 1,000 Train Loss 0.8450123793200442 Train NLL 0.4156974271724099 Train KL 42931496.0 Test NLL 0.5346071720123291\n",
      "[INFO] - 2024-05-16 11:25:02,471 - UQpy: Scientific Machine Learning: Epoch 395 / 1,000 Train Loss 0.8829077576336107 Train NLL 0.45551082491874695 Train KL 42739700.0 Test NLL 0.491540789604187\n",
      "[INFO] - 2024-05-16 11:25:06,355 - UQpy: Scientific Machine Learning: Epoch 396 / 1,000 Train Loss 0.9747253028970015 Train NLL 0.5492213948776847 Train KL 42550388.0 Test NLL 0.756679892539978\n",
      "[INFO] - 2024-05-16 11:25:10,211 - UQpy: Scientific Machine Learning: Epoch 397 / 1,000 Train Loss 0.9928306341171265 Train NLL 0.5691702522729573 Train KL 42366036.0 Test NLL 0.6709298491477966\n",
      "[INFO] - 2024-05-16 11:25:14,181 - UQpy: Scientific Machine Learning: Epoch 398 / 1,000 Train Loss 0.943827189897236 Train NLL 0.5219704634264896 Train KL 42185676.0 Test NLL 0.8303532600402832\n",
      "[INFO] - 2024-05-16 11:25:18,053 - UQpy: Scientific Machine Learning: Epoch 399 / 1,000 Train Loss 0.9390734622353002 Train NLL 0.519005370767493 Train KL 42006808.0 Test NLL 0.5262305736541748\n",
      "[INFO] - 2024-05-16 11:25:21,927 - UQpy: Scientific Machine Learning: Epoch 400 / 1,000 Train Loss 0.8921550669168171 Train NLL 0.4738850107318477 Train KL 41827004.0 Test NLL 0.49893516302108765\n",
      "[INFO] - 2024-05-16 11:25:25,868 - UQpy: Scientific Machine Learning: Epoch 401 / 1,000 Train Loss 0.8850937171986228 Train NLL 0.4686444386055595 Train KL 41644936.0 Test NLL 0.5232791304588318\n",
      "[INFO] - 2024-05-16 11:25:29,881 - UQpy: Scientific Machine Learning: Epoch 402 / 1,000 Train Loss 0.8627108053157204 Train NLL 0.44809029447404963 Train KL 41462052.0 Test NLL 0.5734580159187317\n",
      "[INFO] - 2024-05-16 11:25:33,804 - UQpy: Scientific Machine Learning: Epoch 403 / 1,000 Train Loss 0.8608362517858806 Train NLL 0.44806212343667684 Train KL 41277412.0 Test NLL 0.4722070097923279\n",
      "[INFO] - 2024-05-16 11:25:37,727 - UQpy: Scientific Machine Learning: Epoch 404 / 1,000 Train Loss 0.8327327088305825 Train NLL 0.4218354538867348 Train KL 41089728.0 Test NLL 0.4491756856441498\n",
      "[INFO] - 2024-05-16 11:25:41,588 - UQpy: Scientific Machine Learning: Epoch 405 / 1,000 Train Loss 0.8341115430781716 Train NLL 0.4251227159249155 Train KL 40898884.0 Test NLL 0.49363863468170166\n",
      "[INFO] - 2024-05-16 11:25:45,493 - UQpy: Scientific Machine Learning: Epoch 406 / 1,000 Train Loss 0.8291313146290026 Train NLL 0.4220388986562428 Train KL 40709236.0 Test NLL 0.5264330506324768\n",
      "[INFO] - 2024-05-16 11:25:49,424 - UQpy: Scientific Machine Learning: Epoch 407 / 1,000 Train Loss 0.8192963004112244 Train NLL 0.41412219248319926 Train KL 40517416.0 Test NLL 0.4378202557563782\n",
      "[INFO] - 2024-05-16 11:25:53,335 - UQpy: Scientific Machine Learning: Epoch 408 / 1,000 Train Loss 0.8671951827250028 Train NLL 0.4639262494287993 Train KL 40326896.0 Test NLL 0.5103639364242554\n",
      "[INFO] - 2024-05-16 11:25:57,259 - UQpy: Scientific Machine Learning: Epoch 409 / 1,000 Train Loss 0.8853397902689482 Train NLL 0.48390815759959976 Train KL 40143164.0 Test NLL 0.47141769528388977\n",
      "[INFO] - 2024-05-16 11:26:01,173 - UQpy: Scientific Machine Learning: Epoch 410 / 1,000 Train Loss 0.8348591390408968 Train NLL 0.43525061952440364 Train KL 39960856.0 Test NLL 0.4619699716567993\n",
      "[INFO] - 2024-05-16 11:26:05,127 - UQpy: Scientific Machine Learning: Epoch 411 / 1,000 Train Loss 0.8317883610725403 Train NLL 0.43406525881666885 Train KL 39772308.0 Test NLL 0.7672334909439087\n",
      "[INFO] - 2024-05-16 11:26:09,002 - UQpy: Scientific Machine Learning: Epoch 412 / 1,000 Train Loss 0.8545967371840226 Train NLL 0.4587455485996447 Train KL 39585120.0 Test NLL 0.46047723293304443\n",
      "[INFO] - 2024-05-16 11:26:12,900 - UQpy: Scientific Machine Learning: Epoch 413 / 1,000 Train Loss 0.8271420315692299 Train NLL 0.43316322251370076 Train KL 39397876.0 Test NLL 0.5435909032821655\n",
      "[INFO] - 2024-05-16 11:26:16,770 - UQpy: Scientific Machine Learning: Epoch 414 / 1,000 Train Loss 0.8009608387947083 Train NLL 0.40887154717194407 Train KL 39208928.0 Test NLL 0.49507129192352295\n",
      "[INFO] - 2024-05-16 11:26:20,661 - UQpy: Scientific Machine Learning: Epoch 415 / 1,000 Train Loss 0.8012464015107406 Train NLL 0.4110492577678279 Train KL 39019712.0 Test NLL 0.5301153063774109\n",
      "[INFO] - 2024-05-16 11:26:24,525 - UQpy: Scientific Machine Learning: Epoch 416 / 1,000 Train Loss 0.8460598870327598 Train NLL 0.45774815898192556 Train KL 38831172.0 Test NLL 0.5707770586013794\n",
      "[INFO] - 2024-05-16 11:26:28,388 - UQpy: Scientific Machine Learning: Epoch 417 / 1,000 Train Loss 0.8314894092710394 Train NLL 0.44504182589681524 Train KL 38644760.0 Test NLL 0.4600614905357361\n",
      "[INFO] - 2024-05-16 11:26:32,289 - UQpy: Scientific Machine Learning: Epoch 418 / 1,000 Train Loss 0.8337348542715374 Train NLL 0.44914183177446065 Train KL 38459300.0 Test NLL 0.5165272951126099\n",
      "[INFO] - 2024-05-16 11:26:36,202 - UQpy: Scientific Machine Learning: Epoch 419 / 1,000 Train Loss 0.9568046268663908 Train NLL 0.5740478744632319 Train KL 38275676.0 Test NLL 0.7277156114578247\n",
      "[INFO] - 2024-05-16 11:26:40,081 - UQpy: Scientific Machine Learning: Epoch 420 / 1,000 Train Loss 0.9354446122520849 Train NLL 0.5543631613254547 Train KL 38108148.0 Test NLL 0.5365867614746094\n",
      "[INFO] - 2024-05-16 11:26:43,957 - UQpy: Scientific Machine Learning: Epoch 421 / 1,000 Train Loss 0.8204712899107682 Train NLL 0.4410456764070611 Train KL 37942564.0 Test NLL 0.5694875717163086\n",
      "[INFO] - 2024-05-16 11:26:47,860 - UQpy: Scientific Machine Learning: Epoch 422 / 1,000 Train Loss 0.8325237035751343 Train NLL 0.45490197445216934 Train KL 37762168.0 Test NLL 0.44511181116104126\n",
      "[INFO] - 2024-05-16 11:26:51,715 - UQpy: Scientific Machine Learning: Epoch 423 / 1,000 Train Loss 0.7881794007200944 Train NLL 0.4123750344703072 Train KL 37580436.0 Test NLL 0.43596014380455017\n",
      "[INFO] - 2024-05-16 11:26:55,609 - UQpy: Scientific Machine Learning: Epoch 424 / 1,000 Train Loss 0.7976495905926353 Train NLL 0.4236925307073091 Train KL 37395704.0 Test NLL 0.5324167609214783\n",
      "[INFO] - 2024-05-16 11:26:59,491 - UQpy: Scientific Machine Learning: Epoch 425 / 1,000 Train Loss 0.8072015392152887 Train NLL 0.4350951219859876 Train KL 37210644.0 Test NLL 0.6353012323379517\n",
      "[INFO] - 2024-05-16 11:27:03,428 - UQpy: Scientific Machine Learning: Epoch 426 / 1,000 Train Loss 0.8045349685769332 Train NLL 0.4342585158975501 Train KL 37027644.0 Test NLL 0.4711269736289978\n",
      "[INFO] - 2024-05-16 11:27:07,388 - UQpy: Scientific Machine Learning: Epoch 427 / 1,000 Train Loss 0.7847914256547627 Train NLL 0.4163539252783123 Train KL 36843748.0 Test NLL 0.5050353407859802\n",
      "[INFO] - 2024-05-16 11:27:11,316 - UQpy: Scientific Machine Learning: Epoch 428 / 1,000 Train Loss 0.8565898412152341 Train NLL 0.48996742148148387 Train KL 36662244.0 Test NLL 0.63770592212677\n",
      "[INFO] - 2024-05-16 11:27:15,201 - UQpy: Scientific Machine Learning: Epoch 429 / 1,000 Train Loss 0.775747603491733 Train NLL 0.41087012698775843 Train KL 36487748.0 Test NLL 0.5153185129165649\n",
      "[INFO] - 2024-05-16 11:27:19,105 - UQpy: Scientific Machine Learning: Epoch 430 / 1,000 Train Loss 0.7993872793097245 Train NLL 0.4363075777104026 Train KL 36307972.0 Test NLL 0.4657239317893982\n",
      "[INFO] - 2024-05-16 11:27:23,165 - UQpy: Scientific Machine Learning: Epoch 431 / 1,000 Train Loss 0.7965412234005175 Train NLL 0.43526002450993184 Train KL 36128120.0 Test NLL 0.4434086084365845\n",
      "[INFO] - 2024-05-16 11:27:27,117 - UQpy: Scientific Machine Learning: Epoch 432 / 1,000 Train Loss 0.7995152630304035 Train NLL 0.4400553922904165 Train KL 35945992.0 Test NLL 0.7460751533508301\n",
      "[INFO] - 2024-05-16 11:27:31,043 - UQpy: Scientific Machine Learning: Epoch 433 / 1,000 Train Loss 0.8581287547161704 Train NLL 0.5004758991693196 Train KL 35765288.0 Test NLL 0.6980351209640503\n",
      "[INFO] - 2024-05-16 11:27:34,998 - UQpy: Scientific Machine Learning: Epoch 434 / 1,000 Train Loss 0.8372462642820258 Train NLL 0.4813541017080608 Train KL 35589216.0 Test NLL 0.610498309135437\n",
      "[INFO] - 2024-05-16 11:27:39,163 - UQpy: Scientific Machine Learning: Epoch 435 / 1,000 Train Loss 0.8100598925038388 Train NLL 0.455907730679763 Train KL 35415220.0 Test NLL 0.5735948085784912\n",
      "[INFO] - 2024-05-16 11:27:43,175 - UQpy: Scientific Machine Learning: Epoch 436 / 1,000 Train Loss 0.8183110324960006 Train NLL 0.4658560894037548 Train KL 35245492.0 Test NLL 0.4545617401599884\n",
      "[INFO] - 2024-05-16 11:27:47,105 - UQpy: Scientific Machine Learning: Epoch 437 / 1,000 Train Loss 0.7623397017780104 Train NLL 0.41163091753658493 Train KL 35070880.0 Test NLL 0.5394035577774048\n",
      "[INFO] - 2024-05-16 11:27:51,313 - UQpy: Scientific Machine Learning: Epoch 438 / 1,000 Train Loss 0.8018653173195688 Train NLL 0.4529062619334773 Train KL 34895908.0 Test NLL 0.5360698103904724\n",
      "[INFO] - 2024-05-16 11:27:55,457 - UQpy: Scientific Machine Learning: Epoch 439 / 1,000 Train Loss 0.8616043611576683 Train NLL 0.5142879172375328 Train KL 34731636.0 Test NLL 0.5468642115592957\n",
      "[INFO] - 2024-05-16 11:27:59,640 - UQpy: Scientific Machine Learning: Epoch 440 / 1,000 Train Loss 0.8583076627630937 Train NLL 0.5125712250408373 Train KL 34573644.0 Test NLL 0.49987292289733887\n",
      "[INFO] - 2024-05-16 11:28:03,732 - UQpy: Scientific Machine Learning: Epoch 441 / 1,000 Train Loss 0.8115277227602506 Train NLL 0.4673793206089421 Train KL 34414840.0 Test NLL 0.5480116009712219\n",
      "[INFO] - 2024-05-16 11:28:07,825 - UQpy: Scientific Machine Learning: Epoch 442 / 1,000 Train Loss 0.8010676816890114 Train NLL 0.4585596464182201 Train KL 34250804.0 Test NLL 0.6834802627563477\n",
      "[INFO] - 2024-05-16 11:28:11,919 - UQpy: Scientific Machine Learning: Epoch 443 / 1,000 Train Loss 0.765380112748397 Train NLL 0.42456114605853434 Train KL 34081896.0 Test NLL 0.498862624168396\n",
      "[INFO] - 2024-05-16 11:28:15,982 - UQpy: Scientific Machine Learning: Epoch 444 / 1,000 Train Loss 0.7377749367764121 Train NLL 0.39872760521738154 Train KL 33904736.0 Test NLL 0.5850461721420288\n",
      "[INFO] - 2024-05-16 11:28:20,056 - UQpy: Scientific Machine Learning: Epoch 445 / 1,000 Train Loss 0.76700191435061 Train NLL 0.42974631409896047 Train KL 33725560.0 Test NLL 0.4912412166595459\n",
      "[INFO] - 2024-05-16 11:28:24,012 - UQpy: Scientific Machine Learning: Epoch 446 / 1,000 Train Loss 0.7563368740834688 Train NLL 0.4208563032903169 Train KL 33548058.0 Test NLL 0.5144497752189636\n",
      "[INFO] - 2024-05-16 11:28:28,036 - UQpy: Scientific Machine Learning: Epoch 447 / 1,000 Train Loss 0.7652418205612584 Train NLL 0.4315061553528434 Train KL 33373564.0 Test NLL 0.5108493566513062\n",
      "[INFO] - 2024-05-16 11:28:32,083 - UQpy: Scientific Machine Learning: Epoch 448 / 1,000 Train Loss 0.7457488304690311 Train NLL 0.4137653771199678 Train KL 33198346.0 Test NLL 0.5086491703987122\n",
      "[INFO] - 2024-05-16 11:28:35,992 - UQpy: Scientific Machine Learning: Epoch 449 / 1,000 Train Loss 0.7645832519782217 Train NLL 0.4343747339750591 Train KL 33020850.0 Test NLL 0.5408576726913452\n",
      "[INFO] - 2024-05-16 11:28:39,936 - UQpy: Scientific Machine Learning: Epoch 450 / 1,000 Train Loss 0.7582478805592185 Train NLL 0.4297683568377244 Train KL 32847952.0 Test NLL 0.4854586124420166\n",
      "[INFO] - 2024-05-16 11:28:43,897 - UQpy: Scientific Machine Learning: Epoch 451 / 1,000 Train Loss 0.7618146883813959 Train NLL 0.43506415109885366 Train KL 32675058.0 Test NLL 0.5361069440841675\n",
      "[INFO] - 2024-05-16 11:28:47,826 - UQpy: Scientific Machine Learning: Epoch 452 / 1,000 Train Loss 0.7324186406637493 Train NLL 0.4073537337152581 Train KL 32506492.0 Test NLL 0.503269612789154\n",
      "[INFO] - 2024-05-16 11:28:51,761 - UQpy: Scientific Machine Learning: Epoch 453 / 1,000 Train Loss 0.710814937164909 Train NLL 0.3874716241108744 Train KL 32334336.0 Test NLL 0.4741392135620117\n",
      "[INFO] - 2024-05-16 11:28:55,702 - UQpy: Scientific Machine Learning: Epoch 454 / 1,000 Train Loss 0.7077672575649462 Train NLL 0.3861844257304543 Train KL 32158286.0 Test NLL 0.5249402523040771\n",
      "[INFO] - 2024-05-16 11:28:59,634 - UQpy: Scientific Machine Learning: Epoch 455 / 1,000 Train Loss 0.7387158431504902 Train NLL 0.41889312706495585 Train KL 31982272.0 Test NLL 0.4297277331352234\n",
      "[INFO] - 2024-05-16 11:29:03,562 - UQpy: Scientific Machine Learning: Epoch 456 / 1,000 Train Loss 0.7476339151984767 Train NLL 0.42952119833544683 Train KL 31811278.0 Test NLL 0.4842635989189148\n",
      "[INFO] - 2024-05-16 11:29:07,497 - UQpy: Scientific Machine Learning: Epoch 457 / 1,000 Train Loss 0.7200490110798886 Train NLL 0.4036408443199961 Train KL 31640818.0 Test NLL 0.4912751019001007\n",
      "[INFO] - 2024-05-16 11:29:11,425 - UQpy: Scientific Machine Learning: Epoch 458 / 1,000 Train Loss 0.7369128904844585 Train NLL 0.42223182477449117 Train KL 31468106.0 Test NLL 0.46415266394615173\n",
      "[INFO] - 2024-05-16 11:29:15,389 - UQpy: Scientific Machine Learning: Epoch 459 / 1,000 Train Loss 0.7416502488286871 Train NLL 0.42867264935844823 Train KL 31297758.0 Test NLL 0.46809637546539307\n",
      "[INFO] - 2024-05-16 11:29:19,303 - UQpy: Scientific Machine Learning: Epoch 460 / 1,000 Train Loss 0.7419619497499967 Train NLL 0.4306643197410985 Train KL 31129766.0 Test NLL 0.5404412746429443\n",
      "[INFO] - 2024-05-16 11:29:23,237 - UQpy: Scientific Machine Learning: Epoch 461 / 1,000 Train Loss 0.7474091178492496 Train NLL 0.4377401267227374 Train KL 30966898.0 Test NLL 0.5197489857673645\n",
      "[INFO] - 2024-05-16 11:29:27,159 - UQpy: Scientific Machine Learning: Epoch 462 / 1,000 Train Loss 0.7992648701918753 Train NLL 0.4911557530101977 Train KL 30810914.0 Test NLL 0.4776008725166321\n",
      "[INFO] - 2024-05-16 11:29:31,101 - UQpy: Scientific Machine Learning: Epoch 463 / 1,000 Train Loss 0.7708173394203186 Train NLL 0.46418812086707667 Train KL 30662926.0 Test NLL 0.6127407550811768\n",
      "[INFO] - 2024-05-16 11:29:35,075 - UQpy: Scientific Machine Learning: Epoch 464 / 1,000 Train Loss 0.7602924986889488 Train NLL 0.45519666765865524 Train KL 30509584.0 Test NLL 0.5726075172424316\n",
      "[INFO] - 2024-05-16 11:29:39,184 - UQpy: Scientific Machine Learning: Epoch 465 / 1,000 Train Loss 0.7367372575559115 Train NLL 0.43318453744838115 Train KL 30355270.0 Test NLL 0.575370192527771\n",
      "[INFO] - 2024-05-16 11:29:43,282 - UQpy: Scientific Machine Learning: Epoch 466 / 1,000 Train Loss 0.747658654263145 Train NLL 0.4456285457862051 Train KL 30203014.0 Test NLL 0.5685343742370605\n",
      "[INFO] - 2024-05-16 11:29:47,399 - UQpy: Scientific Machine Learning: Epoch 467 / 1,000 Train Loss 0.7611434616540608 Train NLL 0.4606777868772808 Train KL 30046568.0 Test NLL 0.5488491058349609\n",
      "[INFO] - 2024-05-16 11:29:51,433 - UQpy: Scientific Machine Learning: Epoch 468 / 1,000 Train Loss 0.6887096103868986 Train NLL 0.3898502964722483 Train KL 29885932.0 Test NLL 0.5599561333656311\n",
      "[INFO] - 2024-05-16 11:29:55,418 - UQpy: Scientific Machine Learning: Epoch 469 / 1,000 Train Loss 0.7763646464598807 Train NLL 0.4791542479866429 Train KL 29721038.0 Test NLL 0.49879738688468933\n",
      "[INFO] - 2024-05-16 11:29:59,514 - UQpy: Scientific Machine Learning: Epoch 470 / 1,000 Train Loss 0.7422831434952585 Train NLL 0.44663452317840174 Train KL 29564860.0 Test NLL 0.49582141637802124\n",
      "[INFO] - 2024-05-16 11:30:03,543 - UQpy: Scientific Machine Learning: Epoch 471 / 1,000 Train Loss 0.7501706324125591 Train NLL 0.4560296111985257 Train KL 29414100.0 Test NLL 0.5721781253814697\n",
      "[INFO] - 2024-05-16 11:30:07,639 - UQpy: Scientific Machine Learning: Epoch 472 / 1,000 Train Loss 0.7651976629307395 Train NLL 0.4725206211993569 Train KL 29267702.0 Test NLL 0.5657105445861816\n",
      "[INFO] - 2024-05-16 11:30:11,746 - UQpy: Scientific Machine Learning: Epoch 473 / 1,000 Train Loss 0.7121612492360567 Train NLL 0.4209786716260408 Train KL 29118262.0 Test NLL 0.5247628092765808\n",
      "[INFO] - 2024-05-16 11:30:15,831 - UQpy: Scientific Machine Learning: Epoch 474 / 1,000 Train Loss 0.7428591220002425 Train NLL 0.453236543818524 Train KL 28962256.0 Test NLL 0.4417775869369507\n",
      "[INFO] - 2024-05-16 11:30:19,880 - UQpy: Scientific Machine Learning: Epoch 475 / 1,000 Train Loss 0.6845499935903048 Train NLL 0.3964907950476596 Train KL 28805922.0 Test NLL 0.5636357665061951\n",
      "[INFO] - 2024-05-16 11:30:23,944 - UQpy: Scientific Machine Learning: Epoch 476 / 1,000 Train Loss 0.6946964985445926 Train NLL 0.4082400045896831 Train KL 28645648.0 Test NLL 0.4622727930545807\n",
      "[INFO] - 2024-05-16 11:30:28,028 - UQpy: Scientific Machine Learning: Epoch 477 / 1,000 Train Loss 0.7163924417997661 Train NLL 0.43149909377098083 Train KL 28489334.0 Test NLL 0.5290856957435608\n",
      "[INFO] - 2024-05-16 11:30:32,069 - UQpy: Scientific Machine Learning: Epoch 478 / 1,000 Train Loss 0.6677396517050894 Train NLL 0.3843842227207987 Train KL 28335542.0 Test NLL 0.6446614265441895\n",
      "[INFO] - 2024-05-16 11:30:36,115 - UQpy: Scientific Machine Learning: Epoch 479 / 1,000 Train Loss 0.7146412589048085 Train NLL 0.43289549413480255 Train KL 28174576.0 Test NLL 0.6304463148117065\n",
      "[INFO] - 2024-05-16 11:30:40,158 - UQpy: Scientific Machine Learning: Epoch 480 / 1,000 Train Loss 0.7519838276662325 Train NLL 0.4717898588431509 Train KL 28019398.0 Test NLL 0.4925532639026642\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tutorial/lib/python3.9/site-packages/UQpy-4.1.5-py3.9.egg/UQpy/scientific_machine_learning/trainers/BBBTrainer.py:102\u001b[0m, in \u001b[0;36mBBBTrainer.run\u001b[0;34m(self, train_data, test_data, epochs, num_samples, tolerance, beta)\u001b[0m\n\u001b[1;32m    100\u001b[0m nll_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(num_samples)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n\u001b[0;32m--> 102\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     nll_loss[sample] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function(prediction, y)\n\u001b[1;32m    104\u001b[0m kl_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcompute_kullback_leibler_divergence()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tutorial/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tutorial/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tutorial/lib/python3.9/site-packages/UQpy-4.1.5-py3.9.egg/UQpy/scientific_machine_learning/neural_networks/DeepOperatorNetwork.py:55\u001b[0m, in \u001b[0;36mDeepOperatorNetwork.forward\u001b[0;34m(self, x, u_x)\u001b[0m\n\u001b[1;32m     53\u001b[0m u_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39matleast_2d(u_x)\n\u001b[1;32m     54\u001b[0m branch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch_network(u_x)\n\u001b[0;32m---> 55\u001b[0m trunk_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrunk_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Implementing multiple outputs\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" ToDo: Find a better way of doing this \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tutorial/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tutorial/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m, in \u001b[0;36mTrunkNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 18\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXmin\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXmax\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXmin\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfnn(x)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tutorial/lib/python3.9/site-packages/torch/_tensor.py:1093\u001b[0m, in \u001b[0;36mTensor.__array_wrap__\u001b[0;34m(self, array)\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# Wrap Numpy array again in a suitable tensor when done, to support e.g.\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;66;03m# `numpy.sin(tensor) -> tensor` or `numpy.greater(tensor, 0) -> ByteTensor`\u001b[39;00m\n\u001b[0;32m-> 1093\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array_wrap__\u001b[39m(\u001b[38;5;28mself\u001b[39m, array):\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1095\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   1096\u001b[0m             Tensor\u001b[38;5;241m.\u001b[39m__array_wrap__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, array\u001b[38;5;241m=\u001b[39marray\n\u001b[1;32m   1097\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.run(\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    "    epochs=1000,\n",
    "    tolerance=1e-4,\n",
    "    beta=1e-8,\n",
    "    num_samples=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76869e2f-5c87-4ffc-af15-8670f87f255b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss = trainer.history[\"train_nll\"].detach().numpy()\n",
    "test_loss = trainer.history[\"test_nll\"].detach().numpy()\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(train_loss, label = 'Train NLL')\n",
    "ax.plot(test_loss, label = 'Test NLL')\n",
    "ax.set_title(\"Bayes By Backpropagation Training Loss\")\n",
    "ax.set(xlabel=\"Epoch\", ylabel=\"Loss\")\n",
    "ax.legend()\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e2645d4-ac14-45d2-824d-ed5d7d16f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate test data and save results\n",
    "def eval_model(test_data,model,uq=False):\n",
    "    if uq:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    ux_pred_list = []\n",
    "    uy_pred_list = []\n",
    "    ux_test_list = []\n",
    "    uy_test_list = []\n",
    "    x_list = []\n",
    "    for batch_number, (*x, y) in enumerate(test_data):\n",
    "        ux_pred, uy_pred = model(*x)\n",
    "        ux_test , uy_test = y\n",
    "        ux_pred_list.append(ux_pred)\n",
    "        uy_pred_list.append(uy_pred)\n",
    "        ux_test_list.append(ux_test)\n",
    "        uy_test_list.append(uy_test)\n",
    "        x_list.append(x[1][:,0,:])\n",
    "    return torch.cat(ux_pred_list), torch.cat(uy_pred_list), torch.cat(ux_test_list), torch.cat(uy_test_list), torch.cat(x_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c731cf5-fecd-4505-9812-bcb74e0d5746",
   "metadata": {},
   "outputs": [],
   "source": [
    "ux_pred, uy_pred, ux_test, uy_test, x_test = eval_model(test_data,model)\n",
    "ux_pred = rescale(ux_pred.detach(), np.squeeze(ux_train_mean, axis=2), np.squeeze(ux_train_std, axis=2))\n",
    "uy_pred = rescale(uy_pred.detach(), np.squeeze(uy_train_mean, axis=2), np.squeeze(uy_train_std, axis=2))\n",
    "ux_test = rescale(ux_test.detach(), np.squeeze(ux_train_mean, axis=2), np.squeeze(ux_train_std, axis=2))\n",
    "uy_test = rescale(uy_test.detach(), np.squeeze(uy_train_mean, axis=2), np.squeeze(uy_train_std, axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ca045f7-4d00-4731-aa6a-a42d2f600a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_uq_samps = 100\n",
    "ux_pred_list = []\n",
    "uy_pred_list = []\n",
    "for i in range(num_uq_samps):\n",
    "    ux_samp_pred, uy_samp_pred, _, _,_  = eval_model(test_data,model,uq=True)\n",
    "    ux_samp_pred = rescale(ux_samp_pred.detach(), np.squeeze(ux_train_mean, axis=2), np.squeeze(ux_train_std, axis=2))\n",
    "    uy_samp_pred = rescale(uy_samp_pred.detach(), np.squeeze(uy_train_mean, axis=2), np.squeeze(uy_train_std, axis=2))\n",
    "    ux_pred_list.append(ux_samp_pred)\n",
    "    uy_pred_list.append(uy_samp_pred)\n",
    "ux_sigma = np.std(ux_pred_list,axis=0)\n",
    "uy_sigma = np.std(uy_pred_list,axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66870726-f6cc-41cc-aa69-c85848313c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "savemat('Bayesian_Elastic_plate.mat',{'x_test': x_test.detach().numpy(), 'ux_test': ux_test.detach().numpy(), 'uy_test': uy_test.detach().numpy(), \n",
    "                              'ux_pred': ux_pred.detach().numpy(), 'uy_pred': uy_pred.detach().numpy(), 'ux_sigma': ux_sigma, 'uy_sigma': uy_sigma} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d9e696-fa92-48c0-b5f0-d2e045489a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
